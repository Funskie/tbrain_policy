{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda3\\envs\\AI\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression,LassoCV, Ridge, LassoLarsCV,ElasticNetCV, SGDRegressor, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, learning_curve, KFold\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from xgboost import XGBRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingset_path = '../dataset/training-set.csv'\n",
    "testingset_path = '../dataset/testing-set.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_910 = pd.read_csv('../dataset/train_df_0910.csv', encoding='utf-8')\n",
    "test_df_910 = pd.read_csv('../dataset/test_df_0910.csv', encoding='utf-8')\n",
    "train_df_912 = pd.read_csv('../dataset/train_claim_df_0912.csv', encoding='utf-8')\n",
    "test_df_912 = pd.read_csv('../dataset/test_claim_df_0912.csv', encoding='utf-8')\n",
    "trainingset_df = pd.read_csv(trainingset_path, encoding='utf-8')\n",
    "testingset_df = pd.read_csv(testingset_path, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_910 = train_df_910.iloc[:, 2:]\n",
    "train_data_912 = train_df_912.iloc[:, 2:]\n",
    "\n",
    "train_label = train_df_910.iloc[:, 1]\n",
    "\n",
    "test_data_910 = test_df_910.iloc[:, 2:]\n",
    "test_data_912 = test_df_912.iloc[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210763, 51) (210763, 90) (140510, 51) (140510, 90) (210763,)\n"
     ]
    }
   ],
   "source": [
    "print(train_data_910.shape, train_data_912.shape, test_data_910.shape, test_data_912.shape, train_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_method(model, X, y):\n",
    "#     estimators = []\n",
    "#     estimators.append(('standardize', StandardScaler()))\n",
    "#     estimators.append(('model', model))\n",
    "#     pipeline = Pipeline(estimators)\n",
    "#     print(pipeline.steps)\n",
    "    kfold = KFold(n_splits=5, random_state=7).get_n_splits(X.values)\n",
    "    results = cross_val_score(model, X.values, y.values, cv=kfold, n_jobs=4, scoring='mean_absolute_error')\n",
    "    return {'mae_mean':-results.mean(), 'mae_std':results.std()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n",
    "ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n",
    "# KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_param = {'boosting_type': 'gbdt',\n",
    "             'objective': 'regression_l1',\n",
    "             'max_depth': 12,\n",
    "             'num_leaves': 80,\n",
    "             'min_child_samples': 19,\n",
    "             'min_child_weight': 0.001,\n",
    "             'bagging_fraction': 0.6,\n",
    "             'feature_fraction': 1.0,\n",
    "             'reg_alpha': 0.001,\n",
    "             'reg_lambda': 0.3,\n",
    "             'metric': 'mae',\n",
    "             'verbose': 0,\n",
    "             'colsample_bytree': 0.6,\n",
    "             'subsample': 0.6,\n",
    "             'n_estimators':420}\n",
    "LGB = make_pipeline(StandardScaler(), lgb.LGBMRegressor(**lgb_param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "GB = make_pipeline(StandardScaler(), GradientBoostingRegressor(n_estimators=2000, learning_rate=0.02,\n",
    "                                   max_depth=10, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = make_pipeline(StandardScaler(), RandomForestRegressor(n_estimators=1500, max_depth=7, random_state =5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_param = {\n",
    "    'n_estimators': 2402,\n",
    "    'learning_rate': 0.02,\n",
    "    'objective': 'reg:linear',\n",
    "    'subsample': 0.9,#checked\n",
    "    'colsample_bytree': 0.9,#checked\n",
    "    'min_child_weight': 1,#checked\n",
    "    'max_depth': 9,#checked\n",
    "    'gamma': 0.0,#checked  \n",
    "    'scale_pos_weight': 1,\n",
    "    'reg_alpha': 0.01,\n",
    "    'reg_lambda': 0.1,\n",
    "}\n",
    "XGB = make_pipeline(StandardScaler(), XGBRegressor(**xgb_param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_cv = cv_method(lasso, train_data_912, train_label)\n",
    "ENet_cv = cv_method(ENet, train_data_912, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_cv = cv_method(LGB, train_data_912, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_cv = cv_method(GB, train_data_912, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_cv = cv_method(XGB, train_data_912, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_cv = cv_method(RF,train_data_912, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso score: 2199.0124 (360.1978)\n",
      "\n",
      "ENet score: 2198.2441 (359.9161)\n",
      "\n",
      "LGB score: 1750.6379 (298.8883)\n",
      "\n",
      "GB score: 1903.8929 (300.4203)\n",
      "\n",
      "XGB score: 1998.0624 (321.7138)\n",
      "\n",
      "RF score: 2015.6454 (324.8090)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Lasso score: {:.4f} ({:.4f})\\n\".format(lasso_cv['mae_mean'], lasso_cv['mae_std']))\n",
    "print(\"ENet score: {:.4f} ({:.4f})\\n\".format(ENet_cv['mae_mean'], ENet_cv['mae_std']))\n",
    "# print(\"KRR score: {:.4f} ({:.4f})\\n\".format(KRR_cv['mae_mean'], KRR_cv['mae_std']))\n",
    "print(\"LGB score: {:.4f} ({:.4f})\\n\".format(lgb_cv['mae_mean'], lgb_cv['mae_std']))\n",
    "print(\"GB score: {:.4f} ({:.4f})\\n\".format(gb_cv['mae_mean'], gb_cv['mae_std']))\n",
    "print(\"XGB score: {:.4f} ({:.4f})\\n\".format(xgb_cv['mae_mean'], xgb_cv['mae_std']))\n",
    "print(\"RF score: {:.4f} ({:.4f})\\n\".format(rf_cv['mae_mean'], rf_cv['mae_std']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso score: 2425.9923 (303.0153)\n",
      "\n",
      "ENet score: 2424.1391 (302.7454)\n",
      "\n",
      "LGB score: 1751.2164 (299.6216)\n",
      "\n",
      "GB score: 1904.2164 (300.6003)\n",
      "\n",
      "XGB score: 1997.8895 (322.3094)\n",
      "\n",
      "RF score: 2015.7044 (324.9064)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Lasso score: {:.4f} ({:.4f})\\n\".format(lasso_cv['mae_mean'], lasso_cv['mae_std']))\n",
    "print(\"ENet score: {:.4f} ({:.4f})\\n\".format(ENet_cv['mae_mean'], ENet_cv['mae_std']))\n",
    "# print(\"KRR score: {:.4f} ({:.4f})\\n\".format(KRR_cv['mae_mean'], KRR_cv['mae_std']))\n",
    "print(\"LGB score: {:.4f} ({:.4f})\\n\".format(lgb_cv['mae_mean'], lgb_cv['mae_std']))\n",
    "print(\"GB score: {:.4f} ({:.4f})\\n\".format(gb_cv['mae_mean'], gb_cv['mae_std']))\n",
    "print(\"XGB score: {:.4f} ({:.4f})\\n\".format(xgb_cv['mae_mean'], xgb_cv['mae_std']))\n",
    "print(\"RF score: {:.4f} ({:.4f})\\n\".format(rf_cv['mae_mean'], rf_cv['mae_std']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model = lgb_cv['pipe'].fit(train_claim_data, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lgb = lgb_model.predict(test_claim_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model = GB.fit(train_claim_data, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_gb = gb_model.predict(test_claim_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result to csv\n",
    "submit = testingset_df.copy()\n",
    "submit['Next_Premium'] = y_pred_gb\n",
    "submit.iloc[submit[submit['Next_Premium'] < 0].index, 1] = 0\n",
    "submit.to_csv('../result_csv/gb_0911_1903.csv', sep=',', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stacking method \n",
    "#### from https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        \n",
    "    # we define clones of the original models to fit the data in\n",
    "    def fit(self, X, y):\n",
    "        self.models_ = [clone(x) for x in self.models]\n",
    "        \n",
    "        # Train cloned base models\n",
    "        for model in self.models_:\n",
    "            model.fit(X, y)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    #Now we do the predictions for cloned models and average them\n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack([\n",
    "            model.predict(X) for model in self.models_\n",
    "        ])\n",
    "        return np.mean(predictions, axis=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, base_models, meta_model, n_folds=5):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    "   \n",
    "    # We again fit the data on clones of the original models\n",
    "    def fit(self, X, y):\n",
    "        self.base_models_ = [list() for x in self.base_models]\n",
    "        self.meta_model_ = clone(self.meta_model)\n",
    "        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n",
    "        \n",
    "        # Train cloned base models then create out-of-fold predictions\n",
    "        # that are needed to train the cloned meta-model\n",
    "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            for train_index, holdout_index in kfold.split(X, y):\n",
    "                instance = clone(model)\n",
    "                self.base_models_[i].append(instance)\n",
    "                instance.fit(X[train_index], y[train_index])\n",
    "                y_pred = instance.predict(X[holdout_index])\n",
    "                out_of_fold_predictions[holdout_index, i] = y_pred\n",
    "                \n",
    "        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n",
    "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "        return self\n",
    "   \n",
    "    #Do the predictions of all base models on the test data and use the averaged predictions as \n",
    "    #meta-features for the final prediction which is done by the meta-model\n",
    "    def predict(self, X):\n",
    "        meta_features = np.column_stack([\n",
    "            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n",
    "            for base_models in self.base_models_ ])\n",
    "        return self.meta_model_.predict(meta_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_models = AveragingModels((XGB, GB, LGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_cv = cv_method(averaged_models, train_data_912, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_averaged_models = StackingAveragedModels(base_models = (XGB, GB, LGB),\n",
    "                                                 meta_model = lasso)\n",
    "\n",
    "#stacked_cv = cv_method(stacked_averaged_models, train_claim_data, train_label)\n",
    "#print(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(stacked_cv.mean(), stacked_cv.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_cv = cv_method(stacked_averaged_models, train_data_912, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(stacked_cv['mae_mean'], stacked_cv['mae_std']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
