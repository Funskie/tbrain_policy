{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, f1_score, accuracy_score, recall_score, precision_score\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingset_path = '../dataset/training-set.csv'\n",
    "testingset_path = '../dataset/testing-set.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_claim_df = pd.read_csv('../dataset/train_claim_df.csv', encoding='utf-8')\n",
    "test_claim_df = pd.read_csv('../dataset/test_claim_df.csv', encoding='utf-8')\n",
    "#train_renew_df = pd.read_csv('../dataset/train_only_renew.csv', encoding='utf-8')\n",
    "trainingset_df = pd.read_csv(trainingset_path, encoding='utf-8')\n",
    "testingset_df = pd.read_csv(testingset_path, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_renew_df = train_claim_df[train_claim_df['Next_Premium'] != 0].copy()\n",
    "train_no_renew_df = train_claim_df[train_claim_df['Next_Premium'] == 0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_claim_data = train_claim_df.iloc[:, 2:]\n",
    "train_label = train_claim_df.iloc[:, 1]\n",
    "\n",
    "train_renew_data = train_renew_df.iloc[:, 2:]\n",
    "train_renew_label = train_renew_df.iloc[:, 1]\n",
    "train_no_renew_data = train_no_renew_df.iloc[:, 2:]\n",
    "train_no_renew_label = train_no_renew_df.iloc[:, 1]\n",
    "\n",
    "test_claim_data = test_claim_df.iloc[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda3\\envs\\AI\\lib\\site-packages\\ipykernel_launcher.py:4: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\Asus\\Anaconda3\\envs\\AI\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.0926244 ],\n",
       "       [ 0.43511441],\n",
       "       [ 0.14749538],\n",
       "       ...,\n",
       "       [ 1.05716133],\n",
       "       [-0.28386413],\n",
       "       [ 0.01134926]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss_x = StandardScaler()\n",
    "ss_x.fit_transform(train_claim_data)\n",
    "ss_y = StandardScaler()\n",
    "ss_y.fit_transform(train_label.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210763, 41)\n",
      "(210763,)\n",
      "(164553, 41)\n",
      "(164553,)\n",
      "(46210, 41)\n",
      "(46210,)\n",
      "(140510, 41)\n"
     ]
    }
   ],
   "source": [
    "print(train_claim_data.shape)\n",
    "print(train_label.shape)\n",
    "print(train_renew_data.shape)\n",
    "print(train_renew_label.shape)\n",
    "print(train_no_renew_data.shape)\n",
    "print(train_no_renew_label.shape)\n",
    "print(test_claim_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classified model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier)\n",
    "import lightgbm as lgb\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_no_renew_data['renew'] = 0\n",
    "train_renew_data['renew'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_renew_data_sample = train_renew_data.sample(n=46210, axis=0, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46210, 42)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_renew_data_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cancellation</th>\n",
       "      <th>Manafactured_Year_and_Month</th>\n",
       "      <th>Engine_Displacement_(Cubic_Centimeter)</th>\n",
       "      <th>qpt</th>\n",
       "      <th>Main_Insurance_Coverage_Group_0</th>\n",
       "      <th>Main_Insurance_Coverage_Group_1</th>\n",
       "      <th>Main_Insurance_Coverage_Group_2</th>\n",
       "      <th>Coverage_Deductible_if_applied</th>\n",
       "      <th>Coverage_Deductible_if_applied_percent</th>\n",
       "      <th>Premium</th>\n",
       "      <th>...</th>\n",
       "      <th>Accident_date</th>\n",
       "      <th>Paid_Loss_Amount</th>\n",
       "      <th>paid_Expenses_Amount</th>\n",
       "      <th>Salvage_or_Subrogation?</th>\n",
       "      <th>At_Fault?</th>\n",
       "      <th>Claim_Status_(close,_open,_reopen_etc)</th>\n",
       "      <th>Deductible</th>\n",
       "      <th>number_of_claimants</th>\n",
       "      <th>Insured_Amount</th>\n",
       "      <th>renew</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100797</th>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>2400</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>17798</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59625500</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158308</th>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>1200</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6904</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4645000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37851</th>\n",
       "      <td>1</td>\n",
       "      <td>2009</td>\n",
       "      <td>1598</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7833</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46677000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2986</th>\n",
       "      <td>1</td>\n",
       "      <td>2005</td>\n",
       "      <td>2349</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13837</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3561000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59933</th>\n",
       "      <td>0</td>\n",
       "      <td>2011</td>\n",
       "      <td>2378</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8062</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23861000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Cancellation  Manafactured_Year_and_Month  \\\n",
       "100797             0                         2014   \n",
       "158308             0                         2009   \n",
       "37851              1                         2009   \n",
       "2986               1                         2005   \n",
       "59933              0                         2011   \n",
       "\n",
       "        Engine_Displacement_(Cubic_Centimeter)  qpt  \\\n",
       "100797                                    2400  5.0   \n",
       "158308                                    1200  2.0   \n",
       "37851                                     1598  5.0   \n",
       "2986                                      2349  5.0   \n",
       "59933                                     2378  5.0   \n",
       "\n",
       "        Main_Insurance_Coverage_Group_0  Main_Insurance_Coverage_Group_1  \\\n",
       "100797                                5                                2   \n",
       "158308                                3                                1   \n",
       "37851                                 6                                0   \n",
       "2986                                  2                                1   \n",
       "59933                                 4                                3   \n",
       "\n",
       "        Main_Insurance_Coverage_Group_2  Coverage_Deductible_if_applied  \\\n",
       "100797                                3                               0   \n",
       "158308                                2                               0   \n",
       "37851                                 1                               0   \n",
       "2986                                  2                               0   \n",
       "59933                                 2                               0   \n",
       "\n",
       "        Coverage_Deductible_if_applied_percent  Premium  ...    Accident_date  \\\n",
       "100797                                       1    17798  ...              0.0   \n",
       "158308                                       1     6904  ...              0.0   \n",
       "37851                                        0     7833  ...              0.0   \n",
       "2986                                         1    13837  ...              0.0   \n",
       "59933                                        1     8062  ...              0.0   \n",
       "\n",
       "        Paid_Loss_Amount  paid_Expenses_Amount  Salvage_or_Subrogation?  \\\n",
       "100797               0.0                   0.0                      0.0   \n",
       "158308               0.0                   0.0                      0.0   \n",
       "37851                0.0                   0.0                      0.0   \n",
       "2986                 0.0                   0.0                      0.0   \n",
       "59933                0.0                   0.0                      0.0   \n",
       "\n",
       "        At_Fault?  Claim_Status_(close,_open,_reopen_etc)  Deductible  \\\n",
       "100797        0.0                                     0.0         0.0   \n",
       "158308        0.0                                     0.0         0.0   \n",
       "37851         0.0                                     0.0         0.0   \n",
       "2986          0.0                                     0.0         0.0   \n",
       "59933         0.0                                     0.0         0.0   \n",
       "\n",
       "        number_of_claimants  Insured_Amount  renew  \n",
       "100797                  0.0        59625500      1  \n",
       "158308                  0.0         4645000      1  \n",
       "37851                   0.0        46677000      1  \n",
       "2986                    0.0         3561000      1  \n",
       "59933                   0.0        23861000      1  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_renew_data = pd.concat([train_renew_data_sample, train_no_renew_data], axis=0)\n",
    "all_train_renew_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73936, 41)\n",
      "(73936,)\n",
      "(18484, 41)\n",
      "(18484,)\n"
     ]
    }
   ],
   "source": [
    "x_train_re, x_val_re, y_train_re, y_val_re = train_test_split(all_train_renew_data.iloc[:,:-1], all_train_renew_data.iloc[:,-1], test_size=0.2, random_state = 7, shuffle=True)\n",
    "print(x_train_re.shape)\n",
    "print(y_train_re.shape)\n",
    "print(x_val_re.shape)\n",
    "print(y_val_re.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_re = ss_x.transform(x_train_re)\n",
    "x_val_re = ss_x.transform(x_val_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "#setting the checkpoint dir and checkpoint model name\n",
    "parent_path = os.path.abspath(\"..\")\n",
    "save_dir = os.path.join(parent_path, 'checkpoints/')\n",
    "model_name = time.strftime('class_%m%d_%H_%M_%S.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = Sequential()\n",
    "nn_model.add(Dense(16, input_dim=41, kernel_initializer='normal', activation='relu'))\n",
    "nn_model.add(BatchNormalization())\n",
    "# nn_model.add(Dense(8, kernel_initializer='normal', activation='relu'))\n",
    "# nn_model.add(BatchNormalization())\n",
    "nn_model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "# Compile model\n",
    "opt = keras.optimizers.Adam(lr=0.01)\n",
    "nn_model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 73936 samples, validate on 18484 samples\n",
      "Epoch 1/100\n",
      "73936/73936 [==============================] - 2s 22us/step - loss: 0.6415 - acc: 0.6185 - val_loss: 0.6287 - val_acc: 0.6398\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.62874, saving model to D:\\fang\\AI_proj\\tbrain_policy\\checkpoints/class_0827_09_27_35.h5\n",
      "Epoch 2/100\n",
      "73936/73936 [==============================] - 1s 18us/step - loss: 0.6207 - acc: 0.6358 - val_loss: 0.6179 - val_acc: 0.6409\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.62874 to 0.61785, saving model to D:\\fang\\AI_proj\\tbrain_policy\\checkpoints/class_0827_09_27_35.h5\n",
      "Epoch 3/100\n",
      "73936/73936 [==============================] - 1s 19us/step - loss: 0.6156 - acc: 0.6381 - val_loss: 0.6209 - val_acc: 0.6378\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/100\n",
      "73936/73936 [==============================] - 1s 19us/step - loss: 0.6141 - acc: 0.6412 - val_loss: 0.6178 - val_acc: 0.6403\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.61785 to 0.61781, saving model to D:\\fang\\AI_proj\\tbrain_policy\\checkpoints/class_0827_09_27_35.h5\n",
      "Epoch 5/100\n",
      "73936/73936 [==============================] - 1s 18us/step - loss: 0.6125 - acc: 0.6410 - val_loss: 0.6167 - val_acc: 0.6424\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.61781 to 0.61670, saving model to D:\\fang\\AI_proj\\tbrain_policy\\checkpoints/class_0827_09_27_35.h5\n",
      "Epoch 6/100\n",
      "73936/73936 [==============================] - 1s 19us/step - loss: 0.6116 - acc: 0.6426 - val_loss: 0.6178 - val_acc: 0.6395\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/100\n",
      "73936/73936 [==============================] - 1s 19us/step - loss: 0.6111 - acc: 0.6446 - val_loss: 0.6142 - val_acc: 0.6417\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.61670 to 0.61416, saving model to D:\\fang\\AI_proj\\tbrain_policy\\checkpoints/class_0827_09_27_35.h5\n",
      "Epoch 8/100\n",
      "73936/73936 [==============================] - 1s 20us/step - loss: 0.6100 - acc: 0.6439 - val_loss: 0.6128 - val_acc: 0.6434\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.61416 to 0.61276, saving model to D:\\fang\\AI_proj\\tbrain_policy\\checkpoints/class_0827_09_27_35.h5\n",
      "Epoch 9/100\n",
      "73936/73936 [==============================] - 1s 20us/step - loss: 0.6090 - acc: 0.6441 - val_loss: 0.6167 - val_acc: 0.6407\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/100\n",
      "73936/73936 [==============================] - 1s 18us/step - loss: 0.6091 - acc: 0.6458 - val_loss: 0.6136 - val_acc: 0.6401\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/100\n",
      "73936/73936 [==============================] - 1s 18us/step - loss: 0.6079 - acc: 0.6462 - val_loss: 0.6097 - val_acc: 0.6424\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.61276 to 0.60968, saving model to D:\\fang\\AI_proj\\tbrain_policy\\checkpoints/class_0827_09_27_35.h5\n",
      "Epoch 12/100\n",
      "73936/73936 [==============================] - 1s 18us/step - loss: 0.6071 - acc: 0.6460 - val_loss: 0.6115 - val_acc: 0.6424\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/100\n",
      "73936/73936 [==============================] - 1s 19us/step - loss: 0.6075 - acc: 0.6460 - val_loss: 0.6138 - val_acc: 0.6420\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/100\n",
      "73936/73936 [==============================] - 1s 18us/step - loss: 0.6073 - acc: 0.6458 - val_loss: 0.6135 - val_acc: 0.6439\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/100\n",
      "73936/73936 [==============================] - 1s 19us/step - loss: 0.6070 - acc: 0.6461 - val_loss: 0.6114 - val_acc: 0.6427\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/100\n",
      "73936/73936 [==============================] - 1s 18us/step - loss: 0.6060 - acc: 0.6468 - val_loss: 0.6103 - val_acc: 0.6413\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.6054 - acc: 0.6477 - val_loss: 0.6128 - val_acc: 0.6398\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.6067 - acc: 0.6463 - val_loss: 0.6115 - val_acc: 0.6444\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/100\n",
      "73936/73936 [==============================] - 1s 18us/step - loss: 0.6061 - acc: 0.6473 - val_loss: 0.6118 - val_acc: 0.6412\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/100\n",
      "73936/73936 [==============================] - 1s 18us/step - loss: 0.6070 - acc: 0.6468 - val_loss: 0.6123 - val_acc: 0.6434\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 21/100\n",
      "73936/73936 [==============================] - 1s 18us/step - loss: 0.6057 - acc: 0.6482 - val_loss: 0.6109 - val_acc: 0.6404\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/100\n",
      "73936/73936 [==============================] - 1s 18us/step - loss: 0.6048 - acc: 0.6490 - val_loss: 0.6115 - val_acc: 0.6450\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.6043 - acc: 0.6476 - val_loss: 0.6107 - val_acc: 0.6403\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 24/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.6051 - acc: 0.6486 - val_loss: 0.6104 - val_acc: 0.6401\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 25/100\n",
      "73936/73936 [==============================] - 1s 18us/step - loss: 0.6048 - acc: 0.6485 - val_loss: 0.6093 - val_acc: 0.6417\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.60968 to 0.60934, saving model to D:\\fang\\AI_proj\\tbrain_policy\\checkpoints/class_0827_09_27_35.h5\n",
      "Epoch 26/100\n",
      "73936/73936 [==============================] - 1s 18us/step - loss: 0.6048 - acc: 0.6486 - val_loss: 0.6135 - val_acc: 0.6446\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 27/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.6042 - acc: 0.6480 - val_loss: 0.6077 - val_acc: 0.6444\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.60934 to 0.60775, saving model to D:\\fang\\AI_proj\\tbrain_policy\\checkpoints/class_0827_09_27_35.h5\n",
      "Epoch 28/100\n",
      "73936/73936 [==============================] - 1s 18us/step - loss: 0.6043 - acc: 0.6472 - val_loss: 0.6091 - val_acc: 0.6449\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      "Epoch 29/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.6042 - acc: 0.6490 - val_loss: 0.6152 - val_acc: 0.6390\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 30/100\n",
      "73936/73936 [==============================] - 1s 18us/step - loss: 0.6043 - acc: 0.6472 - val_loss: 0.6082 - val_acc: 0.6428\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      "Epoch 31/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.6033 - acc: 0.6494 - val_loss: 0.6095 - val_acc: 0.6453\n",
      "\n",
      "Epoch 00031: val_loss did not improve\n",
      "Epoch 32/100\n",
      "73936/73936 [==============================] - 1s 18us/step - loss: 0.6021 - acc: 0.6486 - val_loss: 0.6150 - val_acc: 0.6427\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 33/100\n",
      "73936/73936 [==============================] - 1s 18us/step - loss: 0.6026 - acc: 0.6481 - val_loss: 0.6077 - val_acc: 0.6444\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.60775 to 0.60774, saving model to D:\\fang\\AI_proj\\tbrain_policy\\checkpoints/class_0827_09_27_35.h5\n",
      "Epoch 34/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.6031 - acc: 0.6491 - val_loss: 0.6092 - val_acc: 0.6453\n",
      "\n",
      "Epoch 00034: val_loss did not improve\n",
      "Epoch 35/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.6022 - acc: 0.6499 - val_loss: 0.6097 - val_acc: 0.6446\n",
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 36/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.6022 - acc: 0.6497 - val_loss: 0.6091 - val_acc: 0.6436\n",
      "\n",
      "Epoch 00036: val_loss did not improve\n",
      "Epoch 37/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.6029 - acc: 0.6480 - val_loss: 0.6065 - val_acc: 0.6448\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.60774 to 0.60649, saving model to D:\\fang\\AI_proj\\tbrain_policy\\checkpoints/class_0827_09_27_35.h5\n",
      "Epoch 38/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.6008 - acc: 0.6494 - val_loss: 0.6076 - val_acc: 0.6459\n",
      "\n",
      "Epoch 00038: val_loss did not improve\n",
      "Epoch 39/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.6010 - acc: 0.6506 - val_loss: 0.6078 - val_acc: 0.6459\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 40/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.6005 - acc: 0.6495 - val_loss: 0.6052 - val_acc: 0.6448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00040: val_loss improved from 0.60649 to 0.60521, saving model to D:\\fang\\AI_proj\\tbrain_policy\\checkpoints/class_0827_09_27_35.h5\n",
      "Epoch 41/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.6001 - acc: 0.6500 - val_loss: 0.6095 - val_acc: 0.6418\n",
      "\n",
      "Epoch 00041: val_loss did not improve\n",
      "Epoch 42/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.5994 - acc: 0.6507 - val_loss: 0.6063 - val_acc: 0.6416\n",
      "\n",
      "Epoch 00042: val_loss did not improve\n",
      "Epoch 43/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.6022 - acc: 0.6512 - val_loss: 0.6078 - val_acc: 0.6450\n",
      "\n",
      "Epoch 00043: val_loss did not improve\n",
      "Epoch 44/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.6000 - acc: 0.6497 - val_loss: 0.6063 - val_acc: 0.6437\n",
      "\n",
      "Epoch 00044: val_loss did not improve\n",
      "Epoch 45/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.6004 - acc: 0.6502 - val_loss: 0.6071 - val_acc: 0.6452\n",
      "\n",
      "Epoch 00045: val_loss did not improve\n",
      "Epoch 46/100\n",
      "73936/73936 [==============================] - 1s 18us/step - loss: 0.6004 - acc: 0.6490 - val_loss: 0.6054 - val_acc: 0.6448\n",
      "\n",
      "Epoch 00046: val_loss did not improve\n",
      "Epoch 47/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.5992 - acc: 0.6509 - val_loss: 0.6053 - val_acc: 0.6439\n",
      "\n",
      "Epoch 00047: val_loss did not improve\n",
      "Epoch 48/100\n",
      "73936/73936 [==============================] - 1s 18us/step - loss: 0.5992 - acc: 0.6509 - val_loss: 0.6045 - val_acc: 0.6453\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.60521 to 0.60451, saving model to D:\\fang\\AI_proj\\tbrain_policy\\checkpoints/class_0827_09_27_35.h5\n",
      "Epoch 49/100\n",
      "73936/73936 [==============================] - 1s 18us/step - loss: 0.6003 - acc: 0.6504 - val_loss: 0.6090 - val_acc: 0.6415\n",
      "\n",
      "Epoch 00049: val_loss did not improve\n",
      "Epoch 50/100\n",
      "73936/73936 [==============================] - 1s 18us/step - loss: 0.5986 - acc: 0.6500 - val_loss: 0.6059 - val_acc: 0.6423\n",
      "\n",
      "Epoch 00050: val_loss did not improve\n",
      "Epoch 51/100\n",
      "73936/73936 [==============================] - 1s 18us/step - loss: 0.5998 - acc: 0.6515 - val_loss: 0.6050 - val_acc: 0.6442\n",
      "\n",
      "Epoch 00051: val_loss did not improve\n",
      "Epoch 52/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.5994 - acc: 0.6508 - val_loss: 0.6085 - val_acc: 0.6422\n",
      "\n",
      "Epoch 00052: val_loss did not improve\n",
      "Epoch 53/100\n",
      "73936/73936 [==============================] - 1s 18us/step - loss: 0.5986 - acc: 0.6523 - val_loss: 0.6064 - val_acc: 0.6456\n",
      "\n",
      "Epoch 00053: val_loss did not improve\n",
      "Epoch 54/100\n",
      "73936/73936 [==============================] - 1s 19us/step - loss: 0.5979 - acc: 0.6515 - val_loss: 0.6045 - val_acc: 0.6469\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.60451 to 0.60448, saving model to D:\\fang\\AI_proj\\tbrain_policy\\checkpoints/class_0827_09_27_35.h5\n",
      "Epoch 55/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.5987 - acc: 0.6521 - val_loss: 0.6059 - val_acc: 0.6419\n",
      "\n",
      "Epoch 00055: val_loss did not improve\n",
      "Epoch 56/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.5992 - acc: 0.6505 - val_loss: 0.6049 - val_acc: 0.6454\n",
      "\n",
      "Epoch 00056: val_loss did not improve\n",
      "Epoch 57/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.5981 - acc: 0.6521 - val_loss: 0.6078 - val_acc: 0.6446\n",
      "\n",
      "Epoch 00057: val_loss did not improve\n",
      "Epoch 58/100\n",
      "73936/73936 [==============================] - 1s 19us/step - loss: 0.5985 - acc: 0.6522 - val_loss: 0.6051 - val_acc: 0.6462\n",
      "\n",
      "Epoch 00058: val_loss did not improve\n",
      "Epoch 59/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.5980 - acc: 0.6525 - val_loss: 0.6063 - val_acc: 0.6443\n",
      "\n",
      "Epoch 00059: val_loss did not improve\n",
      "Epoch 60/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.5990 - acc: 0.6518 - val_loss: 0.6073 - val_acc: 0.6419\n",
      "\n",
      "Epoch 00060: val_loss did not improve\n",
      "Epoch 61/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.5981 - acc: 0.6507 - val_loss: 0.6090 - val_acc: 0.6421\n",
      "\n",
      "Epoch 00061: val_loss did not improve\n",
      "Epoch 62/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.5981 - acc: 0.6511 - val_loss: 0.6050 - val_acc: 0.6416\n",
      "\n",
      "Epoch 00062: val_loss did not improve\n",
      "Epoch 63/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.5979 - acc: 0.6510 - val_loss: 0.6055 - val_acc: 0.6453\n",
      "\n",
      "Epoch 00063: val_loss did not improve\n",
      "Epoch 64/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.5981 - acc: 0.6514 - val_loss: 0.6113 - val_acc: 0.6432\n",
      "\n",
      "Epoch 00064: val_loss did not improve\n",
      "Epoch 65/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.6005 - acc: 0.6495 - val_loss: 0.6079 - val_acc: 0.6413\n",
      "\n",
      "Epoch 00065: val_loss did not improve\n",
      "Epoch 66/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.5997 - acc: 0.6501 - val_loss: 0.6109 - val_acc: 0.6413\n",
      "\n",
      "Epoch 00066: val_loss did not improve\n",
      "Epoch 67/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.5986 - acc: 0.6511 - val_loss: 0.6044 - val_acc: 0.6434\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.60448 to 0.60436, saving model to D:\\fang\\AI_proj\\tbrain_policy\\checkpoints/class_0827_09_27_35.h5\n",
      "Epoch 68/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.5980 - acc: 0.6516 - val_loss: 0.6067 - val_acc: 0.6443\n",
      "\n",
      "Epoch 00068: val_loss did not improve\n",
      "Epoch 69/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.5977 - acc: 0.6521 - val_loss: 0.6067 - val_acc: 0.6434\n",
      "\n",
      "Epoch 00069: val_loss did not improve\n",
      "Epoch 70/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.5995 - acc: 0.6508 - val_loss: 0.6096 - val_acc: 0.6390\n",
      "\n",
      "Epoch 00070: val_loss did not improve\n",
      "Epoch 71/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.6000 - acc: 0.6491 - val_loss: 0.6063 - val_acc: 0.6459\n",
      "\n",
      "Epoch 00071: val_loss did not improve\n",
      "Epoch 72/100\n",
      "73936/73936 [==============================] - 1s 18us/step - loss: 0.5983 - acc: 0.6523 - val_loss: 0.6062 - val_acc: 0.6474\n",
      "\n",
      "Epoch 00072: val_loss did not improve\n",
      "Epoch 73/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.5984 - acc: 0.6518 - val_loss: 0.6060 - val_acc: 0.6446\n",
      "\n",
      "Epoch 00073: val_loss did not improve\n",
      "Epoch 74/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.5978 - acc: 0.6510 - val_loss: 0.6053 - val_acc: 0.6427\n",
      "\n",
      "Epoch 00074: val_loss did not improve\n",
      "Epoch 75/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.5988 - acc: 0.6512 - val_loss: 0.6090 - val_acc: 0.6416\n",
      "\n",
      "Epoch 00075: val_loss did not improve\n",
      "Epoch 76/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.5986 - acc: 0.6518 - val_loss: 0.6071 - val_acc: 0.6433\n",
      "\n",
      "Epoch 00076: val_loss did not improve\n",
      "Epoch 77/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.5976 - acc: 0.6525 - val_loss: 0.6045 - val_acc: 0.6444\n",
      "\n",
      "Epoch 00077: val_loss did not improve\n",
      "Epoch 78/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.5980 - acc: 0.6510 - val_loss: 0.6079 - val_acc: 0.6445\n",
      "\n",
      "Epoch 00078: val_loss did not improve\n",
      "Epoch 79/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.5978 - acc: 0.6518 - val_loss: 0.6056 - val_acc: 0.6436\n",
      "\n",
      "Epoch 00079: val_loss did not improve\n",
      "Epoch 80/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.5991 - acc: 0.6505 - val_loss: 0.6075 - val_acc: 0.6447\n",
      "\n",
      "Epoch 00080: val_loss did not improve\n",
      "Epoch 81/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.5980 - acc: 0.6523 - val_loss: 0.6075 - val_acc: 0.6424\n",
      "\n",
      "Epoch 00081: val_loss did not improve\n",
      "Epoch 82/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.5971 - acc: 0.6526 - val_loss: 0.6059 - val_acc: 0.6442\n",
      "\n",
      "Epoch 00082: val_loss did not improve\n",
      "Epoch 83/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.5985 - acc: 0.6514 - val_loss: 0.6060 - val_acc: 0.6442\n",
      "\n",
      "Epoch 00083: val_loss did not improve\n",
      "Epoch 84/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.5978 - acc: 0.6511 - val_loss: 0.6060 - val_acc: 0.6452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00084: val_loss did not improve\n",
      "Epoch 85/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.5974 - acc: 0.6515 - val_loss: 0.6100 - val_acc: 0.6413\n",
      "\n",
      "Epoch 00085: val_loss did not improve\n",
      "Epoch 86/100\n",
      "73936/73936 [==============================] - 1s 19us/step - loss: 0.5976 - acc: 0.6526 - val_loss: 0.6046 - val_acc: 0.6455\n",
      "\n",
      "Epoch 00086: val_loss did not improve\n",
      "Epoch 87/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.5980 - acc: 0.6520 - val_loss: 0.6049 - val_acc: 0.6414\n",
      "\n",
      "Epoch 00087: val_loss did not improve\n",
      "Epoch 88/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.5979 - acc: 0.6524 - val_loss: 0.6063 - val_acc: 0.6456\n",
      "\n",
      "Epoch 00088: val_loss did not improve\n",
      "Epoch 89/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.5972 - acc: 0.6531 - val_loss: 0.6059 - val_acc: 0.6417\n",
      "\n",
      "Epoch 00089: val_loss did not improve\n",
      "Epoch 90/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.5984 - acc: 0.6506 - val_loss: 0.6057 - val_acc: 0.6455\n",
      "\n",
      "Epoch 00090: val_loss did not improve\n",
      "Epoch 91/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.5984 - acc: 0.6506 - val_loss: 0.6053 - val_acc: 0.6447\n",
      "\n",
      "Epoch 00091: val_loss did not improve\n",
      "Epoch 92/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.5979 - acc: 0.6503 - val_loss: 0.6044 - val_acc: 0.6449\n",
      "\n",
      "Epoch 00092: val_loss did not improve\n",
      "Epoch 93/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.5978 - acc: 0.6519 - val_loss: 0.6071 - val_acc: 0.6420\n",
      "\n",
      "Epoch 00093: val_loss did not improve\n",
      "Epoch 94/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.5971 - acc: 0.6527 - val_loss: 0.6037 - val_acc: 0.6452\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.60436 to 0.60368, saving model to D:\\fang\\AI_proj\\tbrain_policy\\checkpoints/class_0827_09_27_35.h5\n",
      "Epoch 95/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.5981 - acc: 0.6528 - val_loss: 0.6062 - val_acc: 0.6455\n",
      "\n",
      "Epoch 00095: val_loss did not improve\n",
      "Epoch 96/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.5974 - acc: 0.6521 - val_loss: 0.6087 - val_acc: 0.6458\n",
      "\n",
      "Epoch 00096: val_loss did not improve\n",
      "Epoch 97/100\n",
      "73936/73936 [==============================] - 1s 18us/step - loss: 0.5980 - acc: 0.6526 - val_loss: 0.6046 - val_acc: 0.6452\n",
      "\n",
      "Epoch 00097: val_loss did not improve\n",
      "Epoch 98/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.5982 - acc: 0.6522 - val_loss: 0.6133 - val_acc: 0.6458\n",
      "\n",
      "Epoch 00098: val_loss did not improve\n",
      "Epoch 99/100\n",
      "73936/73936 [==============================] - 1s 17us/step - loss: 0.5973 - acc: 0.6537 - val_loss: 0.6053 - val_acc: 0.6444\n",
      "\n",
      "Epoch 00099: val_loss did not improve\n",
      "Epoch 100/100\n",
      "73936/73936 [==============================] - 1s 18us/step - loss: 0.5970 - acc: 0.6517 - val_loss: 0.6068 - val_acc: 0.6411\n",
      "\n",
      "Epoch 00100: val_loss did not improve\n"
     ]
    }
   ],
   "source": [
    "earlystop = EarlyStopping(monitor='val_loss', patience=30, verbose=1)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "checkpoint = ModelCheckpoint(model_path, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "model_history = nn_model.fit(x_train_re, y_train_re, validation_data=(x_val_re, y_val_re), batch_size=500, epochs=100, shuffle=True,\n",
    "                            callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_loss = model_history.history['loss']\n",
    "val_loss = model_history.history['val_loss']\n",
    "\n",
    "plt.plot(training_loss, label=\"training_loss\")\n",
    "plt.plot(val_loss, label=\"validation_loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "best_nn_model = load_model('../checkpoints/class_0827_09_27_35.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_nn = best_nn_model.predict(ss_x.transform(test_claim_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-error:0.353546\ttest-error:0.361867\n",
      "Multiple eval metrics have been passed: 'test-error' will be used for early stopping.\n",
      "\n",
      "Will train until test-error hasn't improved in 50 rounds.\n",
      "[1]\ttrain-error:0.349583\ttest-error:0.358783\n",
      "[2]\ttrain-error:0.346584\ttest-error:0.355889\n",
      "[3]\ttrain-error:0.345394\ttest-error:0.354861\n",
      "[4]\ttrain-error:0.343496\ttest-error:0.354888\n",
      "[5]\ttrain-error:0.343111\ttest-error:0.354564\n",
      "[6]\ttrain-error:0.343588\ttest-error:0.354428\n",
      "[7]\ttrain-error:0.343053\ttest-error:0.35455\n",
      "[8]\ttrain-error:0.342032\ttest-error:0.354103\n",
      "[9]\ttrain-error:0.341566\ttest-error:0.353535\n",
      "[10]\ttrain-error:0.341265\ttest-error:0.353143\n",
      "[11]\ttrain-error:0.341288\ttest-error:0.35267\n",
      "[12]\ttrain-error:0.34067\ttest-error:0.352602\n",
      "[13]\ttrain-error:0.340727\ttest-error:0.353224\n",
      "[14]\ttrain-error:0.340612\ttest-error:0.352372\n",
      "[15]\ttrain-error:0.340436\ttest-error:0.352589\n",
      "[16]\ttrain-error:0.340704\ttest-error:0.352886\n",
      "[17]\ttrain-error:0.340284\ttest-error:0.352237\n",
      "[18]\ttrain-error:0.340558\ttest-error:0.352562\n",
      "[19]\ttrain-error:0.340149\ttest-error:0.35248\n",
      "[20]\ttrain-error:0.340051\ttest-error:0.352413\n",
      "[21]\ttrain-error:0.339851\ttest-error:0.351669\n",
      "[22]\ttrain-error:0.339821\ttest-error:0.351764\n",
      "[23]\ttrain-error:0.339811\ttest-error:0.351493\n",
      "[24]\ttrain-error:0.339912\ttest-error:0.351655\n",
      "[25]\ttrain-error:0.339645\ttest-error:0.351304\n",
      "[26]\ttrain-error:0.339655\ttest-error:0.351358\n",
      "[27]\ttrain-error:0.339669\ttest-error:0.351385\n",
      "[28]\ttrain-error:0.339638\ttest-error:0.351223\n",
      "[29]\ttrain-error:0.339533\ttest-error:0.351169\n",
      "[30]\ttrain-error:0.339419\ttest-error:0.351655\n",
      "[31]\ttrain-error:0.339327\ttest-error:0.351331\n",
      "[32]\ttrain-error:0.339294\ttest-error:0.351683\n",
      "[33]\ttrain-error:0.339165\ttest-error:0.351399\n",
      "[34]\ttrain-error:0.339314\ttest-error:0.35171\n",
      "[35]\ttrain-error:0.339232\ttest-error:0.35194\n",
      "[36]\ttrain-error:0.339185\ttest-error:0.351832\n",
      "[37]\ttrain-error:0.339053\ttest-error:0.351547\n",
      "[38]\ttrain-error:0.338945\ttest-error:0.351412\n",
      "[39]\ttrain-error:0.338726\ttest-error:0.351142\n",
      "[40]\ttrain-error:0.3386\ttest-error:0.351453\n",
      "[41]\ttrain-error:0.338353\ttest-error:0.351263\n",
      "[42]\ttrain-error:0.33835\ttest-error:0.351331\n",
      "[43]\ttrain-error:0.338232\ttest-error:0.351196\n",
      "[44]\ttrain-error:0.338276\ttest-error:0.351196\n",
      "[45]\ttrain-error:0.338036\ttest-error:0.35129\n",
      "[46]\ttrain-error:0.338073\ttest-error:0.351263\n",
      "[47]\ttrain-error:0.33808\ttest-error:0.351006\n",
      "[48]\ttrain-error:0.338046\ttest-error:0.351196\n",
      "[49]\ttrain-error:0.338171\ttest-error:0.350979\n",
      "[50]\ttrain-error:0.33813\ttest-error:0.351088\n",
      "[51]\ttrain-error:0.337937\ttest-error:0.350925\n",
      "[52]\ttrain-error:0.337887\ttest-error:0.351115\n",
      "[53]\ttrain-error:0.33785\ttest-error:0.351006\n",
      "[54]\ttrain-error:0.337701\ttest-error:0.351087\n",
      "[55]\ttrain-error:0.337735\ttest-error:0.351115\n",
      "[56]\ttrain-error:0.337789\ttest-error:0.350803\n",
      "[57]\ttrain-error:0.337724\ttest-error:0.351196\n",
      "[58]\ttrain-error:0.337664\ttest-error:0.350979\n",
      "[59]\ttrain-error:0.337556\ttest-error:0.350831\n",
      "[60]\ttrain-error:0.337474\ttest-error:0.351033\n",
      "[61]\ttrain-error:0.337363\ttest-error:0.350993\n",
      "[62]\ttrain-error:0.337349\ttest-error:0.351074\n",
      "[63]\ttrain-error:0.337282\ttest-error:0.35102\n",
      "[64]\ttrain-error:0.337251\ttest-error:0.350993\n",
      "[65]\ttrain-error:0.337217\ttest-error:0.350979\n",
      "[66]\ttrain-error:0.337251\ttest-error:0.35079\n",
      "[67]\ttrain-error:0.337312\ttest-error:0.350438\n",
      "[68]\ttrain-error:0.33713\ttest-error:0.350492\n",
      "[69]\ttrain-error:0.337089\ttest-error:0.350438\n",
      "[70]\ttrain-error:0.337197\ttest-error:0.350668\n",
      "[71]\ttrain-error:0.337055\ttest-error:0.350803\n",
      "[72]\ttrain-error:0.336788\ttest-error:0.350654\n",
      "[73]\ttrain-error:0.337008\ttest-error:0.350682\n",
      "[74]\ttrain-error:0.336822\ttest-error:0.350587\n",
      "[75]\ttrain-error:0.336737\ttest-error:0.350587\n",
      "[76]\ttrain-error:0.336562\ttest-error:0.350614\n",
      "[77]\ttrain-error:0.336504\ttest-error:0.350546\n",
      "[78]\ttrain-error:0.33645\ttest-error:0.350546\n",
      "[79]\ttrain-error:0.336443\ttest-error:0.350573\n",
      "[80]\ttrain-error:0.33651\ttest-error:0.350452\n",
      "[81]\ttrain-error:0.336348\ttest-error:0.35037\n",
      "[82]\ttrain-error:0.336298\ttest-error:0.35029\n",
      "[83]\ttrain-error:0.336216\ttest-error:0.350519\n",
      "[84]\ttrain-error:0.33622\ttest-error:0.350425\n",
      "[85]\ttrain-error:0.336243\ttest-error:0.350411\n",
      "[86]\ttrain-error:0.336108\ttest-error:0.350357\n",
      "[87]\ttrain-error:0.336129\ttest-error:0.350276\n",
      "[88]\ttrain-error:0.336075\ttest-error:0.350668\n",
      "[89]\ttrain-error:0.336098\ttest-error:0.350736\n",
      "[90]\ttrain-error:0.336013\ttest-error:0.350533\n",
      "[91]\ttrain-error:0.335865\ttest-error:0.350344\n",
      "[92]\ttrain-error:0.335878\ttest-error:0.350533\n",
      "[93]\ttrain-error:0.335818\ttest-error:0.350438\n",
      "[94]\ttrain-error:0.335794\ttest-error:0.3506\n",
      "[95]\ttrain-error:0.335689\ttest-error:0.350546\n",
      "[96]\ttrain-error:0.335655\ttest-error:0.350479\n",
      "[97]\ttrain-error:0.335672\ttest-error:0.350384\n",
      "[98]\ttrain-error:0.335709\ttest-error:0.3501\n",
      "[99]\ttrain-error:0.335564\ttest-error:0.350154\n",
      "[100]\ttrain-error:0.33554\ttest-error:0.350303\n",
      "[101]\ttrain-error:0.335469\ttest-error:0.350303\n",
      "[102]\ttrain-error:0.335469\ttest-error:0.350506\n",
      "[103]\ttrain-error:0.335429\ttest-error:0.350519\n",
      "[104]\ttrain-error:0.335327\ttest-error:0.350465\n",
      "[105]\ttrain-error:0.335253\ttest-error:0.350709\n",
      "[106]\ttrain-error:0.335351\ttest-error:0.35083\n",
      "[107]\ttrain-error:0.335185\ttest-error:0.350628\n",
      "[108]\ttrain-error:0.335107\ttest-error:0.350492\n",
      "[109]\ttrain-error:0.335016\ttest-error:0.35056\n",
      "[110]\ttrain-error:0.33505\ttest-error:0.350506\n",
      "[111]\ttrain-error:0.335053\ttest-error:0.350546\n",
      "[112]\ttrain-error:0.335023\ttest-error:0.350573\n",
      "[113]\ttrain-error:0.334861\ttest-error:0.350695\n",
      "[114]\ttrain-error:0.334779\ttest-error:0.350709\n",
      "[115]\ttrain-error:0.334688\ttest-error:0.350573\n",
      "[116]\ttrain-error:0.334624\ttest-error:0.350492\n",
      "[117]\ttrain-error:0.334577\ttest-error:0.350533\n",
      "[118]\ttrain-error:0.334461\ttest-error:0.350303\n",
      "[119]\ttrain-error:0.334377\ttest-error:0.350317\n",
      "[120]\ttrain-error:0.334228\ttest-error:0.350384\n",
      "[121]\ttrain-error:0.334306\ttest-error:0.350154\n",
      "[122]\ttrain-error:0.334289\ttest-error:0.350141\n",
      "[123]\ttrain-error:0.33434\ttest-error:0.350222\n",
      "[124]\ttrain-error:0.334269\ttest-error:0.35033\n",
      "[125]\ttrain-error:0.334184\ttest-error:0.350222\n",
      "[126]\ttrain-error:0.334147\ttest-error:0.350195\n",
      "[127]\ttrain-error:0.333985\ttest-error:0.350384\n",
      "[128]\ttrain-error:0.333934\ttest-error:0.350087\n",
      "[129]\ttrain-error:0.333772\ttest-error:0.350208\n",
      "[130]\ttrain-error:0.333792\ttest-error:0.350154\n",
      "[131]\ttrain-error:0.333775\ttest-error:0.350235\n",
      "[132]\ttrain-error:0.333748\ttest-error:0.349965\n",
      "[133]\ttrain-error:0.333633\ttest-error:0.349884\n",
      "[134]\ttrain-error:0.333474\ttest-error:0.349965\n",
      "[135]\ttrain-error:0.333478\ttest-error:0.350168\n",
      "[136]\ttrain-error:0.333403\ttest-error:0.350276\n",
      "[137]\ttrain-error:0.333434\ttest-error:0.350317\n",
      "[138]\ttrain-error:0.333248\ttest-error:0.350019\n",
      "[139]\ttrain-error:0.333305\ttest-error:0.350195\n",
      "[140]\ttrain-error:0.333275\ttest-error:0.350276\n",
      "[141]\ttrain-error:0.333184\ttest-error:0.350208\n",
      "[142]\ttrain-error:0.33316\ttest-error:0.350181\n",
      "[143]\ttrain-error:0.333065\ttest-error:0.3501\n",
      "[144]\ttrain-error:0.332974\ttest-error:0.350141\n",
      "[145]\ttrain-error:0.332981\ttest-error:0.3501\n",
      "[146]\ttrain-error:0.332906\ttest-error:0.350262\n",
      "[147]\ttrain-error:0.332784\ttest-error:0.350154\n",
      "[148]\ttrain-error:0.332798\ttest-error:0.350195\n",
      "[149]\ttrain-error:0.332815\ttest-error:0.35006\n",
      "[150]\ttrain-error:0.332727\ttest-error:0.350195\n",
      "[151]\ttrain-error:0.332625\ttest-error:0.349911\n",
      "[152]\ttrain-error:0.332585\ttest-error:0.34987\n",
      "[153]\ttrain-error:0.3325\ttest-error:0.350019\n",
      "[154]\ttrain-error:0.332372\ttest-error:0.349992\n",
      "[155]\ttrain-error:0.332281\ttest-error:0.349924\n",
      "[156]\ttrain-error:0.332226\ttest-error:0.349938\n",
      "[157]\ttrain-error:0.332129\ttest-error:0.349816\n",
      "[158]\ttrain-error:0.332091\ttest-error:0.349911\n",
      "[159]\ttrain-error:0.331973\ttest-error:0.350046\n",
      "[160]\ttrain-error:0.331966\ttest-error:0.350073\n",
      "[161]\ttrain-error:0.331956\ttest-error:0.3501\n",
      "[162]\ttrain-error:0.331912\ttest-error:0.350006\n",
      "[163]\ttrain-error:0.331773\ttest-error:0.349978\n",
      "[164]\ttrain-error:0.331675\ttest-error:0.350154\n",
      "[165]\ttrain-error:0.331662\ttest-error:0.349965\n",
      "[166]\ttrain-error:0.331608\ttest-error:0.350046\n",
      "[167]\ttrain-error:0.331425\ttest-error:0.349884\n",
      "[168]\ttrain-error:0.331334\ttest-error:0.34987\n",
      "[169]\ttrain-error:0.331324\ttest-error:0.349803\n",
      "[170]\ttrain-error:0.331239\ttest-error:0.34987\n",
      "[171]\ttrain-error:0.331192\ttest-error:0.349951\n",
      "[172]\ttrain-error:0.331178\ttest-error:0.349667\n",
      "[173]\ttrain-error:0.331111\ttest-error:0.34983\n",
      "[174]\ttrain-error:0.330975\ttest-error:0.349722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[175]\ttrain-error:0.330982\ttest-error:0.349748\n",
      "[176]\ttrain-error:0.330861\ttest-error:0.349897\n",
      "[177]\ttrain-error:0.33085\ttest-error:0.349924\n",
      "[178]\ttrain-error:0.330769\ttest-error:0.350059\n",
      "[179]\ttrain-error:0.330701\ttest-error:0.349884\n",
      "[180]\ttrain-error:0.330675\ttest-error:0.34987\n",
      "[181]\ttrain-error:0.330617\ttest-error:0.349789\n",
      "[182]\ttrain-error:0.330576\ttest-error:0.349762\n",
      "[183]\ttrain-error:0.330536\ttest-error:0.349816\n",
      "[184]\ttrain-error:0.330472\ttest-error:0.349735\n",
      "[185]\ttrain-error:0.330333\ttest-error:0.349789\n",
      "[186]\ttrain-error:0.330286\ttest-error:0.349735\n",
      "[187]\ttrain-error:0.330252\ttest-error:0.349667\n",
      "[188]\ttrain-error:0.330137\ttest-error:0.349654\n",
      "[189]\ttrain-error:0.33011\ttest-error:0.349613\n",
      "[190]\ttrain-error:0.330019\ttest-error:0.349721\n",
      "[191]\ttrain-error:0.329954\ttest-error:0.349478\n",
      "[192]\ttrain-error:0.32989\ttest-error:0.349518\n",
      "[193]\ttrain-error:0.329914\ttest-error:0.349572\n",
      "[194]\ttrain-error:0.329897\ttest-error:0.349491\n",
      "[195]\ttrain-error:0.329728\ttest-error:0.349518\n",
      "[196]\ttrain-error:0.329714\ttest-error:0.349424\n",
      "[197]\ttrain-error:0.329616\ttest-error:0.349424\n",
      "[198]\ttrain-error:0.329626\ttest-error:0.34941\n",
      "[199]\ttrain-error:0.329542\ttest-error:0.349559\n",
      "[200]\ttrain-error:0.329505\ttest-error:0.349424\n",
      "[201]\ttrain-error:0.329386\ttest-error:0.349478\n",
      "[202]\ttrain-error:0.329325\ttest-error:0.349397\n",
      "[203]\ttrain-error:0.329291\ttest-error:0.349545\n",
      "[204]\ttrain-error:0.329295\ttest-error:0.349627\n",
      "[205]\ttrain-error:0.329329\ttest-error:0.349532\n",
      "[206]\ttrain-error:0.329234\ttest-error:0.349573\n",
      "[207]\ttrain-error:0.329173\ttest-error:0.349667\n",
      "[208]\ttrain-error:0.32916\ttest-error:0.349735\n",
      "[209]\ttrain-error:0.328997\ttest-error:0.349748\n",
      "[210]\ttrain-error:0.329048\ttest-error:0.349789\n",
      "[211]\ttrain-error:0.328883\ttest-error:0.349681\n",
      "[212]\ttrain-error:0.328805\ttest-error:0.349816\n",
      "[213]\ttrain-error:0.328761\ttest-error:0.349951\n",
      "[214]\ttrain-error:0.328798\ttest-error:0.349924\n",
      "[215]\ttrain-error:0.328686\ttest-error:0.349979\n",
      "[216]\ttrain-error:0.328595\ttest-error:0.349978\n",
      "[217]\ttrain-error:0.328531\ttest-error:0.349843\n",
      "[218]\ttrain-error:0.328443\ttest-error:0.349843\n",
      "[219]\ttrain-error:0.328362\ttest-error:0.349965\n",
      "[220]\ttrain-error:0.328284\ttest-error:0.349992\n",
      "[221]\ttrain-error:0.328277\ttest-error:0.349992\n",
      "[222]\ttrain-error:0.328176\ttest-error:0.349992\n",
      "[223]\ttrain-error:0.328139\ttest-error:0.349979\n",
      "[224]\ttrain-error:0.328162\ttest-error:0.350033\n",
      "[225]\ttrain-error:0.328145\ttest-error:0.350141\n",
      "[226]\ttrain-error:0.328074\ttest-error:0.349978\n",
      "[227]\ttrain-error:0.327993\ttest-error:0.350087\n",
      "[228]\ttrain-error:0.327895\ttest-error:0.350127\n",
      "[229]\ttrain-error:0.327899\ttest-error:0.349992\n",
      "[230]\ttrain-error:0.327929\ttest-error:0.350154\n",
      "[231]\ttrain-error:0.327723\ttest-error:0.350168\n",
      "[232]\ttrain-error:0.327692\ttest-error:0.3501\n",
      "[233]\ttrain-error:0.32774\ttest-error:0.349978\n",
      "[234]\ttrain-error:0.327723\ttest-error:0.349803\n",
      "[235]\ttrain-error:0.327621\ttest-error:0.349911\n",
      "[236]\ttrain-error:0.327584\ttest-error:0.349965\n",
      "[237]\ttrain-error:0.32755\ttest-error:0.350033\n",
      "[238]\ttrain-error:0.327489\ttest-error:0.350127\n",
      "[239]\ttrain-error:0.327533\ttest-error:0.349992\n",
      "[240]\ttrain-error:0.327462\ttest-error:0.349965\n",
      "[241]\ttrain-error:0.327381\ttest-error:0.349924\n",
      "[242]\ttrain-error:0.32733\ttest-error:0.350006\n",
      "[243]\ttrain-error:0.327259\ttest-error:0.350046\n",
      "[244]\ttrain-error:0.327232\ttest-error:0.350033\n",
      "[245]\ttrain-error:0.327212\ttest-error:0.349965\n",
      "[246]\ttrain-error:0.327165\ttest-error:0.349992\n",
      "[247]\ttrain-error:0.327107\ttest-error:0.349884\n",
      "[248]\ttrain-error:0.327023\ttest-error:0.349911\n",
      "[249]\ttrain-error:0.326962\ttest-error:0.350006\n",
      "[250]\ttrain-error:0.326982\ttest-error:0.350046\n",
      "[251]\ttrain-error:0.326904\ttest-error:0.350033\n",
      "[252]\ttrain-error:0.326867\ttest-error:0.350046\n",
      "Stopping. Best iteration:\n",
      "[202]\ttrain-error:0.329325+0.000676531\ttest-error:0.349397+0.00190523\n",
      "\n",
      "Best round num:  202\n"
     ]
    }
   ],
   "source": [
    "num_rounds = 5000\n",
    "params = {\n",
    "    'eta': 0.01,\n",
    "    'objective': 'binary:logistic',\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'min_child_weight': 1.1,\n",
    "    'max_depth': 7,\n",
    "}\n",
    "xgtrain = xgb.DMatrix(x_train_re, label=y_train_re)\n",
    "#求出最佳num_rounds\n",
    "cvresult = xgb.cv(params, xgtrain, num_boost_round=num_rounds, nfold=5, metrics='error', seed=0,\n",
    "                  callbacks=[xgb.callback.print_evaluation(show_stdv=False), xgb.callback.early_stop(50)])\n",
    "num_round_best = cvresult.shape[0] - 1\n",
    "print('Best round num: ', num_round_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_param = {\n",
    "    'n_estimators': num_round_best,\n",
    "    'eta': 0.01,\n",
    "    'objective': 'binary:logistic',\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'min_child_weight': 1.1,\n",
    "    'max_depth': 7,\n",
    "}\n",
    "best_model = xgb.XGBClassifier(**best_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.8, eta=0.01, gamma=0, learning_rate=0.1,\n",
       "       max_delta_step=0, max_depth=7, min_child_weight=1.1, missing=None,\n",
       "       n_estimators=202, n_jobs=1, nthread=None,\n",
       "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n",
       "       subsample=0.9)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training...')\n",
    "best_model.fit(x_train_re, y_train_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda3\\envs\\AI\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "y_pred_val = best_model.predict(x_val_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6899748889318138"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred_val, y_val_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6526725816922744"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred_val, y_val_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6236033519553073"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_pred_val, y_val_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7721573713791613"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_pred_val, y_val_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda3\\envs\\AI\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "y_pred_xgb = best_model.predict(ss_x.transform(test_claim_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBoost = GradientBoostingClassifier(n_estimators=500, learning_rate=0.01,\n",
    "                                   max_depth=7, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='deviance', random_state =5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBoost.fit(x_train_re,y_train_re)\n",
    "y_pred_GBoost = GBoost.predict(x_val_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6924763993515781"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred_GBoost, y_val_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6510495563730794"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred_GBoost, y_val_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6195188534379799"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_pred_GBoost, y_val_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7849113705144833"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_pred_GBoost, y_val_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_GB = GBoost.predict(ss_x.transform(test_claim_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = RandomForestClassifier(n_estimators=2000,\n",
    "                            max_depth=5,\n",
    "                            min_samples_split=2,\n",
    "                            min_samples_leaf=1,\n",
    "                            min_weight_fraction_leaf=0.0,\n",
    "                            max_leaf_nodes=None,\n",
    "                            min_impurity_decrease=0.0,\n",
    "                            min_impurity_split=None,\n",
    "                            bootstrap=True, oob_score=False, n_jobs=1, random_state=7, verbose=0, warm_start=False, class_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF.fit(x_train_re,y_train_re)\n",
    "y_pred_RF = RF.predict(x_val_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6909090909090909"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred_RF, y_val_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6449902618480848"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred_RF, y_val_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6122891968609117"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_pred_RF, y_val_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7926934716817985"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_pred_RF, y_val_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_RF = RF.predict(ss_x.transform(test_claim_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'boosting_type': 'gbdt', \n",
    "    'objective': 'binary', \n",
    "\n",
    "    'learning_rate': 0.01, \n",
    "    'num_leaves': 80, \n",
    "    'max_depth': 10,\n",
    "    'min_child_samples': 22,\n",
    "    'min_child_weight': 0.001,\n",
    "    'bagging_fraction': 0.6,\n",
    "    'feature_fraction': 0.75,\n",
    "\n",
    "    'subsample': 0.8, \n",
    "    'colsample_bytree': 0.8, \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tcv_agg's auc: 0.709134 + 0.00124974\n",
      "[200]\tcv_agg's auc: 0.710786 + 0.00135635\n",
      "[300]\tcv_agg's auc: 0.711676 + 0.00137517\n",
      "[400]\tcv_agg's auc: 0.712296 + 0.00128624\n",
      "[500]\tcv_agg's auc: 0.712581 + 0.00127616\n",
      "[600]\tcv_agg's auc: 0.712701 + 0.00124824\n",
      "best n_estimators: 575\n",
      "best cv score: 0.7127212447215724\n"
     ]
    }
   ],
   "source": [
    "data_train = lgb.Dataset(x_train_re, y_train_re, silent=True)\n",
    "cv_results = lgb.cv(params, data_train, num_boost_round=5000, nfold=5, stratified=False, shuffle=True,\n",
    "                    metrics='auc',early_stopping_rounds=30, verbose_eval=100, show_stdv=True, seed=0)\n",
    "\n",
    "print('best n_estimators:', len(cv_results['auc-mean']))\n",
    "print('best cv score:', cv_results['auc-mean'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lgb_model = lgb.LGBMRegressor(objective='binary',num_leaves=80, min_child_samples=22, min_child_weight=0.001, \n",
    "                                   bagging_fraction=0.6, feature_fraction=0.75, \n",
    "                                   learning_rate=0.01, n_estimators=575, max_depth=10,\n",
    "                                   metric='auc',subsample=0.8, colsample_bytree=0.8, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Predicting...\n"
     ]
    }
   ],
   "source": [
    "print('Training...')\n",
    "best_lgb_model.fit(x_train_re, y_train_re)\n",
    "print('Predicting...')\n",
    "y_pred_lgb = best_lgb_model.predict(x_val_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 1, 1, 1, 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_RF[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00434432, 0.52707988, 0.00667602, 0.00602231, 0.52417674,\n",
       "       0.34254184, 0.37003961, 0.00833697, 0.53074158, 0.59412496])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_lgb = y_pred_lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of continuous and binary targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-f8d96adbf607>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred_lgb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val_re\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\AI\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mf1_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[0;32m    712\u001b[0m     return fbeta_score(y_true, y_pred, 1, labels=labels,\n\u001b[0;32m    713\u001b[0m                        \u001b[0mpos_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 714\u001b[1;33m                        sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    715\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    716\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\AI\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mfbeta_score\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[0;32m    826\u001b[0m                                                  \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'f-score'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m                                                  sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    829\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\AI\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight)\u001b[0m\n\u001b[0;32m   1023\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"beta should be >0 in the F-beta score\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1025\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1026\u001b[0m     \u001b[0mpresent_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1027\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\AI\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[1;32m---> 81\u001b[1;33m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of continuous and binary targets"
     ]
    }
   ],
   "source": [
    "f1_score(y_pred_lgb, y_val_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6277320926206449"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred_lgb, y_val_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6206124732933157"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_pred_lgb, y_val_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6593169044530912"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_pred_lgb, y_val_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lgb = best_lgb_model.predict(ss_x.transform(test_claim_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_result = pd.DataFrame({'nn':y_pred_nn.reshape(-1), 'GBOOST': y_pred_GB, 'XGB': y_pred_xgb, 'RF':y_pred_RF, 'lgb':y_pred_lgb})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_result.iloc[class_result[class_result['nn'] > 0.3].index, 4] = 1\n",
    "class_result.iloc[class_result[class_result['nn'] < 0.3].index, 4] = 0\n",
    "class_result.iloc[class_result[class_result['lgb'] > 0.3].index, 3] = 1\n",
    "class_result.iloc[class_result[class_result['lgb'] < 0.3].index, 3] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GBOOST</th>\n",
       "      <th>RF</th>\n",
       "      <th>XGB</th>\n",
       "      <th>lgb</th>\n",
       "      <th>nn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140480</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140481</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140482</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140483</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140484</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140485</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140486</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140487</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140488</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140489</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140490</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140491</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140492</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140493</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140494</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140495</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140496</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140497</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140498</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140499</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140500</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140501</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140502</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140503</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140504</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140505</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140506</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140507</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140508</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140509</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140510 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        GBOOST  RF  XGB  lgb   nn\n",
       "0            1   1    1  1.0  1.0\n",
       "1            0   0    0  1.0  1.0\n",
       "2            0   1    0  1.0  1.0\n",
       "3            1   1    1  1.0  1.0\n",
       "4            1   1    1  1.0  1.0\n",
       "5            1   1    1  1.0  1.0\n",
       "6            1   1    1  1.0  1.0\n",
       "7            0   0    0  0.0  0.0\n",
       "8            1   1    1  1.0  1.0\n",
       "9            1   1    1  1.0  1.0\n",
       "10           1   1    0  1.0  1.0\n",
       "11           1   1    1  1.0  1.0\n",
       "12           0   0    0  0.0  0.0\n",
       "13           1   1    1  1.0  1.0\n",
       "14           1   1    1  1.0  1.0\n",
       "15           1   1    1  1.0  1.0\n",
       "16           1   1    1  1.0  1.0\n",
       "17           0   0    0  1.0  1.0\n",
       "18           1   1    1  1.0  1.0\n",
       "19           1   1    1  1.0  1.0\n",
       "20           1   1    1  1.0  1.0\n",
       "21           1   1    1  1.0  1.0\n",
       "22           1   1    1  1.0  1.0\n",
       "23           1   1    1  1.0  1.0\n",
       "24           0   0    0  0.0  0.0\n",
       "25           0   0    0  0.0  0.0\n",
       "26           0   0    0  1.0  1.0\n",
       "27           0   0    0  1.0  1.0\n",
       "28           0   0    0  1.0  1.0\n",
       "29           1   1    1  1.0  1.0\n",
       "...        ...  ..  ...  ...  ...\n",
       "140480       0   0    0  0.0  0.0\n",
       "140481       1   1    1  1.0  1.0\n",
       "140482       0   0    0  1.0  1.0\n",
       "140483       1   1    1  1.0  1.0\n",
       "140484       1   1    1  1.0  1.0\n",
       "140485       1   1    1  1.0  1.0\n",
       "140486       0   0    0  1.0  1.0\n",
       "140487       0   0    0  1.0  0.0\n",
       "140488       1   1    1  1.0  1.0\n",
       "140489       0   0    0  1.0  1.0\n",
       "140490       0   1    0  1.0  1.0\n",
       "140491       0   0    0  0.0  0.0\n",
       "140492       1   1    1  1.0  1.0\n",
       "140493       0   0    0  1.0  1.0\n",
       "140494       1   1    1  1.0  1.0\n",
       "140495       0   0    0  1.0  1.0\n",
       "140496       1   1    1  1.0  1.0\n",
       "140497       0   1    0  1.0  1.0\n",
       "140498       1   1    1  1.0  1.0\n",
       "140499       0   0    0  0.0  0.0\n",
       "140500       1   1    1  1.0  1.0\n",
       "140501       1   1    1  1.0  1.0\n",
       "140502       0   0    0  0.0  0.0\n",
       "140503       1   1    1  1.0  1.0\n",
       "140504       1   1    1  1.0  1.0\n",
       "140505       1   1    1  1.0  1.0\n",
       "140506       1   1    1  1.0  1.0\n",
       "140507       1   1    1  1.0  1.0\n",
       "140508       1   1    1  1.0  1.0\n",
       "140509       1   1    1  1.0  1.0\n",
       "\n",
       "[140510 rows x 5 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_result['sum'] = (class_result.GBOOST + class_result.XGB + class_result.nn + class_result.RF + class_result.lgb)/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_result.iloc[class_result[class_result['sum'] > 0.5].index, 5] = 1\n",
    "class_result.iloc[class_result[class_result['sum'] < 0.5].index, 5] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127335.0"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_result['nn'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129030.0"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_result.lgb.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99035"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_result.RF.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda3\\envs\\AI\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  \n",
      "C:\\Users\\Asus\\Anaconda3\\envs\\AI\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "train_regression = ss_x.transform(train_renew_data.iloc[:,:-1])\n",
    "train_regression_label = ss_y.transform(train_renew_label.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(131642, 41)\n",
      "(131642, 1)\n",
      "(32911, 41)\n",
      "(32911, 1)\n"
     ]
    }
   ],
   "source": [
    "x_trian_regression, x_val_regression, y_train_regression, y_val_regression = train_test_split(train_regression, train_regression_label, test_size=0.2, random_state = 7, shuffle=True)\n",
    "print(x_trian_regression.shape)\n",
    "print(y_train_regression.shape)\n",
    "print(x_val_regression.shape)\n",
    "print(y_val_regression.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'boosting_type': 'gbdt', \n",
    "    'objective': 'regression_l1', \n",
    "\n",
    "    'learning_rate': 0.01, \n",
    "    'num_leaves': 80, \n",
    "    'max_depth': 10,\n",
    "    'min_child_samples': 22,\n",
    "    'min_child_weight': 0.001,\n",
    "    'bagging_fraction': 0.6,\n",
    "    'feature_fraction': 0.75,\n",
    "\n",
    "    'subsample': 0.8, \n",
    "    'colsample_bytree': 0.8, \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tcv_agg's l1: 0.295977 + 0.00639797\n",
      "[200]\tcv_agg's l1: 0.195814 + 0.00360844\n",
      "[300]\tcv_agg's l1: 0.155319 + 0.00237232\n",
      "[400]\tcv_agg's l1: 0.138438 + 0.00198027\n",
      "[500]\tcv_agg's l1: 0.131345 + 0.00182762\n",
      "[600]\tcv_agg's l1: 0.1285 + 0.00184462\n",
      "[700]\tcv_agg's l1: 0.127137 + 0.00187269\n",
      "[800]\tcv_agg's l1: 0.126198 + 0.00184118\n",
      "[900]\tcv_agg's l1: 0.12554 + 0.00179175\n",
      "[1000]\tcv_agg's l1: 0.125085 + 0.00172492\n",
      "[1100]\tcv_agg's l1: 0.12475 + 0.00170848\n",
      "[1200]\tcv_agg's l1: 0.124501 + 0.0016685\n",
      "[1300]\tcv_agg's l1: 0.124318 + 0.00165237\n",
      "[1400]\tcv_agg's l1: 0.12419 + 0.00164521\n",
      "[1500]\tcv_agg's l1: 0.124066 + 0.00163856\n",
      "[1600]\tcv_agg's l1: 0.123958 + 0.00164965\n",
      "[1700]\tcv_agg's l1: 0.123842 + 0.00166849\n",
      "[1800]\tcv_agg's l1: 0.123763 + 0.00166096\n",
      "[1900]\tcv_agg's l1: 0.123693 + 0.00165332\n",
      "[2000]\tcv_agg's l1: 0.123624 + 0.00161871\n",
      "[2100]\tcv_agg's l1: 0.123571 + 0.00160833\n",
      "[2200]\tcv_agg's l1: 0.123537 + 0.00158258\n",
      "[2300]\tcv_agg's l1: 0.123503 + 0.0015299\n",
      "[2400]\tcv_agg's l1: 0.123458 + 0.00153994\n",
      "[2500]\tcv_agg's l1: 0.123414 + 0.00152921\n",
      "[2600]\tcv_agg's l1: 0.123383 + 0.00153459\n",
      "[2700]\tcv_agg's l1: 0.123348 + 0.00153155\n",
      "[2800]\tcv_agg's l1: 0.123311 + 0.00152303\n",
      "[2900]\tcv_agg's l1: 0.123286 + 0.00151051\n",
      "[3000]\tcv_agg's l1: 0.123262 + 0.00151526\n",
      "best n_estimators: 3011\n",
      "best cv score: 0.12326165011528001\n"
     ]
    }
   ],
   "source": [
    "data_train = lgb.Dataset(x_trian_regression, y_train_regression.reshape(-1), silent=True)\n",
    "cv_results = lgb.cv(params, data_train, num_boost_round=5000, nfold=5, stratified=False, shuffle=True,\n",
    "                    metrics='mae',early_stopping_rounds=30, verbose_eval=100, show_stdv=True, seed=0)\n",
    "\n",
    "print('best n_estimators:', len(cv_results['l1-mean']))\n",
    "print('best cv score:', cv_results['l1-mean'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_lgb_model = lgb.LGBMRegressor(objective='regression_l1',num_leaves=80, min_child_samples=22, min_child_weight=0.001, \n",
    "                                   bagging_fraction=0.6, feature_fraction=0.75, \n",
    "                                   learning_rate=0.01, n_estimators=3011, max_depth=10,\n",
    "                                   metric='mae',subsample=0.8, colsample_bytree=0.8, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda3\\envs\\AI\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting...\n"
     ]
    }
   ],
   "source": [
    "print('Training...')\n",
    "reg_lgb_model.fit(x_trian_regression, y_train_regression)\n",
    "print('Predicting...')\n",
    "y_reg_lgb = ss_y.inverse_transform(reg_lgb_model.predict(ss_x.transform(test_claim_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_result['reg_lgb'] = y_reg_lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReG NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model = Sequential()\n",
    "reg_model.add(Dense(100, input_dim=70, kernel_initializer='normal', activation='relu'))\n",
    "reg_model.add(Dense(1, kernel_initializer='normal'))\n",
    "# Compile model\n",
    "opt = keras.optimizers.Adam(lr=0.01)\n",
    "reg_model.compile(loss='mean_absolute_error', optimizer=opt, metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 131642 samples, validate on 32911 samples\n",
      "Epoch 1/500\n",
      "131642/131642 [==============================] - 1s 6us/step - loss: 0.1821 - mean_absolute_error: 0.1821 - val_loss: 0.1451 - val_mean_absolute_error: 0.1451\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.14514, saving model to D:\\fang\\AI_proj\\tbrain_policy\\checkpoints/reg_0816_17_40_19.h5\n",
      "Epoch 2/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1399 - mean_absolute_error: 0.1399 - val_loss: 0.1403 - val_mean_absolute_error: 0.1403\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.14514 to 0.14027, saving model to D:\\fang\\AI_proj\\tbrain_policy\\checkpoints/reg_0816_17_40_19.h5\n",
      "Epoch 3/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1347 - mean_absolute_error: 0.1347 - val_loss: 0.1344 - val_mean_absolute_error: 0.1344\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.14027 to 0.13442, saving model to D:\\fang\\AI_proj\\tbrain_policy\\checkpoints/reg_0816_17_40_19.h5\n",
      "Epoch 4/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1338 - mean_absolute_error: 0.1338 - val_loss: 0.1333 - val_mean_absolute_error: 0.1333\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.13442 to 0.13327, saving model to D:\\fang\\AI_proj\\tbrain_policy\\checkpoints/reg_0816_17_40_19.h5\n",
      "Epoch 5/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1342 - mean_absolute_error: 0.1342 - val_loss: 0.1346 - val_mean_absolute_error: 0.1346\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1308 - mean_absolute_error: 0.1308 - val_loss: 0.1316 - val_mean_absolute_error: 0.1316\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.13327 to 0.13157, saving model to D:\\fang\\AI_proj\\tbrain_policy\\checkpoints/reg_0816_17_40_19.h5\n",
      "Epoch 7/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1315 - mean_absolute_error: 0.1315 - val_loss: 0.1305 - val_mean_absolute_error: 0.1305\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.13157 to 0.13046, saving model to D:\\fang\\AI_proj\\tbrain_policy\\checkpoints/reg_0816_17_40_19.h5\n",
      "Epoch 8/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1293 - mean_absolute_error: 0.1293 - val_loss: 0.1328 - val_mean_absolute_error: 0.1328\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1301 - mean_absolute_error: 0.1301 - val_loss: 0.1305 - val_mean_absolute_error: 0.1305\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1284 - mean_absolute_error: 0.1284 - val_loss: 0.1323 - val_mean_absolute_error: 0.1323\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1299 - mean_absolute_error: 0.1299 - val_loss: 0.1294 - val_mean_absolute_error: 0.1294\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.13046 to 0.12935, saving model to D:\\fang\\AI_proj\\tbrain_policy\\checkpoints/reg_0816_17_40_19.h5\n",
      "Epoch 12/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1278 - mean_absolute_error: 0.1278 - val_loss: 0.1345 - val_mean_absolute_error: 0.1345\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1275 - mean_absolute_error: 0.1275 - val_loss: 0.1272 - val_mean_absolute_error: 0.1272\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.12935 to 0.12717, saving model to D:\\fang\\AI_proj\\tbrain_policy\\checkpoints/reg_0816_17_40_19.h5\n",
      "Epoch 14/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1278 - mean_absolute_error: 0.1278 - val_loss: 0.1277 - val_mean_absolute_error: 0.1277\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1277 - mean_absolute_error: 0.1277 - val_loss: 0.1275 - val_mean_absolute_error: 0.1275\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1271 - mean_absolute_error: 0.1271 - val_loss: 0.1274 - val_mean_absolute_error: 0.1274\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1263 - mean_absolute_error: 0.1263 - val_loss: 0.1290 - val_mean_absolute_error: 0.1290\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1277 - mean_absolute_error: 0.1277 - val_loss: 0.1313 - val_mean_absolute_error: 0.1313\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1287 - mean_absolute_error: 0.1287 - val_loss: 0.1279 - val_mean_absolute_error: 0.1279\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1270 - mean_absolute_error: 0.1270 - val_loss: 0.1270 - val_mean_absolute_error: 0.1270\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.12717 to 0.12701, saving model to D:\\fang\\AI_proj\\tbrain_policy\\checkpoints/reg_0816_17_40_19.h5\n",
      "Epoch 21/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1260 - mean_absolute_error: 0.1260 - val_loss: 0.1289 - val_mean_absolute_error: 0.1289\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1253 - mean_absolute_error: 0.1253 - val_loss: 0.1270 - val_mean_absolute_error: 0.1270\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1265 - mean_absolute_error: 0.1265 - val_loss: 0.1273 - val_mean_absolute_error: 0.1273\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 24/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1269 - mean_absolute_error: 0.1269 - val_loss: 0.1296 - val_mean_absolute_error: 0.1296\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 25/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1260 - mean_absolute_error: 0.1260 - val_loss: 0.1284 - val_mean_absolute_error: 0.1284\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 26/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1254 - mean_absolute_error: 0.1254 - val_loss: 0.1270 - val_mean_absolute_error: 0.1270\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.12701 to 0.12696, saving model to D:\\fang\\AI_proj\\tbrain_policy\\checkpoints/reg_0816_17_40_19.h5\n",
      "Epoch 27/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1261 - mean_absolute_error: 0.1261 - val_loss: 0.1268 - val_mean_absolute_error: 0.1268\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.12696 to 0.12678, saving model to D:\\fang\\AI_proj\\tbrain_policy\\checkpoints/reg_0816_17_40_19.h5\n",
      "Epoch 28/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1264 - mean_absolute_error: 0.1264 - val_loss: 0.1270 - val_mean_absolute_error: 0.1270\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      "Epoch 29/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1266 - mean_absolute_error: 0.1266 - val_loss: 0.1286 - val_mean_absolute_error: 0.1286\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 30/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1255 - mean_absolute_error: 0.1255 - val_loss: 0.1264 - val_mean_absolute_error: 0.1264\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.12678 to 0.12642, saving model to D:\\fang\\AI_proj\\tbrain_policy\\checkpoints/reg_0816_17_40_19.h5\n",
      "Epoch 31/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1257 - mean_absolute_error: 0.1257 - val_loss: 0.1263 - val_mean_absolute_error: 0.1263\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.12642 to 0.12627, saving model to D:\\fang\\AI_proj\\tbrain_policy\\checkpoints/reg_0816_17_40_19.h5\n",
      "Epoch 32/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1255 - mean_absolute_error: 0.1255 - val_loss: 0.1273 - val_mean_absolute_error: 0.1273\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 33/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1249 - mean_absolute_error: 0.1249 - val_loss: 0.1274 - val_mean_absolute_error: 0.1274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00033: val_loss did not improve\n",
      "Epoch 34/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1264 - mean_absolute_error: 0.1264 - val_loss: 0.1265 - val_mean_absolute_error: 0.1265\n",
      "\n",
      "Epoch 00034: val_loss did not improve\n",
      "Epoch 35/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1250 - mean_absolute_error: 0.1250 - val_loss: 0.1271 - val_mean_absolute_error: 0.1271\n",
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 36/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1263 - mean_absolute_error: 0.1263 - val_loss: 0.1264 - val_mean_absolute_error: 0.1264\n",
      "\n",
      "Epoch 00036: val_loss did not improve\n",
      "Epoch 37/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1244 - mean_absolute_error: 0.1244 - val_loss: 0.1281 - val_mean_absolute_error: 0.1281\n",
      "\n",
      "Epoch 00037: val_loss did not improve\n",
      "Epoch 38/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1254 - mean_absolute_error: 0.1254 - val_loss: 0.1269 - val_mean_absolute_error: 0.1269\n",
      "\n",
      "Epoch 00038: val_loss did not improve\n",
      "Epoch 39/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1252 - mean_absolute_error: 0.1252 - val_loss: 0.1276 - val_mean_absolute_error: 0.1276\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 40/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1250 - mean_absolute_error: 0.1250 - val_loss: 0.1275 - val_mean_absolute_error: 0.1275\n",
      "\n",
      "Epoch 00040: val_loss did not improve\n",
      "Epoch 41/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1255 - mean_absolute_error: 0.1255 - val_loss: 0.1305 - val_mean_absolute_error: 0.1305\n",
      "\n",
      "Epoch 00041: val_loss did not improve\n",
      "Epoch 42/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1256 - mean_absolute_error: 0.1256 - val_loss: 0.1261 - val_mean_absolute_error: 0.1261\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.12627 to 0.12608, saving model to D:\\fang\\AI_proj\\tbrain_policy\\checkpoints/reg_0816_17_40_19.h5\n",
      "Epoch 43/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1239 - mean_absolute_error: 0.1239 - val_loss: 0.1258 - val_mean_absolute_error: 0.1258\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.12608 to 0.12579, saving model to D:\\fang\\AI_proj\\tbrain_policy\\checkpoints/reg_0816_17_40_19.h5\n",
      "Epoch 44/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1251 - mean_absolute_error: 0.1251 - val_loss: 0.1276 - val_mean_absolute_error: 0.1276\n",
      "\n",
      "Epoch 00044: val_loss did not improve\n",
      "Epoch 45/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1240 - mean_absolute_error: 0.1240 - val_loss: 0.1274 - val_mean_absolute_error: 0.1274\n",
      "\n",
      "Epoch 00045: val_loss did not improve\n",
      "Epoch 46/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1253 - mean_absolute_error: 0.1253 - val_loss: 0.1273 - val_mean_absolute_error: 0.1273\n",
      "\n",
      "Epoch 00046: val_loss did not improve\n",
      "Epoch 47/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1244 - mean_absolute_error: 0.1244 - val_loss: 0.1266 - val_mean_absolute_error: 0.1266\n",
      "\n",
      "Epoch 00047: val_loss did not improve\n",
      "Epoch 48/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1247 - mean_absolute_error: 0.1247 - val_loss: 0.1285 - val_mean_absolute_error: 0.1285\n",
      "\n",
      "Epoch 00048: val_loss did not improve\n",
      "Epoch 49/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1247 - mean_absolute_error: 0.1247 - val_loss: 0.1281 - val_mean_absolute_error: 0.1281\n",
      "\n",
      "Epoch 00049: val_loss did not improve\n",
      "Epoch 50/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1238 - mean_absolute_error: 0.1238 - val_loss: 0.1264 - val_mean_absolute_error: 0.1264\n",
      "\n",
      "Epoch 00050: val_loss did not improve\n",
      "Epoch 51/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1257 - mean_absolute_error: 0.1257 - val_loss: 0.1265 - val_mean_absolute_error: 0.1265\n",
      "\n",
      "Epoch 00051: val_loss did not improve\n",
      "Epoch 52/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1246 - mean_absolute_error: 0.1246 - val_loss: 0.1318 - val_mean_absolute_error: 0.1318\n",
      "\n",
      "Epoch 00052: val_loss did not improve\n",
      "Epoch 53/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1250 - mean_absolute_error: 0.1250 - val_loss: 0.1279 - val_mean_absolute_error: 0.1279\n",
      "\n",
      "Epoch 00053: val_loss did not improve\n",
      "Epoch 54/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1252 - mean_absolute_error: 0.1252 - val_loss: 0.1252 - val_mean_absolute_error: 0.1252\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.12579 to 0.12515, saving model to D:\\fang\\AI_proj\\tbrain_policy\\checkpoints/reg_0816_17_40_19.h5\n",
      "Epoch 55/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1237 - mean_absolute_error: 0.1237 - val_loss: 0.1290 - val_mean_absolute_error: 0.1290\n",
      "\n",
      "Epoch 00055: val_loss did not improve\n",
      "Epoch 56/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1244 - mean_absolute_error: 0.1244 - val_loss: 0.1288 - val_mean_absolute_error: 0.1288\n",
      "\n",
      "Epoch 00056: val_loss did not improve\n",
      "Epoch 57/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1247 - mean_absolute_error: 0.1247 - val_loss: 0.1284 - val_mean_absolute_error: 0.1284\n",
      "\n",
      "Epoch 00057: val_loss did not improve\n",
      "Epoch 58/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1245 - mean_absolute_error: 0.1245 - val_loss: 0.1265 - val_mean_absolute_error: 0.1265\n",
      "\n",
      "Epoch 00058: val_loss did not improve\n",
      "Epoch 59/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1246 - mean_absolute_error: 0.1246 - val_loss: 0.1289 - val_mean_absolute_error: 0.1289\n",
      "\n",
      "Epoch 00059: val_loss did not improve\n",
      "Epoch 60/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1246 - mean_absolute_error: 0.1246 - val_loss: 0.1302 - val_mean_absolute_error: 0.1302\n",
      "\n",
      "Epoch 00060: val_loss did not improve\n",
      "Epoch 61/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1242 - mean_absolute_error: 0.1242 - val_loss: 0.1250 - val_mean_absolute_error: 0.1250\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.12515 to 0.12502, saving model to D:\\fang\\AI_proj\\tbrain_policy\\checkpoints/reg_0816_17_40_19.h5\n",
      "Epoch 62/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1240 - mean_absolute_error: 0.1240 - val_loss: 0.1316 - val_mean_absolute_error: 0.1316\n",
      "\n",
      "Epoch 00062: val_loss did not improve\n",
      "Epoch 63/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1237 - mean_absolute_error: 0.1237 - val_loss: 0.1255 - val_mean_absolute_error: 0.1255\n",
      "\n",
      "Epoch 00063: val_loss did not improve\n",
      "Epoch 64/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1240 - mean_absolute_error: 0.1240 - val_loss: 0.1252 - val_mean_absolute_error: 0.1252\n",
      "\n",
      "Epoch 00064: val_loss did not improve\n",
      "Epoch 65/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1245 - mean_absolute_error: 0.1245 - val_loss: 0.1270 - val_mean_absolute_error: 0.1270\n",
      "\n",
      "Epoch 00065: val_loss did not improve\n",
      "Epoch 66/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1243 - mean_absolute_error: 0.1243 - val_loss: 0.1265 - val_mean_absolute_error: 0.1265\n",
      "\n",
      "Epoch 00066: val_loss did not improve\n",
      "Epoch 67/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1243 - mean_absolute_error: 0.1243 - val_loss: 0.1294 - val_mean_absolute_error: 0.1294\n",
      "\n",
      "Epoch 00067: val_loss did not improve\n",
      "Epoch 68/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1239 - mean_absolute_error: 0.1239 - val_loss: 0.1267 - val_mean_absolute_error: 0.1267\n",
      "\n",
      "Epoch 00068: val_loss did not improve\n",
      "Epoch 69/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1239 - mean_absolute_error: 0.1239 - val_loss: 0.1295 - val_mean_absolute_error: 0.1295\n",
      "\n",
      "Epoch 00069: val_loss did not improve\n",
      "Epoch 70/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1240 - mean_absolute_error: 0.1240 - val_loss: 0.1265 - val_mean_absolute_error: 0.1265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00070: val_loss did not improve\n",
      "Epoch 71/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1238 - mean_absolute_error: 0.1238 - val_loss: 0.1280 - val_mean_absolute_error: 0.1280\n",
      "\n",
      "Epoch 00071: val_loss did not improve\n",
      "Epoch 72/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1236 - mean_absolute_error: 0.1236 - val_loss: 0.1292 - val_mean_absolute_error: 0.1292\n",
      "\n",
      "Epoch 00072: val_loss did not improve\n",
      "Epoch 73/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1238 - mean_absolute_error: 0.1238 - val_loss: 0.1276 - val_mean_absolute_error: 0.1276\n",
      "\n",
      "Epoch 00073: val_loss did not improve\n",
      "Epoch 74/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1234 - mean_absolute_error: 0.1234 - val_loss: 0.1297 - val_mean_absolute_error: 0.1297\n",
      "\n",
      "Epoch 00074: val_loss did not improve\n",
      "Epoch 75/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1244 - mean_absolute_error: 0.1244 - val_loss: 0.1281 - val_mean_absolute_error: 0.1281\n",
      "\n",
      "Epoch 00075: val_loss did not improve\n",
      "Epoch 76/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1236 - mean_absolute_error: 0.1236 - val_loss: 0.1284 - val_mean_absolute_error: 0.1284\n",
      "\n",
      "Epoch 00076: val_loss did not improve\n",
      "Epoch 77/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1234 - mean_absolute_error: 0.1234 - val_loss: 0.1283 - val_mean_absolute_error: 0.1283\n",
      "\n",
      "Epoch 00077: val_loss did not improve\n",
      "Epoch 78/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1244 - mean_absolute_error: 0.1244 - val_loss: 0.1267 - val_mean_absolute_error: 0.1267\n",
      "\n",
      "Epoch 00078: val_loss did not improve\n",
      "Epoch 79/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1235 - mean_absolute_error: 0.1235 - val_loss: 0.1275 - val_mean_absolute_error: 0.1275\n",
      "\n",
      "Epoch 00079: val_loss did not improve\n",
      "Epoch 80/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1235 - mean_absolute_error: 0.1235 - val_loss: 0.1324 - val_mean_absolute_error: 0.1324\n",
      "\n",
      "Epoch 00080: val_loss did not improve\n",
      "Epoch 81/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1235 - mean_absolute_error: 0.1235 - val_loss: 0.1281 - val_mean_absolute_error: 0.1281\n",
      "\n",
      "Epoch 00081: val_loss did not improve\n",
      "Epoch 82/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1237 - mean_absolute_error: 0.1237 - val_loss: 0.1281 - val_mean_absolute_error: 0.1281\n",
      "\n",
      "Epoch 00082: val_loss did not improve\n",
      "Epoch 83/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1230 - mean_absolute_error: 0.1230 - val_loss: 0.1286 - val_mean_absolute_error: 0.1286\n",
      "\n",
      "Epoch 00083: val_loss did not improve\n",
      "Epoch 84/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1238 - mean_absolute_error: 0.1238 - val_loss: 0.1276 - val_mean_absolute_error: 0.1276\n",
      "\n",
      "Epoch 00084: val_loss did not improve\n",
      "Epoch 85/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1230 - mean_absolute_error: 0.1230 - val_loss: 0.1270 - val_mean_absolute_error: 0.1270\n",
      "\n",
      "Epoch 00085: val_loss did not improve\n",
      "Epoch 86/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1231 - mean_absolute_error: 0.1231 - val_loss: 0.1269 - val_mean_absolute_error: 0.1269\n",
      "\n",
      "Epoch 00086: val_loss did not improve\n",
      "Epoch 87/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1237 - mean_absolute_error: 0.1237 - val_loss: 0.1259 - val_mean_absolute_error: 0.1259\n",
      "\n",
      "Epoch 00087: val_loss did not improve\n",
      "Epoch 88/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1231 - mean_absolute_error: 0.1231 - val_loss: 0.1262 - val_mean_absolute_error: 0.1262\n",
      "\n",
      "Epoch 00088: val_loss did not improve\n",
      "Epoch 89/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1233 - mean_absolute_error: 0.1233 - val_loss: 0.1268 - val_mean_absolute_error: 0.1268\n",
      "\n",
      "Epoch 00089: val_loss did not improve\n",
      "Epoch 90/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1234 - mean_absolute_error: 0.1234 - val_loss: 0.1288 - val_mean_absolute_error: 0.1288\n",
      "\n",
      "Epoch 00090: val_loss did not improve\n",
      "Epoch 91/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1227 - mean_absolute_error: 0.1227 - val_loss: 0.1258 - val_mean_absolute_error: 0.1258\n",
      "\n",
      "Epoch 00091: val_loss did not improve\n",
      "Epoch 92/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1225 - mean_absolute_error: 0.1225 - val_loss: 0.1271 - val_mean_absolute_error: 0.1271\n",
      "\n",
      "Epoch 00092: val_loss did not improve\n",
      "Epoch 93/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1232 - mean_absolute_error: 0.1232 - val_loss: 0.1265 - val_mean_absolute_error: 0.1265\n",
      "\n",
      "Epoch 00093: val_loss did not improve\n",
      "Epoch 94/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1228 - mean_absolute_error: 0.1228 - val_loss: 0.1281 - val_mean_absolute_error: 0.1281\n",
      "\n",
      "Epoch 00094: val_loss did not improve\n",
      "Epoch 95/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1230 - mean_absolute_error: 0.1230 - val_loss: 0.1269 - val_mean_absolute_error: 0.1269\n",
      "\n",
      "Epoch 00095: val_loss did not improve\n",
      "Epoch 96/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1229 - mean_absolute_error: 0.1229 - val_loss: 0.1250 - val_mean_absolute_error: 0.1250\n",
      "\n",
      "Epoch 00096: val_loss did not improve\n",
      "Epoch 97/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1234 - mean_absolute_error: 0.1234 - val_loss: 0.1300 - val_mean_absolute_error: 0.1300\n",
      "\n",
      "Epoch 00097: val_loss did not improve\n",
      "Epoch 98/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1232 - mean_absolute_error: 0.1232 - val_loss: 0.1254 - val_mean_absolute_error: 0.1254\n",
      "\n",
      "Epoch 00098: val_loss did not improve\n",
      "Epoch 99/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1228 - mean_absolute_error: 0.1228 - val_loss: 0.1256 - val_mean_absolute_error: 0.1256\n",
      "\n",
      "Epoch 00099: val_loss did not improve\n",
      "Epoch 100/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1227 - mean_absolute_error: 0.1227 - val_loss: 0.1256 - val_mean_absolute_error: 0.1256\n",
      "\n",
      "Epoch 00100: val_loss did not improve\n",
      "Epoch 101/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1223 - mean_absolute_error: 0.1223 - val_loss: 0.1261 - val_mean_absolute_error: 0.1261\n",
      "\n",
      "Epoch 00101: val_loss did not improve\n",
      "Epoch 102/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1231 - mean_absolute_error: 0.1231 - val_loss: 0.1251 - val_mean_absolute_error: 0.1251\n",
      "\n",
      "Epoch 00102: val_loss did not improve\n",
      "Epoch 103/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1225 - mean_absolute_error: 0.1225 - val_loss: 0.1255 - val_mean_absolute_error: 0.1255\n",
      "\n",
      "Epoch 00103: val_loss did not improve\n",
      "Epoch 104/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1225 - mean_absolute_error: 0.1225 - val_loss: 0.1281 - val_mean_absolute_error: 0.1281\n",
      "\n",
      "Epoch 00104: val_loss did not improve\n",
      "Epoch 105/500\n",
      "131642/131642 [==============================] - 1s 6us/step - loss: 0.1235 - mean_absolute_error: 0.1235 - val_loss: 0.1263 - val_mean_absolute_error: 0.1263\n",
      "\n",
      "Epoch 00105: val_loss did not improve\n",
      "Epoch 106/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1226 - mean_absolute_error: 0.1226 - val_loss: 0.1264 - val_mean_absolute_error: 0.1264\n",
      "\n",
      "Epoch 00106: val_loss did not improve\n",
      "Epoch 107/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1233 - mean_absolute_error: 0.1233 - val_loss: 0.1251 - val_mean_absolute_error: 0.1251\n",
      "\n",
      "Epoch 00107: val_loss did not improve\n",
      "Epoch 108/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1235 - mean_absolute_error: 0.1235 - val_loss: 0.1354 - val_mean_absolute_error: 0.1354\n",
      "\n",
      "Epoch 00108: val_loss did not improve\n",
      "Epoch 109/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1237 - mean_absolute_error: 0.1237 - val_loss: 0.1269 - val_mean_absolute_error: 0.1269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00109: val_loss did not improve\n",
      "Epoch 110/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1231 - mean_absolute_error: 0.1231 - val_loss: 0.1256 - val_mean_absolute_error: 0.1256\n",
      "\n",
      "Epoch 00110: val_loss did not improve\n",
      "Epoch 111/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1225 - mean_absolute_error: 0.1225 - val_loss: 0.1267 - val_mean_absolute_error: 0.1267\n",
      "\n",
      "Epoch 00111: val_loss did not improve\n",
      "Epoch 112/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1229 - mean_absolute_error: 0.1229 - val_loss: 0.1267 - val_mean_absolute_error: 0.1267\n",
      "\n",
      "Epoch 00112: val_loss did not improve\n",
      "Epoch 113/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1225 - mean_absolute_error: 0.1225 - val_loss: 0.1255 - val_mean_absolute_error: 0.1255\n",
      "\n",
      "Epoch 00113: val_loss did not improve\n",
      "Epoch 114/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1227 - mean_absolute_error: 0.1227 - val_loss: 0.1268 - val_mean_absolute_error: 0.1268\n",
      "\n",
      "Epoch 00114: val_loss did not improve\n",
      "Epoch 115/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1227 - mean_absolute_error: 0.1227 - val_loss: 0.1268 - val_mean_absolute_error: 0.1268\n",
      "\n",
      "Epoch 00115: val_loss did not improve\n",
      "Epoch 116/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1231 - mean_absolute_error: 0.1231 - val_loss: 0.1250 - val_mean_absolute_error: 0.1250\n",
      "\n",
      "Epoch 00116: val_loss did not improve\n",
      "Epoch 117/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1232 - mean_absolute_error: 0.1232 - val_loss: 0.1260 - val_mean_absolute_error: 0.1260\n",
      "\n",
      "Epoch 00117: val_loss did not improve\n",
      "Epoch 118/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1227 - mean_absolute_error: 0.1227 - val_loss: 0.1260 - val_mean_absolute_error: 0.1260\n",
      "\n",
      "Epoch 00118: val_loss did not improve\n",
      "Epoch 119/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1222 - mean_absolute_error: 0.1222 - val_loss: 0.1270 - val_mean_absolute_error: 0.1270\n",
      "\n",
      "Epoch 00119: val_loss did not improve\n",
      "Epoch 120/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1235 - mean_absolute_error: 0.1235 - val_loss: 0.1266 - val_mean_absolute_error: 0.1266\n",
      "\n",
      "Epoch 00120: val_loss did not improve\n",
      "Epoch 121/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1222 - mean_absolute_error: 0.1222 - val_loss: 0.1258 - val_mean_absolute_error: 0.1258\n",
      "\n",
      "Epoch 00121: val_loss did not improve\n",
      "Epoch 122/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1219 - mean_absolute_error: 0.1219 - val_loss: 0.1257 - val_mean_absolute_error: 0.1257\n",
      "\n",
      "Epoch 00122: val_loss did not improve\n",
      "Epoch 123/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1221 - mean_absolute_error: 0.1221 - val_loss: 0.1252 - val_mean_absolute_error: 0.1252\n",
      "\n",
      "Epoch 00123: val_loss did not improve\n",
      "Epoch 124/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1228 - mean_absolute_error: 0.1228 - val_loss: 0.1282 - val_mean_absolute_error: 0.1282\n",
      "\n",
      "Epoch 00124: val_loss did not improve\n",
      "Epoch 125/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1229 - mean_absolute_error: 0.1229 - val_loss: 0.1273 - val_mean_absolute_error: 0.1273\n",
      "\n",
      "Epoch 00125: val_loss did not improve\n",
      "Epoch 126/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1233 - mean_absolute_error: 0.1233 - val_loss: 0.1255 - val_mean_absolute_error: 0.1255\n",
      "\n",
      "Epoch 00126: val_loss did not improve\n",
      "Epoch 127/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1221 - mean_absolute_error: 0.1221 - val_loss: 0.1271 - val_mean_absolute_error: 0.1271\n",
      "\n",
      "Epoch 00127: val_loss did not improve\n",
      "Epoch 128/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1224 - mean_absolute_error: 0.1224 - val_loss: 0.1257 - val_mean_absolute_error: 0.1257\n",
      "\n",
      "Epoch 00128: val_loss did not improve\n",
      "Epoch 129/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1218 - mean_absolute_error: 0.1218 - val_loss: 0.1263 - val_mean_absolute_error: 0.1263\n",
      "\n",
      "Epoch 00129: val_loss did not improve\n",
      "Epoch 130/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1231 - mean_absolute_error: 0.1231 - val_loss: 0.1260 - val_mean_absolute_error: 0.1260\n",
      "\n",
      "Epoch 00130: val_loss did not improve\n",
      "Epoch 131/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1221 - mean_absolute_error: 0.1221 - val_loss: 0.1261 - val_mean_absolute_error: 0.1261\n",
      "\n",
      "Epoch 00131: val_loss did not improve\n",
      "Epoch 132/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1236 - mean_absolute_error: 0.1236 - val_loss: 0.1263 - val_mean_absolute_error: 0.1263\n",
      "\n",
      "Epoch 00132: val_loss did not improve\n",
      "Epoch 133/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1229 - mean_absolute_error: 0.1229 - val_loss: 0.1275 - val_mean_absolute_error: 0.1275\n",
      "\n",
      "Epoch 00133: val_loss did not improve\n",
      "Epoch 134/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1229 - mean_absolute_error: 0.1229 - val_loss: 0.1281 - val_mean_absolute_error: 0.1281\n",
      "\n",
      "Epoch 00134: val_loss did not improve\n",
      "Epoch 135/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1225 - mean_absolute_error: 0.1225 - val_loss: 0.1267 - val_mean_absolute_error: 0.1267\n",
      "\n",
      "Epoch 00135: val_loss did not improve\n",
      "Epoch 136/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1221 - mean_absolute_error: 0.1221 - val_loss: 0.1275 - val_mean_absolute_error: 0.1275\n",
      "\n",
      "Epoch 00136: val_loss did not improve\n",
      "Epoch 137/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1229 - mean_absolute_error: 0.1229 - val_loss: 0.1258 - val_mean_absolute_error: 0.1258\n",
      "\n",
      "Epoch 00137: val_loss did not improve\n",
      "Epoch 138/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1227 - mean_absolute_error: 0.1227 - val_loss: 0.1277 - val_mean_absolute_error: 0.1277\n",
      "\n",
      "Epoch 00138: val_loss did not improve\n",
      "Epoch 139/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1224 - mean_absolute_error: 0.1224 - val_loss: 0.1262 - val_mean_absolute_error: 0.1262\n",
      "\n",
      "Epoch 00139: val_loss did not improve\n",
      "Epoch 140/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1218 - mean_absolute_error: 0.1218 - val_loss: 0.1262 - val_mean_absolute_error: 0.1262\n",
      "\n",
      "Epoch 00140: val_loss did not improve\n",
      "Epoch 141/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1224 - mean_absolute_error: 0.1224 - val_loss: 0.1260 - val_mean_absolute_error: 0.1260\n",
      "\n",
      "Epoch 00141: val_loss did not improve\n",
      "Epoch 142/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1221 - mean_absolute_error: 0.1221 - val_loss: 0.1260 - val_mean_absolute_error: 0.1260\n",
      "\n",
      "Epoch 00142: val_loss did not improve\n",
      "Epoch 143/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1222 - mean_absolute_error: 0.1222 - val_loss: 0.1272 - val_mean_absolute_error: 0.1272\n",
      "\n",
      "Epoch 00143: val_loss did not improve\n",
      "Epoch 144/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1219 - mean_absolute_error: 0.1219 - val_loss: 0.1277 - val_mean_absolute_error: 0.1277\n",
      "\n",
      "Epoch 00144: val_loss did not improve\n",
      "Epoch 145/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1221 - mean_absolute_error: 0.1221 - val_loss: 0.1254 - val_mean_absolute_error: 0.1254\n",
      "\n",
      "Epoch 00145: val_loss did not improve\n",
      "Epoch 146/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1224 - mean_absolute_error: 0.1224 - val_loss: 0.1262 - val_mean_absolute_error: 0.1262\n",
      "\n",
      "Epoch 00146: val_loss did not improve\n",
      "Epoch 147/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1221 - mean_absolute_error: 0.1221 - val_loss: 0.1259 - val_mean_absolute_error: 0.1259\n",
      "\n",
      "Epoch 00147: val_loss did not improve\n",
      "Epoch 148/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1219 - mean_absolute_error: 0.1219 - val_loss: 0.1281 - val_mean_absolute_error: 0.1281\n",
      "\n",
      "Epoch 00148: val_loss did not improve\n",
      "Epoch 149/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1222 - mean_absolute_error: 0.1222 - val_loss: 0.1275 - val_mean_absolute_error: 0.1275\n",
      "\n",
      "Epoch 00149: val_loss did not improve\n",
      "Epoch 150/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1217 - mean_absolute_error: 0.1217 - val_loss: 0.1262 - val_mean_absolute_error: 0.1262\n",
      "\n",
      "Epoch 00150: val_loss did not improve\n",
      "Epoch 151/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1225 - mean_absolute_error: 0.1225 - val_loss: 0.1269 - val_mean_absolute_error: 0.1269\n",
      "\n",
      "Epoch 00151: val_loss did not improve\n",
      "Epoch 152/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1221 - mean_absolute_error: 0.1221 - val_loss: 0.1269 - val_mean_absolute_error: 0.1269\n",
      "\n",
      "Epoch 00152: val_loss did not improve\n",
      "Epoch 153/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1219 - mean_absolute_error: 0.1219 - val_loss: 0.1246 - val_mean_absolute_error: 0.1246\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.12502 to 0.12465, saving model to D:\\fang\\AI_proj\\tbrain_policy\\checkpoints/reg_0816_17_40_19.h5\n",
      "Epoch 154/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1223 - mean_absolute_error: 0.1223 - val_loss: 0.1274 - val_mean_absolute_error: 0.1274\n",
      "\n",
      "Epoch 00154: val_loss did not improve\n",
      "Epoch 155/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1221 - mean_absolute_error: 0.1221 - val_loss: 0.1265 - val_mean_absolute_error: 0.1265\n",
      "\n",
      "Epoch 00155: val_loss did not improve\n",
      "Epoch 156/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1218 - mean_absolute_error: 0.1218 - val_loss: 0.1269 - val_mean_absolute_error: 0.1269\n",
      "\n",
      "Epoch 00156: val_loss did not improve\n",
      "Epoch 157/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1218 - mean_absolute_error: 0.1218 - val_loss: 0.1261 - val_mean_absolute_error: 0.1261\n",
      "\n",
      "Epoch 00157: val_loss did not improve\n",
      "Epoch 158/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1220 - mean_absolute_error: 0.1220 - val_loss: 0.1294 - val_mean_absolute_error: 0.1294\n",
      "\n",
      "Epoch 00158: val_loss did not improve\n",
      "Epoch 159/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1218 - mean_absolute_error: 0.1218 - val_loss: 0.1258 - val_mean_absolute_error: 0.1258\n",
      "\n",
      "Epoch 00159: val_loss did not improve\n",
      "Epoch 160/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1226 - mean_absolute_error: 0.1226 - val_loss: 0.1253 - val_mean_absolute_error: 0.1253\n",
      "\n",
      "Epoch 00160: val_loss did not improve\n",
      "Epoch 161/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1218 - mean_absolute_error: 0.1218 - val_loss: 0.1263 - val_mean_absolute_error: 0.1263\n",
      "\n",
      "Epoch 00161: val_loss did not improve\n",
      "Epoch 162/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1218 - mean_absolute_error: 0.1218 - val_loss: 0.1289 - val_mean_absolute_error: 0.1289\n",
      "\n",
      "Epoch 00162: val_loss did not improve\n",
      "Epoch 163/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1217 - mean_absolute_error: 0.1217 - val_loss: 0.1263 - val_mean_absolute_error: 0.1263\n",
      "\n",
      "Epoch 00163: val_loss did not improve\n",
      "Epoch 164/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1221 - mean_absolute_error: 0.1221 - val_loss: 0.1258 - val_mean_absolute_error: 0.1258\n",
      "\n",
      "Epoch 00164: val_loss did not improve\n",
      "Epoch 165/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1220 - mean_absolute_error: 0.1220 - val_loss: 0.1267 - val_mean_absolute_error: 0.1267\n",
      "\n",
      "Epoch 00165: val_loss did not improve\n",
      "Epoch 166/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1228 - mean_absolute_error: 0.1228 - val_loss: 0.1259 - val_mean_absolute_error: 0.1259\n",
      "\n",
      "Epoch 00166: val_loss did not improve\n",
      "Epoch 167/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1226 - mean_absolute_error: 0.1226 - val_loss: 0.1259 - val_mean_absolute_error: 0.1259\n",
      "\n",
      "Epoch 00167: val_loss did not improve\n",
      "Epoch 168/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1218 - mean_absolute_error: 0.1218 - val_loss: 0.1289 - val_mean_absolute_error: 0.1289\n",
      "\n",
      "Epoch 00168: val_loss did not improve\n",
      "Epoch 169/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1222 - mean_absolute_error: 0.1222 - val_loss: 0.1282 - val_mean_absolute_error: 0.1282\n",
      "\n",
      "Epoch 00169: val_loss did not improve\n",
      "Epoch 170/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1219 - mean_absolute_error: 0.1219 - val_loss: 0.1272 - val_mean_absolute_error: 0.1272\n",
      "\n",
      "Epoch 00170: val_loss did not improve\n",
      "Epoch 171/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1218 - mean_absolute_error: 0.1218 - val_loss: 0.1251 - val_mean_absolute_error: 0.1251\n",
      "\n",
      "Epoch 00171: val_loss did not improve\n",
      "Epoch 172/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1213 - mean_absolute_error: 0.1213 - val_loss: 0.1259 - val_mean_absolute_error: 0.1259\n",
      "\n",
      "Epoch 00172: val_loss did not improve\n",
      "Epoch 173/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1220 - mean_absolute_error: 0.1220 - val_loss: 0.1277 - val_mean_absolute_error: 0.1277\n",
      "\n",
      "Epoch 00173: val_loss did not improve\n",
      "Epoch 174/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1215 - mean_absolute_error: 0.1215 - val_loss: 0.1264 - val_mean_absolute_error: 0.1264\n",
      "\n",
      "Epoch 00174: val_loss did not improve\n",
      "Epoch 175/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1214 - mean_absolute_error: 0.1214 - val_loss: 0.1257 - val_mean_absolute_error: 0.1257\n",
      "\n",
      "Epoch 00175: val_loss did not improve\n",
      "Epoch 176/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1222 - mean_absolute_error: 0.1222 - val_loss: 0.1285 - val_mean_absolute_error: 0.1285\n",
      "\n",
      "Epoch 00176: val_loss did not improve\n",
      "Epoch 177/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1217 - mean_absolute_error: 0.1217 - val_loss: 0.1256 - val_mean_absolute_error: 0.1256\n",
      "\n",
      "Epoch 00177: val_loss did not improve\n",
      "Epoch 178/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1217 - mean_absolute_error: 0.1217 - val_loss: 0.1263 - val_mean_absolute_error: 0.1263\n",
      "\n",
      "Epoch 00178: val_loss did not improve\n",
      "Epoch 179/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1219 - mean_absolute_error: 0.1219 - val_loss: 0.1273 - val_mean_absolute_error: 0.1273\n",
      "\n",
      "Epoch 00179: val_loss did not improve\n",
      "Epoch 180/500\n",
      "131642/131642 [==============================] - 1s 6us/step - loss: 0.1215 - mean_absolute_error: 0.1215 - val_loss: 0.1281 - val_mean_absolute_error: 0.1281\n",
      "\n",
      "Epoch 00180: val_loss did not improve\n",
      "Epoch 181/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1221 - mean_absolute_error: 0.1221 - val_loss: 0.1270 - val_mean_absolute_error: 0.1270\n",
      "\n",
      "Epoch 00181: val_loss did not improve\n",
      "Epoch 182/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1219 - mean_absolute_error: 0.1219 - val_loss: 0.1272 - val_mean_absolute_error: 0.1272\n",
      "\n",
      "Epoch 00182: val_loss did not improve\n",
      "Epoch 183/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1215 - mean_absolute_error: 0.1215 - val_loss: 0.1252 - val_mean_absolute_error: 0.1252\n",
      "\n",
      "Epoch 00183: val_loss did not improve\n",
      "Epoch 184/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1217 - mean_absolute_error: 0.1217 - val_loss: 0.1247 - val_mean_absolute_error: 0.1247\n",
      "\n",
      "Epoch 00184: val_loss did not improve\n",
      "Epoch 185/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1218 - mean_absolute_error: 0.1218 - val_loss: 0.1245 - val_mean_absolute_error: 0.1245\n",
      "\n",
      "Epoch 00185: val_loss improved from 0.12465 to 0.12454, saving model to D:\\fang\\AI_proj\\tbrain_policy\\checkpoints/reg_0816_17_40_19.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 186/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1217 - mean_absolute_error: 0.1217 - val_loss: 0.1266 - val_mean_absolute_error: 0.1266\n",
      "\n",
      "Epoch 00186: val_loss did not improve\n",
      "Epoch 187/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1226 - mean_absolute_error: 0.1226 - val_loss: 0.1249 - val_mean_absolute_error: 0.1249\n",
      "\n",
      "Epoch 00187: val_loss did not improve\n",
      "Epoch 188/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1211 - mean_absolute_error: 0.1211 - val_loss: 0.1278 - val_mean_absolute_error: 0.1278\n",
      "\n",
      "Epoch 00188: val_loss did not improve\n",
      "Epoch 189/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1215 - mean_absolute_error: 0.1215 - val_loss: 0.1263 - val_mean_absolute_error: 0.1263\n",
      "\n",
      "Epoch 00189: val_loss did not improve\n",
      "Epoch 190/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1221 - mean_absolute_error: 0.1221 - val_loss: 0.1251 - val_mean_absolute_error: 0.1251\n",
      "\n",
      "Epoch 00190: val_loss did not improve\n",
      "Epoch 191/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1216 - mean_absolute_error: 0.1216 - val_loss: 0.1264 - val_mean_absolute_error: 0.1264\n",
      "\n",
      "Epoch 00191: val_loss did not improve\n",
      "Epoch 192/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1217 - mean_absolute_error: 0.1217 - val_loss: 0.1253 - val_mean_absolute_error: 0.1253\n",
      "\n",
      "Epoch 00192: val_loss did not improve\n",
      "Epoch 193/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1221 - mean_absolute_error: 0.1221 - val_loss: 0.1275 - val_mean_absolute_error: 0.1275\n",
      "\n",
      "Epoch 00193: val_loss did not improve\n",
      "Epoch 194/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1217 - mean_absolute_error: 0.1217 - val_loss: 0.1254 - val_mean_absolute_error: 0.1254\n",
      "\n",
      "Epoch 00194: val_loss did not improve\n",
      "Epoch 195/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1212 - mean_absolute_error: 0.1212 - val_loss: 0.1275 - val_mean_absolute_error: 0.1275\n",
      "\n",
      "Epoch 00195: val_loss did not improve\n",
      "Epoch 196/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1219 - mean_absolute_error: 0.1219 - val_loss: 0.1251 - val_mean_absolute_error: 0.1251\n",
      "\n",
      "Epoch 00196: val_loss did not improve\n",
      "Epoch 197/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1216 - mean_absolute_error: 0.1216 - val_loss: 0.1256 - val_mean_absolute_error: 0.1256\n",
      "\n",
      "Epoch 00197: val_loss did not improve\n",
      "Epoch 198/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1216 - mean_absolute_error: 0.1216 - val_loss: 0.1299 - val_mean_absolute_error: 0.1299\n",
      "\n",
      "Epoch 00198: val_loss did not improve\n",
      "Epoch 199/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1224 - mean_absolute_error: 0.1224 - val_loss: 0.1256 - val_mean_absolute_error: 0.1256\n",
      "\n",
      "Epoch 00199: val_loss did not improve\n",
      "Epoch 200/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1219 - mean_absolute_error: 0.1219 - val_loss: 0.1263 - val_mean_absolute_error: 0.1263\n",
      "\n",
      "Epoch 00200: val_loss did not improve\n",
      "Epoch 201/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1223 - mean_absolute_error: 0.1223 - val_loss: 0.1247 - val_mean_absolute_error: 0.1247\n",
      "\n",
      "Epoch 00201: val_loss did not improve\n",
      "Epoch 202/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1211 - mean_absolute_error: 0.1211 - val_loss: 0.1263 - val_mean_absolute_error: 0.1263\n",
      "\n",
      "Epoch 00202: val_loss did not improve\n",
      "Epoch 203/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1214 - mean_absolute_error: 0.1214 - val_loss: 0.1251 - val_mean_absolute_error: 0.1251\n",
      "\n",
      "Epoch 00203: val_loss did not improve\n",
      "Epoch 204/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1215 - mean_absolute_error: 0.1215 - val_loss: 0.1248 - val_mean_absolute_error: 0.1248\n",
      "\n",
      "Epoch 00204: val_loss did not improve\n",
      "Epoch 205/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1220 - mean_absolute_error: 0.1220 - val_loss: 0.1265 - val_mean_absolute_error: 0.1265\n",
      "\n",
      "Epoch 00205: val_loss did not improve\n",
      "Epoch 206/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1217 - mean_absolute_error: 0.1217 - val_loss: 0.1258 - val_mean_absolute_error: 0.1258\n",
      "\n",
      "Epoch 00206: val_loss did not improve\n",
      "Epoch 207/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1218 - mean_absolute_error: 0.1218 - val_loss: 0.1271 - val_mean_absolute_error: 0.1271\n",
      "\n",
      "Epoch 00207: val_loss did not improve\n",
      "Epoch 208/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1212 - mean_absolute_error: 0.1212 - val_loss: 0.1255 - val_mean_absolute_error: 0.1255\n",
      "\n",
      "Epoch 00208: val_loss did not improve\n",
      "Epoch 209/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1223 - mean_absolute_error: 0.1223 - val_loss: 0.1257 - val_mean_absolute_error: 0.1257\n",
      "\n",
      "Epoch 00209: val_loss did not improve\n",
      "Epoch 210/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1219 - mean_absolute_error: 0.1219 - val_loss: 0.1264 - val_mean_absolute_error: 0.1264\n",
      "\n",
      "Epoch 00210: val_loss did not improve\n",
      "Epoch 211/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1218 - mean_absolute_error: 0.1218 - val_loss: 0.1245 - val_mean_absolute_error: 0.1245\n",
      "\n",
      "Epoch 00211: val_loss improved from 0.12454 to 0.12452, saving model to D:\\fang\\AI_proj\\tbrain_policy\\checkpoints/reg_0816_17_40_19.h5\n",
      "Epoch 212/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1215 - mean_absolute_error: 0.1215 - val_loss: 0.1275 - val_mean_absolute_error: 0.1275\n",
      "\n",
      "Epoch 00212: val_loss did not improve\n",
      "Epoch 213/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1215 - mean_absolute_error: 0.1215 - val_loss: 0.1266 - val_mean_absolute_error: 0.1266\n",
      "\n",
      "Epoch 00213: val_loss did not improve\n",
      "Epoch 214/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1219 - mean_absolute_error: 0.1219 - val_loss: 0.1247 - val_mean_absolute_error: 0.1247\n",
      "\n",
      "Epoch 00214: val_loss did not improve\n",
      "Epoch 215/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1219 - mean_absolute_error: 0.1219 - val_loss: 0.1323 - val_mean_absolute_error: 0.1323\n",
      "\n",
      "Epoch 00215: val_loss did not improve\n",
      "Epoch 216/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1217 - mean_absolute_error: 0.1217 - val_loss: 0.1249 - val_mean_absolute_error: 0.1249\n",
      "\n",
      "Epoch 00216: val_loss did not improve\n",
      "Epoch 217/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1213 - mean_absolute_error: 0.1213 - val_loss: 0.1244 - val_mean_absolute_error: 0.1244\n",
      "\n",
      "Epoch 00217: val_loss improved from 0.12452 to 0.12443, saving model to D:\\fang\\AI_proj\\tbrain_policy\\checkpoints/reg_0816_17_40_19.h5\n",
      "Epoch 218/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1216 - mean_absolute_error: 0.1216 - val_loss: 0.1265 - val_mean_absolute_error: 0.1265\n",
      "\n",
      "Epoch 00218: val_loss did not improve\n",
      "Epoch 219/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1219 - mean_absolute_error: 0.1219 - val_loss: 0.1245 - val_mean_absolute_error: 0.1245\n",
      "\n",
      "Epoch 00219: val_loss did not improve\n",
      "Epoch 220/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1216 - mean_absolute_error: 0.1216 - val_loss: 0.1263 - val_mean_absolute_error: 0.1263\n",
      "\n",
      "Epoch 00220: val_loss did not improve\n",
      "Epoch 221/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1218 - mean_absolute_error: 0.1218 - val_loss: 0.1275 - val_mean_absolute_error: 0.1275\n",
      "\n",
      "Epoch 00221: val_loss did not improve\n",
      "Epoch 222/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1213 - mean_absolute_error: 0.1213 - val_loss: 0.1256 - val_mean_absolute_error: 0.1256\n",
      "\n",
      "Epoch 00222: val_loss did not improve\n",
      "Epoch 223/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1213 - mean_absolute_error: 0.1213 - val_loss: 0.1255 - val_mean_absolute_error: 0.1255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00223: val_loss did not improve\n",
      "Epoch 224/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1216 - mean_absolute_error: 0.1216 - val_loss: 0.1247 - val_mean_absolute_error: 0.1247\n",
      "\n",
      "Epoch 00224: val_loss did not improve\n",
      "Epoch 225/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1215 - mean_absolute_error: 0.1215 - val_loss: 0.1260 - val_mean_absolute_error: 0.1260\n",
      "\n",
      "Epoch 00225: val_loss did not improve\n",
      "Epoch 226/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1210 - mean_absolute_error: 0.1210 - val_loss: 0.1250 - val_mean_absolute_error: 0.1250\n",
      "\n",
      "Epoch 00226: val_loss did not improve\n",
      "Epoch 227/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1216 - mean_absolute_error: 0.1216 - val_loss: 0.1244 - val_mean_absolute_error: 0.1244\n",
      "\n",
      "Epoch 00227: val_loss did not improve\n",
      "Epoch 228/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1212 - mean_absolute_error: 0.1212 - val_loss: 0.1247 - val_mean_absolute_error: 0.1247\n",
      "\n",
      "Epoch 00228: val_loss did not improve\n",
      "Epoch 229/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1214 - mean_absolute_error: 0.1214 - val_loss: 0.1256 - val_mean_absolute_error: 0.1256\n",
      "\n",
      "Epoch 00229: val_loss did not improve\n",
      "Epoch 230/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1210 - mean_absolute_error: 0.1210 - val_loss: 0.1261 - val_mean_absolute_error: 0.1261\n",
      "\n",
      "Epoch 00230: val_loss did not improve\n",
      "Epoch 231/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1216 - mean_absolute_error: 0.1216 - val_loss: 0.1258 - val_mean_absolute_error: 0.1258\n",
      "\n",
      "Epoch 00231: val_loss did not improve\n",
      "Epoch 232/500\n",
      "131642/131642 [==============================] - 1s 6us/step - loss: 0.1216 - mean_absolute_error: 0.1216 - val_loss: 0.1254 - val_mean_absolute_error: 0.1254\n",
      "\n",
      "Epoch 00232: val_loss did not improve\n",
      "Epoch 233/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1214 - mean_absolute_error: 0.1214 - val_loss: 0.1261 - val_mean_absolute_error: 0.1261\n",
      "\n",
      "Epoch 00233: val_loss did not improve\n",
      "Epoch 234/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1213 - mean_absolute_error: 0.1213 - val_loss: 0.1246 - val_mean_absolute_error: 0.1246\n",
      "\n",
      "Epoch 00234: val_loss did not improve\n",
      "Epoch 235/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1215 - mean_absolute_error: 0.1215 - val_loss: 0.1264 - val_mean_absolute_error: 0.1264\n",
      "\n",
      "Epoch 00235: val_loss did not improve\n",
      "Epoch 236/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1213 - mean_absolute_error: 0.1213 - val_loss: 0.1264 - val_mean_absolute_error: 0.1264\n",
      "\n",
      "Epoch 00236: val_loss did not improve\n",
      "Epoch 237/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1218 - mean_absolute_error: 0.1218 - val_loss: 0.1264 - val_mean_absolute_error: 0.1264\n",
      "\n",
      "Epoch 00237: val_loss did not improve\n",
      "Epoch 238/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1214 - mean_absolute_error: 0.1214 - val_loss: 0.1258 - val_mean_absolute_error: 0.1258\n",
      "\n",
      "Epoch 00238: val_loss did not improve\n",
      "Epoch 239/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1217 - mean_absolute_error: 0.1217 - val_loss: 0.1254 - val_mean_absolute_error: 0.1254\n",
      "\n",
      "Epoch 00239: val_loss did not improve\n",
      "Epoch 240/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1225 - mean_absolute_error: 0.1225 - val_loss: 0.1265 - val_mean_absolute_error: 0.1265\n",
      "\n",
      "Epoch 00240: val_loss did not improve\n",
      "Epoch 241/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1218 - mean_absolute_error: 0.1218 - val_loss: 0.1249 - val_mean_absolute_error: 0.1249\n",
      "\n",
      "Epoch 00241: val_loss did not improve\n",
      "Epoch 242/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1211 - mean_absolute_error: 0.1211 - val_loss: 0.1279 - val_mean_absolute_error: 0.1279\n",
      "\n",
      "Epoch 00242: val_loss did not improve\n",
      "Epoch 243/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1211 - mean_absolute_error: 0.1211 - val_loss: 0.1273 - val_mean_absolute_error: 0.1273\n",
      "\n",
      "Epoch 00243: val_loss did not improve\n",
      "Epoch 244/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1215 - mean_absolute_error: 0.1215 - val_loss: 0.1255 - val_mean_absolute_error: 0.1255\n",
      "\n",
      "Epoch 00244: val_loss did not improve\n",
      "Epoch 245/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1212 - mean_absolute_error: 0.1212 - val_loss: 0.1286 - val_mean_absolute_error: 0.1286\n",
      "\n",
      "Epoch 00245: val_loss did not improve\n",
      "Epoch 246/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1218 - mean_absolute_error: 0.1218 - val_loss: 0.1248 - val_mean_absolute_error: 0.1248\n",
      "\n",
      "Epoch 00246: val_loss did not improve\n",
      "Epoch 247/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1212 - mean_absolute_error: 0.1212 - val_loss: 0.1264 - val_mean_absolute_error: 0.1264\n",
      "\n",
      "Epoch 00247: val_loss did not improve\n",
      "Epoch 248/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1215 - mean_absolute_error: 0.1215 - val_loss: 0.1256 - val_mean_absolute_error: 0.1256\n",
      "\n",
      "Epoch 00248: val_loss did not improve\n",
      "Epoch 249/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1210 - mean_absolute_error: 0.1210 - val_loss: 0.1252 - val_mean_absolute_error: 0.1252\n",
      "\n",
      "Epoch 00249: val_loss did not improve\n",
      "Epoch 250/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1216 - mean_absolute_error: 0.1216 - val_loss: 0.1255 - val_mean_absolute_error: 0.1255\n",
      "\n",
      "Epoch 00250: val_loss did not improve\n",
      "Epoch 251/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1215 - mean_absolute_error: 0.1215 - val_loss: 0.1250 - val_mean_absolute_error: 0.1250\n",
      "\n",
      "Epoch 00251: val_loss did not improve\n",
      "Epoch 252/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1214 - mean_absolute_error: 0.1214 - val_loss: 0.1250 - val_mean_absolute_error: 0.1250\n",
      "\n",
      "Epoch 00252: val_loss did not improve\n",
      "Epoch 253/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1216 - mean_absolute_error: 0.1216 - val_loss: 0.1263 - val_mean_absolute_error: 0.1263\n",
      "\n",
      "Epoch 00253: val_loss did not improve\n",
      "Epoch 254/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1213 - mean_absolute_error: 0.1213 - val_loss: 0.1255 - val_mean_absolute_error: 0.1255\n",
      "\n",
      "Epoch 00254: val_loss did not improve\n",
      "Epoch 255/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1218 - mean_absolute_error: 0.1218 - val_loss: 0.1242 - val_mean_absolute_error: 0.1242\n",
      "\n",
      "Epoch 00255: val_loss improved from 0.12443 to 0.12418, saving model to D:\\fang\\AI_proj\\tbrain_policy\\checkpoints/reg_0816_17_40_19.h5\n",
      "Epoch 256/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1211 - mean_absolute_error: 0.1211 - val_loss: 0.1257 - val_mean_absolute_error: 0.1257\n",
      "\n",
      "Epoch 00256: val_loss did not improve\n",
      "Epoch 257/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1211 - mean_absolute_error: 0.1211 - val_loss: 0.1263 - val_mean_absolute_error: 0.1263\n",
      "\n",
      "Epoch 00257: val_loss did not improve\n",
      "Epoch 258/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1211 - mean_absolute_error: 0.1211 - val_loss: 0.1260 - val_mean_absolute_error: 0.1260\n",
      "\n",
      "Epoch 00258: val_loss did not improve\n",
      "Epoch 259/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1213 - mean_absolute_error: 0.1213 - val_loss: 0.1249 - val_mean_absolute_error: 0.1249\n",
      "\n",
      "Epoch 00259: val_loss did not improve\n",
      "Epoch 260/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1210 - mean_absolute_error: 0.1210 - val_loss: 0.1251 - val_mean_absolute_error: 0.1251\n",
      "\n",
      "Epoch 00260: val_loss did not improve\n",
      "Epoch 261/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1211 - mean_absolute_error: 0.1211 - val_loss: 0.1275 - val_mean_absolute_error: 0.1275\n",
      "\n",
      "Epoch 00261: val_loss did not improve\n",
      "Epoch 262/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1221 - mean_absolute_error: 0.1221 - val_loss: 0.1251 - val_mean_absolute_error: 0.1251\n",
      "\n",
      "Epoch 00262: val_loss did not improve\n",
      "Epoch 263/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1213 - mean_absolute_error: 0.1213 - val_loss: 0.1242 - val_mean_absolute_error: 0.1242\n",
      "\n",
      "Epoch 00263: val_loss did not improve\n",
      "Epoch 264/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1215 - mean_absolute_error: 0.1215 - val_loss: 0.1250 - val_mean_absolute_error: 0.1250\n",
      "\n",
      "Epoch 00264: val_loss did not improve\n",
      "Epoch 265/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1213 - mean_absolute_error: 0.1213 - val_loss: 0.1246 - val_mean_absolute_error: 0.1246\n",
      "\n",
      "Epoch 00265: val_loss did not improve\n",
      "Epoch 266/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1214 - mean_absolute_error: 0.1214 - val_loss: 0.1239 - val_mean_absolute_error: 0.1239\n",
      "\n",
      "Epoch 00266: val_loss improved from 0.12418 to 0.12391, saving model to D:\\fang\\AI_proj\\tbrain_policy\\checkpoints/reg_0816_17_40_19.h5\n",
      "Epoch 267/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1219 - mean_absolute_error: 0.1219 - val_loss: 0.1267 - val_mean_absolute_error: 0.1267\n",
      "\n",
      "Epoch 00267: val_loss did not improve\n",
      "Epoch 268/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1210 - mean_absolute_error: 0.1210 - val_loss: 0.1257 - val_mean_absolute_error: 0.1257\n",
      "\n",
      "Epoch 00268: val_loss did not improve\n",
      "Epoch 269/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1218 - mean_absolute_error: 0.1218 - val_loss: 0.1247 - val_mean_absolute_error: 0.1247\n",
      "\n",
      "Epoch 00269: val_loss did not improve\n",
      "Epoch 270/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1222 - mean_absolute_error: 0.1222 - val_loss: 0.1253 - val_mean_absolute_error: 0.1253\n",
      "\n",
      "Epoch 00270: val_loss did not improve\n",
      "Epoch 271/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1212 - mean_absolute_error: 0.1212 - val_loss: 0.1252 - val_mean_absolute_error: 0.1252\n",
      "\n",
      "Epoch 00271: val_loss did not improve\n",
      "Epoch 272/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1210 - mean_absolute_error: 0.1210 - val_loss: 0.1257 - val_mean_absolute_error: 0.1257\n",
      "\n",
      "Epoch 00272: val_loss did not improve\n",
      "Epoch 273/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1224 - mean_absolute_error: 0.1224 - val_loss: 0.1244 - val_mean_absolute_error: 0.1244\n",
      "\n",
      "Epoch 00273: val_loss did not improve\n",
      "Epoch 274/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1208 - mean_absolute_error: 0.1208 - val_loss: 0.1256 - val_mean_absolute_error: 0.1256\n",
      "\n",
      "Epoch 00274: val_loss did not improve\n",
      "Epoch 275/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1218 - mean_absolute_error: 0.1218 - val_loss: 0.1249 - val_mean_absolute_error: 0.1249\n",
      "\n",
      "Epoch 00275: val_loss did not improve\n",
      "Epoch 276/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1215 - mean_absolute_error: 0.1215 - val_loss: 0.1257 - val_mean_absolute_error: 0.1257\n",
      "\n",
      "Epoch 00276: val_loss did not improve\n",
      "Epoch 277/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1210 - mean_absolute_error: 0.1210 - val_loss: 0.1262 - val_mean_absolute_error: 0.1262\n",
      "\n",
      "Epoch 00277: val_loss did not improve\n",
      "Epoch 278/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1218 - mean_absolute_error: 0.1218 - val_loss: 0.1251 - val_mean_absolute_error: 0.1251\n",
      "\n",
      "Epoch 00278: val_loss did not improve\n",
      "Epoch 279/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1213 - mean_absolute_error: 0.1213 - val_loss: 0.1262 - val_mean_absolute_error: 0.1262\n",
      "\n",
      "Epoch 00279: val_loss did not improve\n",
      "Epoch 280/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1210 - mean_absolute_error: 0.1210 - val_loss: 0.1253 - val_mean_absolute_error: 0.1253\n",
      "\n",
      "Epoch 00280: val_loss did not improve\n",
      "Epoch 281/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1215 - mean_absolute_error: 0.1215 - val_loss: 0.1244 - val_mean_absolute_error: 0.1244\n",
      "\n",
      "Epoch 00281: val_loss did not improve\n",
      "Epoch 282/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1212 - mean_absolute_error: 0.1212 - val_loss: 0.1254 - val_mean_absolute_error: 0.1254\n",
      "\n",
      "Epoch 00282: val_loss did not improve\n",
      "Epoch 283/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1207 - mean_absolute_error: 0.1207 - val_loss: 0.1248 - val_mean_absolute_error: 0.1248\n",
      "\n",
      "Epoch 00283: val_loss did not improve\n",
      "Epoch 284/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1220 - mean_absolute_error: 0.1220 - val_loss: 0.1259 - val_mean_absolute_error: 0.1259\n",
      "\n",
      "Epoch 00284: val_loss did not improve\n",
      "Epoch 285/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1213 - mean_absolute_error: 0.1213 - val_loss: 0.1258 - val_mean_absolute_error: 0.1258\n",
      "\n",
      "Epoch 00285: val_loss did not improve\n",
      "Epoch 286/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1215 - mean_absolute_error: 0.1215 - val_loss: 0.1261 - val_mean_absolute_error: 0.1261\n",
      "\n",
      "Epoch 00286: val_loss did not improve\n",
      "Epoch 287/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1209 - mean_absolute_error: 0.1209 - val_loss: 0.1258 - val_mean_absolute_error: 0.1258\n",
      "\n",
      "Epoch 00287: val_loss did not improve\n",
      "Epoch 288/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1212 - mean_absolute_error: 0.1212 - val_loss: 0.1251 - val_mean_absolute_error: 0.1251\n",
      "\n",
      "Epoch 00288: val_loss did not improve\n",
      "Epoch 289/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1218 - mean_absolute_error: 0.1218 - val_loss: 0.1268 - val_mean_absolute_error: 0.1268\n",
      "\n",
      "Epoch 00289: val_loss did not improve\n",
      "Epoch 290/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1209 - mean_absolute_error: 0.1209 - val_loss: 0.1272 - val_mean_absolute_error: 0.1272\n",
      "\n",
      "Epoch 00290: val_loss did not improve\n",
      "Epoch 291/500\n",
      "131642/131642 [==============================] - 1s 6us/step - loss: 0.1217 - mean_absolute_error: 0.1217 - val_loss: 0.1249 - val_mean_absolute_error: 0.1249\n",
      "\n",
      "Epoch 00291: val_loss did not improve\n",
      "Epoch 292/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1216 - mean_absolute_error: 0.1216 - val_loss: 0.1250 - val_mean_absolute_error: 0.1250\n",
      "\n",
      "Epoch 00292: val_loss did not improve\n",
      "Epoch 293/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1211 - mean_absolute_error: 0.1211 - val_loss: 0.1285 - val_mean_absolute_error: 0.1285\n",
      "\n",
      "Epoch 00293: val_loss did not improve\n",
      "Epoch 294/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1215 - mean_absolute_error: 0.1215 - val_loss: 0.1253 - val_mean_absolute_error: 0.1253\n",
      "\n",
      "Epoch 00294: val_loss did not improve\n",
      "Epoch 295/500\n",
      "131642/131642 [==============================] - 1s 6us/step - loss: 0.1213 - mean_absolute_error: 0.1213 - val_loss: 0.1266 - val_mean_absolute_error: 0.1266\n",
      "\n",
      "Epoch 00295: val_loss did not improve\n",
      "Epoch 296/500\n",
      "131642/131642 [==============================] - 1s 6us/step - loss: 0.1218 - mean_absolute_error: 0.1218 - val_loss: 0.1270 - val_mean_absolute_error: 0.1270\n",
      "\n",
      "Epoch 00296: val_loss did not improve\n",
      "Epoch 297/500\n",
      "131642/131642 [==============================] - 1s 6us/step - loss: 0.1213 - mean_absolute_error: 0.1213 - val_loss: 0.1263 - val_mean_absolute_error: 0.1263\n",
      "\n",
      "Epoch 00297: val_loss did not improve\n",
      "Epoch 298/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1209 - mean_absolute_error: 0.1209 - val_loss: 0.1252 - val_mean_absolute_error: 0.1252\n",
      "\n",
      "Epoch 00298: val_loss did not improve\n",
      "Epoch 299/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1210 - mean_absolute_error: 0.1210 - val_loss: 0.1253 - val_mean_absolute_error: 0.1253\n",
      "\n",
      "Epoch 00299: val_loss did not improve\n",
      "Epoch 300/500\n",
      "131642/131642 [==============================] - 1s 6us/step - loss: 0.1211 - mean_absolute_error: 0.1211 - val_loss: 0.1260 - val_mean_absolute_error: 0.1260\n",
      "\n",
      "Epoch 00300: val_loss did not improve\n",
      "Epoch 301/500\n",
      "131642/131642 [==============================] - 1s 6us/step - loss: 0.1213 - mean_absolute_error: 0.1213 - val_loss: 0.1251 - val_mean_absolute_error: 0.1251\n",
      "\n",
      "Epoch 00301: val_loss did not improve\n",
      "Epoch 302/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1214 - mean_absolute_error: 0.1214 - val_loss: 0.1289 - val_mean_absolute_error: 0.1289\n",
      "\n",
      "Epoch 00302: val_loss did not improve\n",
      "Epoch 303/500\n",
      "131642/131642 [==============================] - 1s 6us/step - loss: 0.1219 - mean_absolute_error: 0.1219 - val_loss: 0.1258 - val_mean_absolute_error: 0.1258\n",
      "\n",
      "Epoch 00303: val_loss did not improve\n",
      "Epoch 304/500\n",
      "131642/131642 [==============================] - 1s 6us/step - loss: 0.1212 - mean_absolute_error: 0.1212 - val_loss: 0.1259 - val_mean_absolute_error: 0.1259\n",
      "\n",
      "Epoch 00304: val_loss did not improve\n",
      "Epoch 305/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1215 - mean_absolute_error: 0.1215 - val_loss: 0.1280 - val_mean_absolute_error: 0.1280\n",
      "\n",
      "Epoch 00305: val_loss did not improve\n",
      "Epoch 306/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1206 - mean_absolute_error: 0.1206 - val_loss: 0.1249 - val_mean_absolute_error: 0.1249\n",
      "\n",
      "Epoch 00306: val_loss did not improve\n",
      "Epoch 307/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1211 - mean_absolute_error: 0.1211 - val_loss: 0.1256 - val_mean_absolute_error: 0.1256\n",
      "\n",
      "Epoch 00307: val_loss did not improve\n",
      "Epoch 308/500\n",
      "131642/131642 [==============================] - 1s 6us/step - loss: 0.1212 - mean_absolute_error: 0.1212 - val_loss: 0.1256 - val_mean_absolute_error: 0.1256\n",
      "\n",
      "Epoch 00308: val_loss did not improve\n",
      "Epoch 309/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1214 - mean_absolute_error: 0.1214 - val_loss: 0.1260 - val_mean_absolute_error: 0.1260\n",
      "\n",
      "Epoch 00309: val_loss did not improve\n",
      "Epoch 310/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1217 - mean_absolute_error: 0.1217 - val_loss: 0.1245 - val_mean_absolute_error: 0.1245\n",
      "\n",
      "Epoch 00310: val_loss did not improve\n",
      "Epoch 311/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1205 - mean_absolute_error: 0.1205 - val_loss: 0.1250 - val_mean_absolute_error: 0.1250\n",
      "\n",
      "Epoch 00311: val_loss did not improve\n",
      "Epoch 312/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1213 - mean_absolute_error: 0.1213 - val_loss: 0.1262 - val_mean_absolute_error: 0.1262\n",
      "\n",
      "Epoch 00312: val_loss did not improve\n",
      "Epoch 313/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1214 - mean_absolute_error: 0.1214 - val_loss: 0.1267 - val_mean_absolute_error: 0.1267\n",
      "\n",
      "Epoch 00313: val_loss did not improve\n",
      "Epoch 314/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1214 - mean_absolute_error: 0.1214 - val_loss: 0.1252 - val_mean_absolute_error: 0.1252\n",
      "\n",
      "Epoch 00314: val_loss did not improve\n",
      "Epoch 315/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1210 - mean_absolute_error: 0.1210 - val_loss: 0.1261 - val_mean_absolute_error: 0.1261\n",
      "\n",
      "Epoch 00315: val_loss did not improve\n",
      "Epoch 316/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1210 - mean_absolute_error: 0.1210 - val_loss: 0.1246 - val_mean_absolute_error: 0.1246\n",
      "\n",
      "Epoch 00316: val_loss did not improve\n",
      "Epoch 317/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1210 - mean_absolute_error: 0.1210 - val_loss: 0.1255 - val_mean_absolute_error: 0.1255\n",
      "\n",
      "Epoch 00317: val_loss did not improve\n",
      "Epoch 318/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1211 - mean_absolute_error: 0.1211 - val_loss: 0.1269 - val_mean_absolute_error: 0.1269\n",
      "\n",
      "Epoch 00318: val_loss did not improve\n",
      "Epoch 319/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1212 - mean_absolute_error: 0.1212 - val_loss: 0.1253 - val_mean_absolute_error: 0.1253\n",
      "\n",
      "Epoch 00319: val_loss did not improve\n",
      "Epoch 320/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1209 - mean_absolute_error: 0.1209 - val_loss: 0.1258 - val_mean_absolute_error: 0.1258\n",
      "\n",
      "Epoch 00320: val_loss did not improve\n",
      "Epoch 321/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1213 - mean_absolute_error: 0.1213 - val_loss: 0.1254 - val_mean_absolute_error: 0.1254\n",
      "\n",
      "Epoch 00321: val_loss did not improve\n",
      "Epoch 322/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1211 - mean_absolute_error: 0.1211 - val_loss: 0.1267 - val_mean_absolute_error: 0.1267\n",
      "\n",
      "Epoch 00322: val_loss did not improve\n",
      "Epoch 323/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1210 - mean_absolute_error: 0.1210 - val_loss: 0.1281 - val_mean_absolute_error: 0.1281\n",
      "\n",
      "Epoch 00323: val_loss did not improve\n",
      "Epoch 324/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1208 - mean_absolute_error: 0.1208 - val_loss: 0.1247 - val_mean_absolute_error: 0.1247\n",
      "\n",
      "Epoch 00324: val_loss did not improve\n",
      "Epoch 325/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1214 - mean_absolute_error: 0.1214 - val_loss: 0.1255 - val_mean_absolute_error: 0.1255\n",
      "\n",
      "Epoch 00325: val_loss did not improve\n",
      "Epoch 326/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1212 - mean_absolute_error: 0.1212 - val_loss: 0.1261 - val_mean_absolute_error: 0.1261\n",
      "\n",
      "Epoch 00326: val_loss did not improve\n",
      "Epoch 327/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1216 - mean_absolute_error: 0.1216 - val_loss: 0.1251 - val_mean_absolute_error: 0.1251\n",
      "\n",
      "Epoch 00327: val_loss did not improve\n",
      "Epoch 328/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1210 - mean_absolute_error: 0.1210 - val_loss: 0.1251 - val_mean_absolute_error: 0.1251\n",
      "\n",
      "Epoch 00328: val_loss did not improve\n",
      "Epoch 329/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1212 - mean_absolute_error: 0.1212 - val_loss: 0.1280 - val_mean_absolute_error: 0.1280\n",
      "\n",
      "Epoch 00329: val_loss did not improve\n",
      "Epoch 330/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1211 - mean_absolute_error: 0.1211 - val_loss: 0.1288 - val_mean_absolute_error: 0.1288\n",
      "\n",
      "Epoch 00330: val_loss did not improve\n",
      "Epoch 331/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1211 - mean_absolute_error: 0.1211 - val_loss: 0.1246 - val_mean_absolute_error: 0.1246\n",
      "\n",
      "Epoch 00331: val_loss did not improve\n",
      "Epoch 332/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1215 - mean_absolute_error: 0.1215 - val_loss: 0.1258 - val_mean_absolute_error: 0.1258\n",
      "\n",
      "Epoch 00332: val_loss did not improve\n",
      "Epoch 333/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1214 - mean_absolute_error: 0.1214 - val_loss: 0.1306 - val_mean_absolute_error: 0.1306\n",
      "\n",
      "Epoch 00333: val_loss did not improve\n",
      "Epoch 334/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1213 - mean_absolute_error: 0.1213 - val_loss: 0.1268 - val_mean_absolute_error: 0.1268\n",
      "\n",
      "Epoch 00334: val_loss did not improve\n",
      "Epoch 335/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1214 - mean_absolute_error: 0.1214 - val_loss: 0.1255 - val_mean_absolute_error: 0.1255\n",
      "\n",
      "Epoch 00335: val_loss did not improve\n",
      "Epoch 336/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1209 - mean_absolute_error: 0.1209 - val_loss: 0.1266 - val_mean_absolute_error: 0.1266\n",
      "\n",
      "Epoch 00336: val_loss did not improve\n",
      "Epoch 337/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1211 - mean_absolute_error: 0.1211 - val_loss: 0.1273 - val_mean_absolute_error: 0.1273\n",
      "\n",
      "Epoch 00337: val_loss did not improve\n",
      "Epoch 338/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1208 - mean_absolute_error: 0.1208 - val_loss: 0.1258 - val_mean_absolute_error: 0.1258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00338: val_loss did not improve\n",
      "Epoch 339/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1206 - mean_absolute_error: 0.1206 - val_loss: 0.1278 - val_mean_absolute_error: 0.1278\n",
      "\n",
      "Epoch 00339: val_loss did not improve\n",
      "Epoch 340/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1213 - mean_absolute_error: 0.1213 - val_loss: 0.1259 - val_mean_absolute_error: 0.1259\n",
      "\n",
      "Epoch 00340: val_loss did not improve\n",
      "Epoch 341/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1211 - mean_absolute_error: 0.1211 - val_loss: 0.1253 - val_mean_absolute_error: 0.1253\n",
      "\n",
      "Epoch 00341: val_loss did not improve\n",
      "Epoch 342/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1210 - mean_absolute_error: 0.1210 - val_loss: 0.1264 - val_mean_absolute_error: 0.1264\n",
      "\n",
      "Epoch 00342: val_loss did not improve\n",
      "Epoch 343/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1210 - mean_absolute_error: 0.1210 - val_loss: 0.1262 - val_mean_absolute_error: 0.1262\n",
      "\n",
      "Epoch 00343: val_loss did not improve\n",
      "Epoch 344/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1211 - mean_absolute_error: 0.1211 - val_loss: 0.1261 - val_mean_absolute_error: 0.1261\n",
      "\n",
      "Epoch 00344: val_loss did not improve\n",
      "Epoch 345/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1219 - mean_absolute_error: 0.1219 - val_loss: 0.1261 - val_mean_absolute_error: 0.1261\n",
      "\n",
      "Epoch 00345: val_loss did not improve\n",
      "Epoch 346/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1208 - mean_absolute_error: 0.1208 - val_loss: 0.1248 - val_mean_absolute_error: 0.1248\n",
      "\n",
      "Epoch 00346: val_loss did not improve\n",
      "Epoch 347/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1208 - mean_absolute_error: 0.1208 - val_loss: 0.1259 - val_mean_absolute_error: 0.1259\n",
      "\n",
      "Epoch 00347: val_loss did not improve\n",
      "Epoch 348/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1209 - mean_absolute_error: 0.1209 - val_loss: 0.1272 - val_mean_absolute_error: 0.1272\n",
      "\n",
      "Epoch 00348: val_loss did not improve\n",
      "Epoch 349/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1212 - mean_absolute_error: 0.1212 - val_loss: 0.1252 - val_mean_absolute_error: 0.1252\n",
      "\n",
      "Epoch 00349: val_loss did not improve\n",
      "Epoch 350/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1216 - mean_absolute_error: 0.1216 - val_loss: 0.1254 - val_mean_absolute_error: 0.1254\n",
      "\n",
      "Epoch 00350: val_loss did not improve\n",
      "Epoch 351/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1212 - mean_absolute_error: 0.1212 - val_loss: 0.1250 - val_mean_absolute_error: 0.1250\n",
      "\n",
      "Epoch 00351: val_loss did not improve\n",
      "Epoch 352/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1210 - mean_absolute_error: 0.1210 - val_loss: 0.1250 - val_mean_absolute_error: 0.1250\n",
      "\n",
      "Epoch 00352: val_loss did not improve\n",
      "Epoch 353/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1215 - mean_absolute_error: 0.1215 - val_loss: 0.1253 - val_mean_absolute_error: 0.1253\n",
      "\n",
      "Epoch 00353: val_loss did not improve\n",
      "Epoch 354/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1209 - mean_absolute_error: 0.1209 - val_loss: 0.1270 - val_mean_absolute_error: 0.1270\n",
      "\n",
      "Epoch 00354: val_loss did not improve\n",
      "Epoch 355/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1219 - mean_absolute_error: 0.1219 - val_loss: 0.1266 - val_mean_absolute_error: 0.1266\n",
      "\n",
      "Epoch 00355: val_loss did not improve\n",
      "Epoch 356/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1210 - mean_absolute_error: 0.1210 - val_loss: 0.1258 - val_mean_absolute_error: 0.1258\n",
      "\n",
      "Epoch 00356: val_loss did not improve\n",
      "Epoch 357/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1212 - mean_absolute_error: 0.1212 - val_loss: 0.1247 - val_mean_absolute_error: 0.1247\n",
      "\n",
      "Epoch 00357: val_loss did not improve\n",
      "Epoch 358/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1205 - mean_absolute_error: 0.1205 - val_loss: 0.1246 - val_mean_absolute_error: 0.1246\n",
      "\n",
      "Epoch 00358: val_loss did not improve\n",
      "Epoch 359/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1205 - mean_absolute_error: 0.1205 - val_loss: 0.1254 - val_mean_absolute_error: 0.1254\n",
      "\n",
      "Epoch 00359: val_loss did not improve\n",
      "Epoch 360/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1208 - mean_absolute_error: 0.1208 - val_loss: 0.1252 - val_mean_absolute_error: 0.1252\n",
      "\n",
      "Epoch 00360: val_loss did not improve\n",
      "Epoch 361/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1214 - mean_absolute_error: 0.1214 - val_loss: 0.1259 - val_mean_absolute_error: 0.1259\n",
      "\n",
      "Epoch 00361: val_loss did not improve\n",
      "Epoch 362/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1212 - mean_absolute_error: 0.1212 - val_loss: 0.1246 - val_mean_absolute_error: 0.1246\n",
      "\n",
      "Epoch 00362: val_loss did not improve\n",
      "Epoch 363/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1213 - mean_absolute_error: 0.1213 - val_loss: 0.1255 - val_mean_absolute_error: 0.1255\n",
      "\n",
      "Epoch 00363: val_loss did not improve\n",
      "Epoch 364/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1207 - mean_absolute_error: 0.1207 - val_loss: 0.1250 - val_mean_absolute_error: 0.1250\n",
      "\n",
      "Epoch 00364: val_loss did not improve\n",
      "Epoch 365/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1214 - mean_absolute_error: 0.1214 - val_loss: 0.1255 - val_mean_absolute_error: 0.1255\n",
      "\n",
      "Epoch 00365: val_loss did not improve\n",
      "Epoch 366/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1206 - mean_absolute_error: 0.1206 - val_loss: 0.1257 - val_mean_absolute_error: 0.1257\n",
      "\n",
      "Epoch 00366: val_loss did not improve\n",
      "Epoch 367/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1210 - mean_absolute_error: 0.1210 - val_loss: 0.1258 - val_mean_absolute_error: 0.1258\n",
      "\n",
      "Epoch 00367: val_loss did not improve\n",
      "Epoch 368/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1213 - mean_absolute_error: 0.1213 - val_loss: 0.1253 - val_mean_absolute_error: 0.1253\n",
      "\n",
      "Epoch 00368: val_loss did not improve\n",
      "Epoch 369/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1207 - mean_absolute_error: 0.1207 - val_loss: 0.1266 - val_mean_absolute_error: 0.1266\n",
      "\n",
      "Epoch 00369: val_loss did not improve\n",
      "Epoch 370/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1210 - mean_absolute_error: 0.1210 - val_loss: 0.1258 - val_mean_absolute_error: 0.1258\n",
      "\n",
      "Epoch 00370: val_loss did not improve\n",
      "Epoch 371/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1216 - mean_absolute_error: 0.1216 - val_loss: 0.1258 - val_mean_absolute_error: 0.1258\n",
      "\n",
      "Epoch 00371: val_loss did not improve\n",
      "Epoch 372/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1210 - mean_absolute_error: 0.1210 - val_loss: 0.1246 - val_mean_absolute_error: 0.1246\n",
      "\n",
      "Epoch 00372: val_loss did not improve\n",
      "Epoch 373/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1208 - mean_absolute_error: 0.1208 - val_loss: 0.1248 - val_mean_absolute_error: 0.1248\n",
      "\n",
      "Epoch 00373: val_loss did not improve\n",
      "Epoch 374/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1211 - mean_absolute_error: 0.1211 - val_loss: 0.1269 - val_mean_absolute_error: 0.1269\n",
      "\n",
      "Epoch 00374: val_loss did not improve\n",
      "Epoch 375/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1205 - mean_absolute_error: 0.1205 - val_loss: 0.1289 - val_mean_absolute_error: 0.1289\n",
      "\n",
      "Epoch 00375: val_loss did not improve\n",
      "Epoch 376/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1208 - mean_absolute_error: 0.1208 - val_loss: 0.1245 - val_mean_absolute_error: 0.1245\n",
      "\n",
      "Epoch 00376: val_loss did not improve\n",
      "Epoch 377/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1209 - mean_absolute_error: 0.1209 - val_loss: 0.1252 - val_mean_absolute_error: 0.1252\n",
      "\n",
      "Epoch 00377: val_loss did not improve\n",
      "Epoch 378/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1206 - mean_absolute_error: 0.1206 - val_loss: 0.1264 - val_mean_absolute_error: 0.1264\n",
      "\n",
      "Epoch 00378: val_loss did not improve\n",
      "Epoch 379/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1209 - mean_absolute_error: 0.1209 - val_loss: 0.1253 - val_mean_absolute_error: 0.1253\n",
      "\n",
      "Epoch 00379: val_loss did not improve\n",
      "Epoch 380/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1213 - mean_absolute_error: 0.1213 - val_loss: 0.1291 - val_mean_absolute_error: 0.1291\n",
      "\n",
      "Epoch 00380: val_loss did not improve\n",
      "Epoch 381/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1217 - mean_absolute_error: 0.1217 - val_loss: 0.1254 - val_mean_absolute_error: 0.1254\n",
      "\n",
      "Epoch 00381: val_loss did not improve\n",
      "Epoch 382/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1211 - mean_absolute_error: 0.1211 - val_loss: 0.1252 - val_mean_absolute_error: 0.1252\n",
      "\n",
      "Epoch 00382: val_loss did not improve\n",
      "Epoch 383/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1212 - mean_absolute_error: 0.1212 - val_loss: 0.1258 - val_mean_absolute_error: 0.1258\n",
      "\n",
      "Epoch 00383: val_loss did not improve\n",
      "Epoch 384/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1211 - mean_absolute_error: 0.1211 - val_loss: 0.1249 - val_mean_absolute_error: 0.1249\n",
      "\n",
      "Epoch 00384: val_loss did not improve\n",
      "Epoch 385/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1209 - mean_absolute_error: 0.1209 - val_loss: 0.1256 - val_mean_absolute_error: 0.1256\n",
      "\n",
      "Epoch 00385: val_loss did not improve\n",
      "Epoch 386/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1209 - mean_absolute_error: 0.1209 - val_loss: 0.1271 - val_mean_absolute_error: 0.1271\n",
      "\n",
      "Epoch 00386: val_loss did not improve\n",
      "Epoch 387/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1205 - mean_absolute_error: 0.1205 - val_loss: 0.1260 - val_mean_absolute_error: 0.1260\n",
      "\n",
      "Epoch 00387: val_loss did not improve\n",
      "Epoch 388/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1214 - mean_absolute_error: 0.1214 - val_loss: 0.1255 - val_mean_absolute_error: 0.1255\n",
      "\n",
      "Epoch 00388: val_loss did not improve\n",
      "Epoch 389/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1212 - mean_absolute_error: 0.1212 - val_loss: 0.1260 - val_mean_absolute_error: 0.1260\n",
      "\n",
      "Epoch 00389: val_loss did not improve\n",
      "Epoch 390/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1213 - mean_absolute_error: 0.1213 - val_loss: 0.1253 - val_mean_absolute_error: 0.1253\n",
      "\n",
      "Epoch 00390: val_loss did not improve\n",
      "Epoch 391/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1213 - mean_absolute_error: 0.1213 - val_loss: 0.1255 - val_mean_absolute_error: 0.1255\n",
      "\n",
      "Epoch 00391: val_loss did not improve\n",
      "Epoch 392/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1209 - mean_absolute_error: 0.1209 - val_loss: 0.1260 - val_mean_absolute_error: 0.1260\n",
      "\n",
      "Epoch 00392: val_loss did not improve\n",
      "Epoch 393/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1215 - mean_absolute_error: 0.1215 - val_loss: 0.1258 - val_mean_absolute_error: 0.1258\n",
      "\n",
      "Epoch 00393: val_loss did not improve\n",
      "Epoch 394/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1210 - mean_absolute_error: 0.1210 - val_loss: 0.1254 - val_mean_absolute_error: 0.1254\n",
      "\n",
      "Epoch 00394: val_loss did not improve\n",
      "Epoch 395/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1210 - mean_absolute_error: 0.1210 - val_loss: 0.1278 - val_mean_absolute_error: 0.1278\n",
      "\n",
      "Epoch 00395: val_loss did not improve\n",
      "Epoch 396/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1212 - mean_absolute_error: 0.1212 - val_loss: 0.1247 - val_mean_absolute_error: 0.1247\n",
      "\n",
      "Epoch 00396: val_loss did not improve\n",
      "Epoch 397/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1211 - mean_absolute_error: 0.1211 - val_loss: 0.1283 - val_mean_absolute_error: 0.1283\n",
      "\n",
      "Epoch 00397: val_loss did not improve\n",
      "Epoch 398/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1208 - mean_absolute_error: 0.1208 - val_loss: 0.1252 - val_mean_absolute_error: 0.1252\n",
      "\n",
      "Epoch 00398: val_loss did not improve\n",
      "Epoch 399/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1213 - mean_absolute_error: 0.1213 - val_loss: 0.1259 - val_mean_absolute_error: 0.1259\n",
      "\n",
      "Epoch 00399: val_loss did not improve\n",
      "Epoch 400/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1210 - mean_absolute_error: 0.1210 - val_loss: 0.1279 - val_mean_absolute_error: 0.1279\n",
      "\n",
      "Epoch 00400: val_loss did not improve\n",
      "Epoch 401/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1207 - mean_absolute_error: 0.1207 - val_loss: 0.1255 - val_mean_absolute_error: 0.1255\n",
      "\n",
      "Epoch 00401: val_loss did not improve\n",
      "Epoch 402/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1209 - mean_absolute_error: 0.1209 - val_loss: 0.1270 - val_mean_absolute_error: 0.1270\n",
      "\n",
      "Epoch 00402: val_loss did not improve\n",
      "Epoch 403/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1205 - mean_absolute_error: 0.1205 - val_loss: 0.1252 - val_mean_absolute_error: 0.1252\n",
      "\n",
      "Epoch 00403: val_loss did not improve\n",
      "Epoch 404/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1214 - mean_absolute_error: 0.1214 - val_loss: 0.1271 - val_mean_absolute_error: 0.1271\n",
      "\n",
      "Epoch 00404: val_loss did not improve\n",
      "Epoch 405/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1212 - mean_absolute_error: 0.1212 - val_loss: 0.1250 - val_mean_absolute_error: 0.1250\n",
      "\n",
      "Epoch 00405: val_loss did not improve\n",
      "Epoch 406/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1212 - mean_absolute_error: 0.1212 - val_loss: 0.1267 - val_mean_absolute_error: 0.1267\n",
      "\n",
      "Epoch 00406: val_loss did not improve\n",
      "Epoch 407/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1206 - mean_absolute_error: 0.1206 - val_loss: 0.1274 - val_mean_absolute_error: 0.1274\n",
      "\n",
      "Epoch 00407: val_loss did not improve\n",
      "Epoch 408/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1209 - mean_absolute_error: 0.1209 - val_loss: 0.1269 - val_mean_absolute_error: 0.1269\n",
      "\n",
      "Epoch 00408: val_loss did not improve\n",
      "Epoch 409/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1207 - mean_absolute_error: 0.1207 - val_loss: 0.1252 - val_mean_absolute_error: 0.1252\n",
      "\n",
      "Epoch 00409: val_loss did not improve\n",
      "Epoch 410/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1207 - mean_absolute_error: 0.1207 - val_loss: 0.1266 - val_mean_absolute_error: 0.1266\n",
      "\n",
      "Epoch 00410: val_loss did not improve\n",
      "Epoch 411/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1205 - mean_absolute_error: 0.1205 - val_loss: 0.1263 - val_mean_absolute_error: 0.1263\n",
      "\n",
      "Epoch 00411: val_loss did not improve\n",
      "Epoch 412/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1212 - mean_absolute_error: 0.1212 - val_loss: 0.1247 - val_mean_absolute_error: 0.1247\n",
      "\n",
      "Epoch 00412: val_loss did not improve\n",
      "Epoch 413/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1209 - mean_absolute_error: 0.1209 - val_loss: 0.1259 - val_mean_absolute_error: 0.1259\n",
      "\n",
      "Epoch 00413: val_loss did not improve\n",
      "Epoch 414/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1205 - mean_absolute_error: 0.1205 - val_loss: 0.1261 - val_mean_absolute_error: 0.1261\n",
      "\n",
      "Epoch 00414: val_loss did not improve\n",
      "Epoch 415/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1213 - mean_absolute_error: 0.1213 - val_loss: 0.1260 - val_mean_absolute_error: 0.1260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00415: val_loss did not improve\n",
      "Epoch 416/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1213 - mean_absolute_error: 0.1213 - val_loss: 0.1257 - val_mean_absolute_error: 0.1257\n",
      "\n",
      "Epoch 00416: val_loss did not improve\n",
      "Epoch 417/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1215 - mean_absolute_error: 0.1215 - val_loss: 0.1251 - val_mean_absolute_error: 0.1251\n",
      "\n",
      "Epoch 00417: val_loss did not improve\n",
      "Epoch 418/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1209 - mean_absolute_error: 0.1209 - val_loss: 0.1264 - val_mean_absolute_error: 0.1264\n",
      "\n",
      "Epoch 00418: val_loss did not improve\n",
      "Epoch 419/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1208 - mean_absolute_error: 0.1208 - val_loss: 0.1258 - val_mean_absolute_error: 0.1258\n",
      "\n",
      "Epoch 00419: val_loss did not improve\n",
      "Epoch 420/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1204 - mean_absolute_error: 0.1204 - val_loss: 0.1258 - val_mean_absolute_error: 0.1258\n",
      "\n",
      "Epoch 00420: val_loss did not improve\n",
      "Epoch 421/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1207 - mean_absolute_error: 0.1207 - val_loss: 0.1258 - val_mean_absolute_error: 0.1258\n",
      "\n",
      "Epoch 00421: val_loss did not improve\n",
      "Epoch 422/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1217 - mean_absolute_error: 0.1217 - val_loss: 0.1258 - val_mean_absolute_error: 0.1258\n",
      "\n",
      "Epoch 00422: val_loss did not improve\n",
      "Epoch 423/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1208 - mean_absolute_error: 0.1208 - val_loss: 0.1264 - val_mean_absolute_error: 0.1264\n",
      "\n",
      "Epoch 00423: val_loss did not improve\n",
      "Epoch 424/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1204 - mean_absolute_error: 0.1204 - val_loss: 0.1247 - val_mean_absolute_error: 0.1247\n",
      "\n",
      "Epoch 00424: val_loss did not improve\n",
      "Epoch 425/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1208 - mean_absolute_error: 0.1208 - val_loss: 0.1283 - val_mean_absolute_error: 0.1283\n",
      "\n",
      "Epoch 00425: val_loss did not improve\n",
      "Epoch 426/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1204 - mean_absolute_error: 0.1204 - val_loss: 0.1250 - val_mean_absolute_error: 0.1250\n",
      "\n",
      "Epoch 00426: val_loss did not improve\n",
      "Epoch 427/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1211 - mean_absolute_error: 0.1211 - val_loss: 0.1254 - val_mean_absolute_error: 0.1254\n",
      "\n",
      "Epoch 00427: val_loss did not improve\n",
      "Epoch 428/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1212 - mean_absolute_error: 0.1212 - val_loss: 0.1258 - val_mean_absolute_error: 0.1258\n",
      "\n",
      "Epoch 00428: val_loss did not improve\n",
      "Epoch 429/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1207 - mean_absolute_error: 0.1207 - val_loss: 0.1253 - val_mean_absolute_error: 0.1253\n",
      "\n",
      "Epoch 00429: val_loss did not improve\n",
      "Epoch 430/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1211 - mean_absolute_error: 0.1211 - val_loss: 0.1263 - val_mean_absolute_error: 0.1263\n",
      "\n",
      "Epoch 00430: val_loss did not improve\n",
      "Epoch 431/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1213 - mean_absolute_error: 0.1213 - val_loss: 0.1260 - val_mean_absolute_error: 0.1260\n",
      "\n",
      "Epoch 00431: val_loss did not improve\n",
      "Epoch 432/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1208 - mean_absolute_error: 0.1208 - val_loss: 0.1263 - val_mean_absolute_error: 0.1263\n",
      "\n",
      "Epoch 00432: val_loss did not improve\n",
      "Epoch 433/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1211 - mean_absolute_error: 0.1211 - val_loss: 0.1267 - val_mean_absolute_error: 0.1267\n",
      "\n",
      "Epoch 00433: val_loss did not improve\n",
      "Epoch 434/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1208 - mean_absolute_error: 0.1208 - val_loss: 0.1264 - val_mean_absolute_error: 0.1264\n",
      "\n",
      "Epoch 00434: val_loss did not improve\n",
      "Epoch 435/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1205 - mean_absolute_error: 0.1205 - val_loss: 0.1267 - val_mean_absolute_error: 0.1267\n",
      "\n",
      "Epoch 00435: val_loss did not improve\n",
      "Epoch 436/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1204 - mean_absolute_error: 0.1204 - val_loss: 0.1252 - val_mean_absolute_error: 0.1252\n",
      "\n",
      "Epoch 00436: val_loss did not improve\n",
      "Epoch 437/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1204 - mean_absolute_error: 0.1204 - val_loss: 0.1254 - val_mean_absolute_error: 0.1254\n",
      "\n",
      "Epoch 00437: val_loss did not improve\n",
      "Epoch 438/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1216 - mean_absolute_error: 0.1216 - val_loss: 0.1273 - val_mean_absolute_error: 0.1273\n",
      "\n",
      "Epoch 00438: val_loss did not improve\n",
      "Epoch 439/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1206 - mean_absolute_error: 0.1206 - val_loss: 0.1263 - val_mean_absolute_error: 0.1263\n",
      "\n",
      "Epoch 00439: val_loss did not improve\n",
      "Epoch 440/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1207 - mean_absolute_error: 0.1207 - val_loss: 0.1254 - val_mean_absolute_error: 0.1254\n",
      "\n",
      "Epoch 00440: val_loss did not improve\n",
      "Epoch 441/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1212 - mean_absolute_error: 0.1212 - val_loss: 0.1252 - val_mean_absolute_error: 0.1252\n",
      "\n",
      "Epoch 00441: val_loss did not improve\n",
      "Epoch 442/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1207 - mean_absolute_error: 0.1207 - val_loss: 0.1270 - val_mean_absolute_error: 0.1270\n",
      "\n",
      "Epoch 00442: val_loss did not improve\n",
      "Epoch 443/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1207 - mean_absolute_error: 0.1207 - val_loss: 0.1258 - val_mean_absolute_error: 0.1258\n",
      "\n",
      "Epoch 00443: val_loss did not improve\n",
      "Epoch 444/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1213 - mean_absolute_error: 0.1213 - val_loss: 0.1274 - val_mean_absolute_error: 0.1274\n",
      "\n",
      "Epoch 00444: val_loss did not improve\n",
      "Epoch 445/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1210 - mean_absolute_error: 0.1210 - val_loss: 0.1281 - val_mean_absolute_error: 0.1281\n",
      "\n",
      "Epoch 00445: val_loss did not improve\n",
      "Epoch 446/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1207 - mean_absolute_error: 0.1207 - val_loss: 0.1272 - val_mean_absolute_error: 0.1272\n",
      "\n",
      "Epoch 00446: val_loss did not improve\n",
      "Epoch 447/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1210 - mean_absolute_error: 0.1210 - val_loss: 0.1273 - val_mean_absolute_error: 0.1273\n",
      "\n",
      "Epoch 00447: val_loss did not improve\n",
      "Epoch 448/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1206 - mean_absolute_error: 0.1206 - val_loss: 0.1278 - val_mean_absolute_error: 0.1278\n",
      "\n",
      "Epoch 00448: val_loss did not improve\n",
      "Epoch 449/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1206 - mean_absolute_error: 0.1206 - val_loss: 0.1265 - val_mean_absolute_error: 0.1265\n",
      "\n",
      "Epoch 00449: val_loss did not improve\n",
      "Epoch 450/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1213 - mean_absolute_error: 0.1213 - val_loss: 0.1269 - val_mean_absolute_error: 0.1269\n",
      "\n",
      "Epoch 00450: val_loss did not improve\n",
      "Epoch 451/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1207 - mean_absolute_error: 0.1207 - val_loss: 0.1301 - val_mean_absolute_error: 0.1301\n",
      "\n",
      "Epoch 00451: val_loss did not improve\n",
      "Epoch 452/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1210 - mean_absolute_error: 0.1210 - val_loss: 0.1263 - val_mean_absolute_error: 0.1263\n",
      "\n",
      "Epoch 00452: val_loss did not improve\n",
      "Epoch 453/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1207 - mean_absolute_error: 0.1207 - val_loss: 0.1282 - val_mean_absolute_error: 0.1282\n",
      "\n",
      "Epoch 00453: val_loss did not improve\n",
      "Epoch 454/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1206 - mean_absolute_error: 0.1206 - val_loss: 0.1267 - val_mean_absolute_error: 0.1267\n",
      "\n",
      "Epoch 00454: val_loss did not improve\n",
      "Epoch 455/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1213 - mean_absolute_error: 0.1213 - val_loss: 0.1279 - val_mean_absolute_error: 0.1279\n",
      "\n",
      "Epoch 00455: val_loss did not improve\n",
      "Epoch 456/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1210 - mean_absolute_error: 0.1210 - val_loss: 0.1261 - val_mean_absolute_error: 0.1261\n",
      "\n",
      "Epoch 00456: val_loss did not improve\n",
      "Epoch 457/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1206 - mean_absolute_error: 0.1206 - val_loss: 0.1259 - val_mean_absolute_error: 0.1259\n",
      "\n",
      "Epoch 00457: val_loss did not improve\n",
      "Epoch 458/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1213 - mean_absolute_error: 0.1213 - val_loss: 0.1257 - val_mean_absolute_error: 0.1257\n",
      "\n",
      "Epoch 00458: val_loss did not improve\n",
      "Epoch 459/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1211 - mean_absolute_error: 0.1211 - val_loss: 0.1314 - val_mean_absolute_error: 0.1314\n",
      "\n",
      "Epoch 00459: val_loss did not improve\n",
      "Epoch 460/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1212 - mean_absolute_error: 0.1212 - val_loss: 0.1252 - val_mean_absolute_error: 0.1252\n",
      "\n",
      "Epoch 00460: val_loss did not improve\n",
      "Epoch 461/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1213 - mean_absolute_error: 0.1213 - val_loss: 0.1277 - val_mean_absolute_error: 0.1277\n",
      "\n",
      "Epoch 00461: val_loss did not improve\n",
      "Epoch 462/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1212 - mean_absolute_error: 0.1212 - val_loss: 0.1253 - val_mean_absolute_error: 0.1253\n",
      "\n",
      "Epoch 00462: val_loss did not improve\n",
      "Epoch 463/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1206 - mean_absolute_error: 0.1206 - val_loss: 0.1267 - val_mean_absolute_error: 0.1267\n",
      "\n",
      "Epoch 00463: val_loss did not improve\n",
      "Epoch 464/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1199 - mean_absolute_error: 0.1199 - val_loss: 0.1266 - val_mean_absolute_error: 0.1266\n",
      "\n",
      "Epoch 00464: val_loss did not improve\n",
      "Epoch 465/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1205 - mean_absolute_error: 0.1205 - val_loss: 0.1265 - val_mean_absolute_error: 0.1265\n",
      "\n",
      "Epoch 00465: val_loss did not improve\n",
      "Epoch 466/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1206 - mean_absolute_error: 0.1206 - val_loss: 0.1286 - val_mean_absolute_error: 0.1286\n",
      "\n",
      "Epoch 00466: val_loss did not improve\n",
      "Epoch 467/500\n",
      "131642/131642 [==============================] - 1s 6us/step - loss: 0.1204 - mean_absolute_error: 0.1204 - val_loss: 0.1261 - val_mean_absolute_error: 0.1261\n",
      "\n",
      "Epoch 00467: val_loss did not improve\n",
      "Epoch 468/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1210 - mean_absolute_error: 0.1210 - val_loss: 0.1260 - val_mean_absolute_error: 0.1260\n",
      "\n",
      "Epoch 00468: val_loss did not improve\n",
      "Epoch 469/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1212 - mean_absolute_error: 0.1212 - val_loss: 0.1269 - val_mean_absolute_error: 0.1269\n",
      "\n",
      "Epoch 00469: val_loss did not improve\n",
      "Epoch 470/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1207 - mean_absolute_error: 0.1207 - val_loss: 0.1258 - val_mean_absolute_error: 0.1258\n",
      "\n",
      "Epoch 00470: val_loss did not improve\n",
      "Epoch 471/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1200 - mean_absolute_error: 0.1200 - val_loss: 0.1265 - val_mean_absolute_error: 0.1265\n",
      "\n",
      "Epoch 00471: val_loss did not improve\n",
      "Epoch 472/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1211 - mean_absolute_error: 0.1211 - val_loss: 0.1261 - val_mean_absolute_error: 0.1261\n",
      "\n",
      "Epoch 00472: val_loss did not improve\n",
      "Epoch 473/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1209 - mean_absolute_error: 0.1209 - val_loss: 0.1248 - val_mean_absolute_error: 0.1248\n",
      "\n",
      "Epoch 00473: val_loss did not improve\n",
      "Epoch 474/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1208 - mean_absolute_error: 0.1208 - val_loss: 0.1262 - val_mean_absolute_error: 0.1262\n",
      "\n",
      "Epoch 00474: val_loss did not improve\n",
      "Epoch 475/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1210 - mean_absolute_error: 0.1210 - val_loss: 0.1265 - val_mean_absolute_error: 0.1265\n",
      "\n",
      "Epoch 00475: val_loss did not improve\n",
      "Epoch 476/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1214 - mean_absolute_error: 0.1214 - val_loss: 0.1264 - val_mean_absolute_error: 0.1264\n",
      "\n",
      "Epoch 00476: val_loss did not improve\n",
      "Epoch 477/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1211 - mean_absolute_error: 0.1211 - val_loss: 0.1269 - val_mean_absolute_error: 0.1269\n",
      "\n",
      "Epoch 00477: val_loss did not improve\n",
      "Epoch 478/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1205 - mean_absolute_error: 0.1205 - val_loss: 0.1262 - val_mean_absolute_error: 0.1262\n",
      "\n",
      "Epoch 00478: val_loss did not improve\n",
      "Epoch 479/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1205 - mean_absolute_error: 0.1205 - val_loss: 0.1267 - val_mean_absolute_error: 0.1267\n",
      "\n",
      "Epoch 00479: val_loss did not improve\n",
      "Epoch 480/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1212 - mean_absolute_error: 0.1212 - val_loss: 0.1270 - val_mean_absolute_error: 0.1270\n",
      "\n",
      "Epoch 00480: val_loss did not improve\n",
      "Epoch 481/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1202 - mean_absolute_error: 0.1202 - val_loss: 0.1284 - val_mean_absolute_error: 0.1284\n",
      "\n",
      "Epoch 00481: val_loss did not improve\n",
      "Epoch 482/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1210 - mean_absolute_error: 0.1210 - val_loss: 0.1255 - val_mean_absolute_error: 0.1255\n",
      "\n",
      "Epoch 00482: val_loss did not improve\n",
      "Epoch 483/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1209 - mean_absolute_error: 0.1209 - val_loss: 0.1258 - val_mean_absolute_error: 0.1258\n",
      "\n",
      "Epoch 00483: val_loss did not improve\n",
      "Epoch 484/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1209 - mean_absolute_error: 0.1209 - val_loss: 0.1260 - val_mean_absolute_error: 0.1260\n",
      "\n",
      "Epoch 00484: val_loss did not improve\n",
      "Epoch 485/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1208 - mean_absolute_error: 0.1208 - val_loss: 0.1269 - val_mean_absolute_error: 0.1269\n",
      "\n",
      "Epoch 00485: val_loss did not improve\n",
      "Epoch 486/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1209 - mean_absolute_error: 0.1209 - val_loss: 0.1259 - val_mean_absolute_error: 0.1259\n",
      "\n",
      "Epoch 00486: val_loss did not improve\n",
      "Epoch 487/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1207 - mean_absolute_error: 0.1207 - val_loss: 0.1271 - val_mean_absolute_error: 0.1271\n",
      "\n",
      "Epoch 00487: val_loss did not improve\n",
      "Epoch 488/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1204 - mean_absolute_error: 0.1204 - val_loss: 0.1268 - val_mean_absolute_error: 0.1268\n",
      "\n",
      "Epoch 00488: val_loss did not improve\n",
      "Epoch 489/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1203 - mean_absolute_error: 0.1203 - val_loss: 0.1252 - val_mean_absolute_error: 0.1252\n",
      "\n",
      "Epoch 00489: val_loss did not improve\n",
      "Epoch 490/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1210 - mean_absolute_error: 0.1210 - val_loss: 0.1262 - val_mean_absolute_error: 0.1262\n",
      "\n",
      "Epoch 00490: val_loss did not improve\n",
      "Epoch 491/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1212 - mean_absolute_error: 0.1212 - val_loss: 0.1262 - val_mean_absolute_error: 0.1262\n",
      "\n",
      "Epoch 00491: val_loss did not improve\n",
      "Epoch 492/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1205 - mean_absolute_error: 0.1205 - val_loss: 0.1273 - val_mean_absolute_error: 0.1273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00492: val_loss did not improve\n",
      "Epoch 493/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1209 - mean_absolute_error: 0.1209 - val_loss: 0.1263 - val_mean_absolute_error: 0.1263\n",
      "\n",
      "Epoch 00493: val_loss did not improve\n",
      "Epoch 494/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1201 - mean_absolute_error: 0.1201 - val_loss: 0.1256 - val_mean_absolute_error: 0.1256\n",
      "\n",
      "Epoch 00494: val_loss did not improve\n",
      "Epoch 495/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1207 - mean_absolute_error: 0.1207 - val_loss: 0.1260 - val_mean_absolute_error: 0.1260\n",
      "\n",
      "Epoch 00495: val_loss did not improve\n",
      "Epoch 496/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1208 - mean_absolute_error: 0.1208 - val_loss: 0.1257 - val_mean_absolute_error: 0.1257\n",
      "\n",
      "Epoch 00496: val_loss did not improve\n",
      "Epoch 497/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1209 - mean_absolute_error: 0.1209 - val_loss: 0.1267 - val_mean_absolute_error: 0.1267\n",
      "\n",
      "Epoch 00497: val_loss did not improve\n",
      "Epoch 498/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1204 - mean_absolute_error: 0.1204 - val_loss: 0.1266 - val_mean_absolute_error: 0.1266\n",
      "\n",
      "Epoch 00498: val_loss did not improve\n",
      "Epoch 499/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1202 - mean_absolute_error: 0.1202 - val_loss: 0.1275 - val_mean_absolute_error: 0.1275\n",
      "\n",
      "Epoch 00499: val_loss did not improve\n",
      "Epoch 500/500\n",
      "131642/131642 [==============================] - 1s 5us/step - loss: 0.1204 - mean_absolute_error: 0.1204 - val_loss: 0.1254 - val_mean_absolute_error: 0.1254\n",
      "\n",
      "Epoch 00500: val_loss did not improve\n"
     ]
    }
   ],
   "source": [
    "earlystop = EarlyStopping(monitor='val_loss', patience=30, verbose=1)\n",
    "model_path = os.path.join(save_dir, time.strftime('reg_%m%d_%H_%M_%S.h5'))\n",
    "checkpoint = ModelCheckpoint(model_path, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "model_history = reg_model.fit(x_trian_regression, y_train_regression, validation_data=(x_val_regression, y_val_regression), batch_size=1000, epochs=500, shuffle=True,\n",
    "                            callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_loss = model_history.history['loss']\n",
    "val_loss = model_history.history['val_loss']\n",
    "\n",
    "plt.plot(training_loss, label=\"training_loss\")\n",
    "plt.plot(val_loss, label=\"validation_loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "best_reg_model = load_model('../checkpoints/reg_0816_17_40_19.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_nn_reg = best_reg_model.predict(ss_x.transform(test_claim_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_result['reg_nn'] = ss_y.inverse_transform(y_pred_nn_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_result.iloc[class_result[class_result['lgb'] == 0].index, -1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_score = pd.read_csv('../result_csv/15/super_weight_815_1598.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_result['super'] = super_score['Next_Premium']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GBOOST</th>\n",
       "      <th>RF</th>\n",
       "      <th>XGB</th>\n",
       "      <th>lgb</th>\n",
       "      <th>nn</th>\n",
       "      <th>reg_lgb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3305.553831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1850.044457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3177.210880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6000.660550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3144.411715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10645.941083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20721.577997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4093.423686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9173.705424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1763.258650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7427.004976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1374.972894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39829.128496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5819.331896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10635.490947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3591.639448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3058.026678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2323.671114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5101.887679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5332.890825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3276.519393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7534.846504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1380.913323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>185.037792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>221.099410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3579.719282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140480</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140481</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5071.292144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140482</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2117.505793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140483</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6343.211906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140484</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1877.729972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140485</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15610.428206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140486</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22190.838149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140487</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>220.891069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140488</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3949.435604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140489</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3050.735459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140490</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3212.996948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140491</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140492</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2409.151017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140493</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3018.570433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140494</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>42758.907759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140495</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3018.764977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140496</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5944.709384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140497</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2538.506877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140498</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3526.851193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140499</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140500</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9220.756934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140501</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3216.943400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140502</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140503</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4121.516222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140504</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2904.388296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140505</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2792.236124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140506</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12403.688436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140507</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1674.721129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140508</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3939.074319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140509</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4385.511355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140510 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        GBOOST  RF  XGB  lgb   nn       reg_lgb\n",
       "0            1   1    1  1.0  1.0   3305.553831\n",
       "1            0   0    0  1.0  1.0   1850.044457\n",
       "2            0   1    0  1.0  1.0   3177.210880\n",
       "3            1   1    1  1.0  1.0   6000.660550\n",
       "4            1   1    1  1.0  1.0   3144.411715\n",
       "5            1   1    1  1.0  1.0  10645.941083\n",
       "6            1   1    1  1.0  1.0  20721.577997\n",
       "7            0   0    0  0.0  0.0      0.000000\n",
       "8            1   1    1  1.0  1.0   4093.423686\n",
       "9            1   1    1  1.0  1.0   9173.705424\n",
       "10           1   1    0  1.0  1.0   1763.258650\n",
       "11           1   1    1  1.0  1.0   7427.004976\n",
       "12           0   0    0  0.0  0.0      0.000000\n",
       "13           1   1    1  1.0  1.0   1374.972894\n",
       "14           1   1    1  1.0  1.0  39829.128496\n",
       "15           1   1    1  1.0  1.0   5819.331896\n",
       "16           1   1    1  1.0  1.0  10635.490947\n",
       "17           0   0    0  1.0  1.0   3591.639448\n",
       "18           1   1    1  1.0  1.0   3058.026678\n",
       "19           1   1    1  1.0  1.0   2323.671114\n",
       "20           1   1    1  1.0  1.0   5101.887679\n",
       "21           1   1    1  1.0  1.0   5332.890825\n",
       "22           1   1    1  1.0  1.0   3276.519393\n",
       "23           1   1    1  1.0  1.0   7534.846504\n",
       "24           0   0    0  0.0  0.0      0.000000\n",
       "25           0   0    0  0.0  0.0      0.000000\n",
       "26           0   0    0  1.0  1.0   1380.913323\n",
       "27           0   0    0  1.0  1.0    185.037792\n",
       "28           0   0    0  1.0  1.0    221.099410\n",
       "29           1   1    1  1.0  1.0   3579.719282\n",
       "...        ...  ..  ...  ...  ...           ...\n",
       "140480       0   0    0  0.0  0.0      0.000000\n",
       "140481       1   1    1  1.0  1.0   5071.292144\n",
       "140482       0   0    0  1.0  1.0   2117.505793\n",
       "140483       1   1    1  1.0  1.0   6343.211906\n",
       "140484       1   1    1  1.0  1.0   1877.729972\n",
       "140485       1   1    1  1.0  1.0  15610.428206\n",
       "140486       0   0    0  1.0  1.0  22190.838149\n",
       "140487       0   0    0  1.0  0.0    220.891069\n",
       "140488       1   1    1  1.0  1.0   3949.435604\n",
       "140489       0   0    0  1.0  1.0   3050.735459\n",
       "140490       0   1    0  1.0  1.0   3212.996948\n",
       "140491       0   0    0  0.0  0.0      0.000000\n",
       "140492       1   1    1  1.0  1.0   2409.151017\n",
       "140493       0   0    0  1.0  1.0   3018.570433\n",
       "140494       1   1    1  1.0  1.0  42758.907759\n",
       "140495       0   0    0  1.0  1.0   3018.764977\n",
       "140496       1   1    1  1.0  1.0   5944.709384\n",
       "140497       0   1    0  1.0  1.0   2538.506877\n",
       "140498       1   1    1  1.0  1.0   3526.851193\n",
       "140499       0   0    0  0.0  0.0      0.000000\n",
       "140500       1   1    1  1.0  1.0   9220.756934\n",
       "140501       1   1    1  1.0  1.0   3216.943400\n",
       "140502       0   0    0  0.0  0.0      0.000000\n",
       "140503       1   1    1  1.0  1.0   4121.516222\n",
       "140504       1   1    1  1.0  1.0   2904.388296\n",
       "140505       1   1    1  1.0  1.0   2792.236124\n",
       "140506       1   1    1  1.0  1.0  12403.688436\n",
       "140507       1   1    1  1.0  1.0   1674.721129\n",
       "140508       1   1    1  1.0  1.0   3939.074319\n",
       "140509       1   1    1  1.0  1.0   4385.511355\n",
       "\n",
       "[140510 rows x 6 columns]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_result['res'] = class_result.reg - class_result.super"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_result.iloc[class_result[class_result['sum'] == 0].index, -2] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_result.iloc[class_result[(class_result['res'] > 3000) | (class_result['res'] < -3000)].index, -3] = class_result['super']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GBOOST</th>\n",
       "      <th>RF</th>\n",
       "      <th>XGB</th>\n",
       "      <th>nn</th>\n",
       "      <th>svm</th>\n",
       "      <th>sum</th>\n",
       "      <th>reg</th>\n",
       "      <th>super</th>\n",
       "      <th>res</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3455.581787</td>\n",
       "      <td>3227.854870</td>\n",
       "      <td>227.726917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6081.053223</td>\n",
       "      <td>6031.602276</td>\n",
       "      <td>49.450947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3286.188721</td>\n",
       "      <td>3117.252210</td>\n",
       "      <td>168.936511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9428.345703</td>\n",
       "      <td>10009.806770</td>\n",
       "      <td>-581.461067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20229.033203</td>\n",
       "      <td>20229.033500</td>\n",
       "      <td>-3217.023734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4011.476807</td>\n",
       "      <td>3892.345975</td>\n",
       "      <td>119.130832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9163.248047</td>\n",
       "      <td>9070.434670</td>\n",
       "      <td>92.813377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1736.641846</td>\n",
       "      <td>1738.275174</td>\n",
       "      <td>-1.633328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7524.509277</td>\n",
       "      <td>7503.139390</td>\n",
       "      <td>21.369887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1395.935791</td>\n",
       "      <td>1349.713815</td>\n",
       "      <td>46.221976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>34851.300781</td>\n",
       "      <td>34470.271400</td>\n",
       "      <td>381.029381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5799.384766</td>\n",
       "      <td>5636.932473</td>\n",
       "      <td>162.452293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9822.543945</td>\n",
       "      <td>9822.544315</td>\n",
       "      <td>535466.830685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3068.991699</td>\n",
       "      <td>2967.692867</td>\n",
       "      <td>101.298832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2380.793701</td>\n",
       "      <td>2230.984270</td>\n",
       "      <td>149.809431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5194.818359</td>\n",
       "      <td>4902.236718</td>\n",
       "      <td>292.581641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5337.241699</td>\n",
       "      <td>5217.625838</td>\n",
       "      <td>119.615861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3287.632080</td>\n",
       "      <td>3223.693017</td>\n",
       "      <td>63.939063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7556.827148</td>\n",
       "      <td>7359.007060</td>\n",
       "      <td>197.820088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3664.081055</td>\n",
       "      <td>3577.556100</td>\n",
       "      <td>86.524955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140480</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140481</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4946.900391</td>\n",
       "      <td>5007.414438</td>\n",
       "      <td>-60.514047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140482</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140483</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6479.264160</td>\n",
       "      <td>6461.989104</td>\n",
       "      <td>17.275056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140484</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1884.978760</td>\n",
       "      <td>1853.947203</td>\n",
       "      <td>31.031557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140485</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15029.787109</td>\n",
       "      <td>15437.955560</td>\n",
       "      <td>-408.168451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140486</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140487</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140488</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3992.347168</td>\n",
       "      <td>3928.444715</td>\n",
       "      <td>63.902453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140489</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140490</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3212.171631</td>\n",
       "      <td>3157.614183</td>\n",
       "      <td>54.557448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140491</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140492</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2384.286865</td>\n",
       "      <td>2388.269256</td>\n",
       "      <td>-3.982391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140493</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140494</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>38483.843750</td>\n",
       "      <td>38483.842770</td>\n",
       "      <td>14217.583011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140495</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140496</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5738.385742</td>\n",
       "      <td>5716.532533</td>\n",
       "      <td>21.853209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140497</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2516.330078</td>\n",
       "      <td>2461.394833</td>\n",
       "      <td>54.935245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140498</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3550.613525</td>\n",
       "      <td>3477.529231</td>\n",
       "      <td>73.084294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140499</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140500</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8756.081055</td>\n",
       "      <td>9342.016230</td>\n",
       "      <td>-585.935175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140501</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3239.167725</td>\n",
       "      <td>3212.722801</td>\n",
       "      <td>26.444924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140502</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140503</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4227.817383</td>\n",
       "      <td>4133.701313</td>\n",
       "      <td>94.116070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140504</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2890.745850</td>\n",
       "      <td>2722.017712</td>\n",
       "      <td>168.728138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140505</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2779.176758</td>\n",
       "      <td>2745.221464</td>\n",
       "      <td>33.955294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140506</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13884.361328</td>\n",
       "      <td>12879.444590</td>\n",
       "      <td>1004.916738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140507</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1826.580078</td>\n",
       "      <td>1631.988647</td>\n",
       "      <td>194.591431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140508</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3857.824219</td>\n",
       "      <td>3890.586723</td>\n",
       "      <td>-32.762504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140509</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4191.587402</td>\n",
       "      <td>3845.848867</td>\n",
       "      <td>345.738535</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140510 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        GBOOST  RF  XGB   nn  svm  sum           reg         super  \\\n",
       "0            1   1    1  1.0    1  1.0   3455.581787   3227.854870   \n",
       "1            0   0    0  0.0    0  0.0      0.000000      0.000000   \n",
       "2            0   1    0  1.0    0  0.0      0.000000      0.000000   \n",
       "3            1   1    1  1.0    1  1.0   6081.053223   6031.602276   \n",
       "4            1   1    1  1.0    1  1.0   3286.188721   3117.252210   \n",
       "5            1   1    1  1.0    1  1.0   9428.345703  10009.806770   \n",
       "6            1   1    1  1.0    0  1.0  20229.033203  20229.033500   \n",
       "7            0   0    0  0.0    1  0.0      0.000000      0.000000   \n",
       "8            1   1    1  1.0    1  1.0   4011.476807   3892.345975   \n",
       "9            1   1    1  1.0    1  1.0   9163.248047   9070.434670   \n",
       "10           1   0    1  1.0    1  1.0   1736.641846   1738.275174   \n",
       "11           1   1    1  1.0    1  1.0   7524.509277   7503.139390   \n",
       "12           0   0    0  0.0    0  0.0      0.000000      0.000000   \n",
       "13           1   1    1  1.0    1  1.0   1395.935791   1349.713815   \n",
       "14           1   1    1  1.0    1  1.0  34851.300781  34470.271400   \n",
       "15           1   1    1  1.0    1  1.0   5799.384766   5636.932473   \n",
       "16           1   1    1  1.0    0  1.0   9822.543945   9822.544315   \n",
       "17           0   0    0  1.0    0  0.0      0.000000      0.000000   \n",
       "18           1   1    1  1.0    1  1.0   3068.991699   2967.692867   \n",
       "19           1   1    1  1.0    0  1.0   2380.793701   2230.984270   \n",
       "20           1   1    1  1.0    0  1.0   5194.818359   4902.236718   \n",
       "21           1   0    1  1.0    0  1.0   5337.241699   5217.625838   \n",
       "22           1   1    1  1.0    1  1.0   3287.632080   3223.693017   \n",
       "23           1   1    1  1.0    1  1.0   7556.827148   7359.007060   \n",
       "24           0   0    0  0.0    0  0.0      0.000000      0.000000   \n",
       "25           0   0    0  0.0    0  0.0      0.000000      0.000000   \n",
       "26           0   0    0  1.0    0  0.0      0.000000      0.000000   \n",
       "27           0   0    0  1.0    0  0.0      0.000000      0.000000   \n",
       "28           0   0    0  0.0    0  0.0      0.000000      0.000000   \n",
       "29           1   1    1  1.0    0  1.0   3664.081055   3577.556100   \n",
       "...        ...  ..  ...  ...  ...  ...           ...           ...   \n",
       "140480       0   0    0  0.0    0  0.0      0.000000      0.000000   \n",
       "140481       1   1    1  1.0    1  1.0   4946.900391   5007.414438   \n",
       "140482       0   0    0  0.0    0  0.0      0.000000      0.000000   \n",
       "140483       1   1    1  1.0    0  1.0   6479.264160   6461.989104   \n",
       "140484       1   1    0  1.0    1  1.0   1884.978760   1853.947203   \n",
       "140485       1   1    1  1.0    0  1.0  15029.787109  15437.955560   \n",
       "140486       0   0    0  1.0    1  0.0      0.000000      0.000000   \n",
       "140487       0   0    0  0.0    0  0.0      0.000000      0.000000   \n",
       "140488       1   1    1  1.0    1  1.0   3992.347168   3928.444715   \n",
       "140489       0   0    0  1.0    0  0.0      0.000000      0.000000   \n",
       "140490       0   1    0  1.0    1  1.0   3212.171631   3157.614183   \n",
       "140491       0   0    0  0.0    0  0.0      0.000000      0.000000   \n",
       "140492       1   1    1  1.0    1  1.0   2384.286865   2388.269256   \n",
       "140493       0   0    0  1.0    0  0.0      0.000000      0.000000   \n",
       "140494       1   1    1  1.0    0  1.0  38483.843750  38483.842770   \n",
       "140495       0   0    0  1.0    0  0.0      0.000000      0.000000   \n",
       "140496       1   1    1  1.0    1  1.0   5738.385742   5716.532533   \n",
       "140497       0   1    0  1.0    1  1.0   2516.330078   2461.394833   \n",
       "140498       1   1    1  1.0    1  1.0   3550.613525   3477.529231   \n",
       "140499       0   0    0  0.0    0  0.0      0.000000      0.000000   \n",
       "140500       1   1    1  1.0    1  1.0   8756.081055   9342.016230   \n",
       "140501       1   1    1  1.0    0  1.0   3239.167725   3212.722801   \n",
       "140502       0   0    0  0.0    0  0.0      0.000000      0.000000   \n",
       "140503       1   1    1  1.0    1  1.0   4227.817383   4133.701313   \n",
       "140504       1   1    1  1.0    0  1.0   2890.745850   2722.017712   \n",
       "140505       1   1    1  1.0    1  1.0   2779.176758   2745.221464   \n",
       "140506       1   1    1  1.0    1  1.0  13884.361328  12879.444590   \n",
       "140507       1   1    1  1.0    1  1.0   1826.580078   1631.988647   \n",
       "140508       1   1    1  1.0    1  1.0   3857.824219   3890.586723   \n",
       "140509       1   1    1  1.0    1  1.0   4191.587402   3845.848867   \n",
       "\n",
       "                  res  \n",
       "0          227.726917  \n",
       "1            0.000000  \n",
       "2            0.000000  \n",
       "3           49.450947  \n",
       "4          168.936511  \n",
       "5         -581.461067  \n",
       "6        -3217.023734  \n",
       "7            0.000000  \n",
       "8          119.130832  \n",
       "9           92.813377  \n",
       "10          -1.633328  \n",
       "11          21.369887  \n",
       "12           0.000000  \n",
       "13          46.221976  \n",
       "14         381.029381  \n",
       "15         162.452293  \n",
       "16      535466.830685  \n",
       "17           0.000000  \n",
       "18         101.298832  \n",
       "19         149.809431  \n",
       "20         292.581641  \n",
       "21         119.615861  \n",
       "22          63.939063  \n",
       "23         197.820088  \n",
       "24           0.000000  \n",
       "25           0.000000  \n",
       "26           0.000000  \n",
       "27           0.000000  \n",
       "28           0.000000  \n",
       "29          86.524955  \n",
       "...               ...  \n",
       "140480       0.000000  \n",
       "140481     -60.514047  \n",
       "140482       0.000000  \n",
       "140483      17.275056  \n",
       "140484      31.031557  \n",
       "140485    -408.168451  \n",
       "140486       0.000000  \n",
       "140487       0.000000  \n",
       "140488      63.902453  \n",
       "140489       0.000000  \n",
       "140490      54.557448  \n",
       "140491       0.000000  \n",
       "140492      -3.982391  \n",
       "140493       0.000000  \n",
       "140494   14217.583011  \n",
       "140495       0.000000  \n",
       "140496      21.853209  \n",
       "140497      54.935245  \n",
       "140498      73.084294  \n",
       "140499       0.000000  \n",
       "140500    -585.935175  \n",
       "140501      26.444924  \n",
       "140502       0.000000  \n",
       "140503      94.116070  \n",
       "140504     168.728138  \n",
       "140505      33.955294  \n",
       "140506    1004.916738  \n",
       "140507     194.591431  \n",
       "140508     -32.762504  \n",
       "140509     345.738535  \n",
       "\n",
       "[140510 rows x 9 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result to csv\n",
    "submit = testingset_df.copy()\n",
    "submit['Next_Premium'] = class_result['reg_lgb']\n",
    "submit.to_csv('../result_csv/two_827.csv', sep=',', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
